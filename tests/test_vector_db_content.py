"""
Vector DB å†…å®¹æŸ¥çœ‹æµ‹è¯•

åŠŸèƒ½ï¼š
1. åŠ è½½æŒ‡å®šæ–‡æ¡£çš„ Vector DB
2. éå†æ˜¾ç¤ºæ‰€æœ‰æ–‡æ¡£å†…å®¹
3. æŒ‰ç±»å‹åˆ†ç»„å±•ç¤ºï¼ˆcontext, title, structureï¼‰
4. æ˜¾ç¤ºæ¯ä¸ªæ–‡æ¡£çš„ metadata å’Œ content

è¿è¡Œæ–¹å¼ï¼š
    python tests/test_vector_db_content.py
"""

import sys
import os
import logging
from pathlib import Path
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.core.vector_db.vector_db_client import VectorDBClient
from src.core.llm.client import LLMBase
from src.config.settings import DATA_ROOT

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s'
)
logger = logging.getLogger(__name__)


def print_section(title: str):
    """æ‰“å°åˆ†éš”çº¿å’Œæ ‡é¢˜"""
    print("\n" + "=" * 80)
    print(f"  {title}")
    print("=" * 80 + "\n")


def print_subsection(title: str):
    """æ‰“å°å­æ ‡é¢˜"""
    print("\n" + "-" * 80)
    print(f"  {title}")
    print("-" * 80 + "\n")


def load_vector_db(doc_name: str) -> VectorDBClient:
    """
    åŠ è½½æŒ‡å®šæ–‡æ¡£çš„ Vector DB

    Args:
        doc_name: æ–‡æ¡£åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰

    Returns:
        VectorDBClient å®ä¾‹
    """
    # æ„å»º vector db è·¯å¾„
    # vector_db_path = Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index"
    vector_db_path = Path(DATA_ROOT) / "vector_db" / "_metadata"

    logger.info(f"ğŸ“‚ Vector DB è·¯å¾„: {vector_db_path}")

    # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
    if not vector_db_path.exists():
        raise FileNotFoundError(f"âŒ Vector DB ä¸å­˜åœ¨: {vector_db_path}")

    logger.info(f"âœ… Vector DB è·¯å¾„å­˜åœ¨")

    # åˆå§‹åŒ– LLMï¼ˆéœ€è¦ embedding_modelï¼‰
    llm = LLMBase(provider="openai")

    # åˆ›å»º VectorDBClient
    vector_db_client = VectorDBClient(
        db_path=str(vector_db_path),
        embedding_model=llm.embedding_model
    )

    if not vector_db_client.vector_db:
        raise ValueError(f"âŒ Vector DB åŠ è½½å¤±è´¥")

    logger.info(f"âœ… Vector DB åŠ è½½æˆåŠŸ")

    return vector_db_client


def analyze_vector_db_content(vector_db_client: VectorDBClient) -> Dict[str, List[Dict[str, Any]]]:
    """
    åˆ†æ Vector DB ä¸­çš„æ‰€æœ‰æ–‡æ¡£

    Args:
        vector_db_client: VectorDBClient å®ä¾‹

    Returns:
        æŒ‰ç±»å‹åˆ†ç»„çš„æ–‡æ¡£å­—å…¸
    """
    # æŒ‰ç±»å‹åˆ†ç»„
    docs_by_type = {
        "context": [],
        "title": [],
        "structure": [],
        "other": []
    }

    # éå† docstore ä¸­çš„æ‰€æœ‰æ–‡æ¡£
    if not vector_db_client.vector_db or not vector_db_client.vector_db.docstore:
        logger.warning("âš ï¸  Docstore ä¸ºç©º")
        return docs_by_type

    total_docs = len(vector_db_client.vector_db.docstore._dict)
    logger.info(f"ğŸ“Š æ–‡æ¡£æ€»æ•°: {total_docs}")

    for doc_id, doc in vector_db_client.vector_db.docstore._dict.items():
        metadata = doc.metadata
        doc_type = metadata.get("type", "other")

        doc_info = {
            "doc_id": doc_id,
            "type": doc_type,
            "content": doc.page_content,
            "metadata": metadata
        }

        # æŒ‰ç±»å‹åˆ†ç±»
        if doc_type in docs_by_type:
            docs_by_type[doc_type].append(doc_info)
        else:
            docs_by_type["other"].append(doc_info)

    return docs_by_type


def display_statistics(docs_by_type: Dict[str, List[Dict[str, Any]]]):
    """
    æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯

    Args:
        docs_by_type: æŒ‰ç±»å‹åˆ†ç»„çš„æ–‡æ¡£å­—å…¸
    """
    print_section("ç»Ÿè®¡ä¿¡æ¯")

    total = sum(len(docs) for docs in docs_by_type.values())
    print(f"ğŸ“Š æ–‡æ¡£æ€»æ•°: {total}\n")

    for doc_type, docs in docs_by_type.items():
        if docs:
            print(f"  - {doc_type}: {len(docs)} ä¸ª")

    print()


def display_context_documents(docs: List[Dict[str, Any]]):
    """
    æ˜¾ç¤º context ç±»å‹çš„æ–‡æ¡£ï¼ˆç« èŠ‚æ‘˜è¦ï¼‰

    Args:
        docs: context ç±»å‹çš„æ–‡æ¡£åˆ—è¡¨
    """
    print_section(f"Context æ–‡æ¡£ (ç« èŠ‚æ‘˜è¦) - å…± {len(docs)} ä¸ª")

    for idx, doc in enumerate(docs, 1):
        metadata = doc["metadata"]
        title = metadata.get("title", "æœªçŸ¥æ ‡é¢˜")
        pages = metadata.get("pages", [])
        content = doc["content"]

        print_subsection(f"ç« èŠ‚ {idx}: {title}")
        print(f"ğŸ“„ é¡µç : {pages}")
        print(f"ğŸ“ æ‘˜è¦é•¿åº¦: {len(content)} å­—ç¬¦")
        print(f"\næ‘˜è¦å†…å®¹:\n{content[:500]}...")

        # æ˜¾ç¤º refactor ä¿¡æ¯
        refactor = metadata.get("refactor", "")
        if refactor:
            print(f"\nğŸ”„ é‡æ„å†…å®¹é•¿åº¦: {len(refactor)} å­—ç¬¦")
            print(f"é‡æ„å†…å®¹é¢„è§ˆ:\n{refactor[:300]}...")

        # æ˜¾ç¤º raw_data ä¿¡æ¯
        raw_data = metadata.get("raw_data", {})
        if raw_data:
            print(f"\nğŸ“‘ åŸå§‹æ•°æ®: {len(raw_data)} é¡µ")


def display_title_documents(docs: List[Dict[str, Any]]):
    """
    æ˜¾ç¤º title ç±»å‹çš„æ–‡æ¡£ï¼ˆç« èŠ‚æ ‡é¢˜ï¼‰

    Args:
        docs: title ç±»å‹çš„æ–‡æ¡£åˆ—è¡¨
    """
    print_section(f"Title æ–‡æ¡£ (ç« èŠ‚æ ‡é¢˜) - å…± {len(docs)} ä¸ª")

    for idx, doc in enumerate(docs, 1):
        metadata = doc["metadata"]
        title_content = doc["content"]
        pages = metadata.get("pages", [])
        summary = metadata.get("summary", "")

        print(f"{idx}. æ ‡é¢˜: {title_content}")
        print(f"   ğŸ“„ é¡µç : {pages}")
        print(f"   ğŸ“ æ‘˜è¦é•¿åº¦: {len(summary)} å­—ç¬¦")
        if summary:
            print(f"   æ‘˜è¦é¢„è§ˆ: {summary[:200]}...")
        print()


def display_structure_documents(docs: List[Dict[str, Any]]):
    """
    æ˜¾ç¤º structure ç±»å‹çš„æ–‡æ¡£ï¼ˆæ–‡æ¡£ç»“æ„ï¼‰

    Args:
        docs: structure ç±»å‹çš„æ–‡æ¡£åˆ—è¡¨
    """
    print_section(f"Structure æ–‡æ¡£ (æ–‡æ¡£ç»“æ„) - å…± {len(docs)} ä¸ª")

    for idx, doc in enumerate(docs, 1):
        metadata = doc["metadata"]
        content = doc["content"]

        print_subsection(f"ç»“æ„æ–‡æ¡£ {idx}")
        print(f"ğŸ“‹ å†…å®¹:\n{content}")
        print(f"\nğŸ“Š Metadata: {metadata}")


def display_other_documents(docs: List[Dict[str, Any]]):
    """
    æ˜¾ç¤ºå…¶ä»–ç±»å‹çš„æ–‡æ¡£

    Args:
        docs: å…¶ä»–ç±»å‹çš„æ–‡æ¡£åˆ—è¡¨
    """
    if not docs:
        return

    print_section(f"å…¶ä»–æ–‡æ¡£ - å…± {len(docs)} ä¸ª")

    for idx, doc in enumerate(docs, 1):
        doc_type = doc["type"]
        content = doc["content"]
        metadata = doc["metadata"]

        print_subsection(f"æ–‡æ¡£ {idx} (ç±»å‹: {doc_type})")
        print(f"ğŸ“ å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        print(f"å†…å®¹é¢„è§ˆ:\n{content[:300]}...")
        print(f"\nğŸ“Š Metadata: {metadata}")


def export_to_json(docs_by_type: Dict[str, List[Dict[str, Any]]], doc_name: str):
    """
    å°† Vector DB å†…å®¹å¯¼å‡ºä¸º JSON æ–‡ä»¶

    Args:
        docs_by_type: æŒ‰ç±»å‹åˆ†ç»„çš„æ–‡æ¡£å­—å…¸
        doc_name: æ–‡æ¡£åç§°
    """
    import json
    from datetime import datetime

    output_dir = Path(DATA_ROOT) / "output"
    output_dir.mkdir(exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"{doc_name}_vector_db_export_{timestamp}.json"

    # å‡†å¤‡å¯¼å‡ºæ•°æ®
    export_data = {
        "doc_name": doc_name,
        "export_time": datetime.now().isoformat(),
        "statistics": {
            doc_type: len(docs) for doc_type, docs in docs_by_type.items()
        },
        "documents": docs_by_type
    }

    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)

    logger.info(f"âœ… å¯¼å‡ºå®Œæˆ: {output_file}")
    print(f"\nğŸ’¾ å·²å¯¼å‡ºåˆ°: {output_file}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print_section("Vector DB å†…å®¹æŸ¥çœ‹å™¨")

    # ==================== é…ç½®æµ‹è¯•å‚æ•° ====================

    # ğŸ“ åœ¨è¿™é‡Œé…ç½®è¦æŸ¥çœ‹çš„PDFåç§°ï¼ˆä¸å«.pdfæ‰©å±•åï¼‰
    doc_name = "1706.03762v7"

    # æ˜¯å¦å¯¼å‡ºä¸ºJSONæ–‡ä»¶
    export_json = True

    # ==================== æ‰§è¡Œæµ‹è¯• ====================

    logger.info(f"ğŸ“‹ æµ‹è¯•é…ç½®:")
    logger.info(f"   - æ–‡æ¡£åç§°: {doc_name}")
    logger.info(f"   - å¯¼å‡ºJSON: {export_json}")

    try:
        # 1. åŠ è½½ Vector DB
        print_section("åŠ è½½ Vector DB")
        vector_db_client = load_vector_db(doc_name)

        # 2. åˆ†æå†…å®¹
        print_section("åˆ†æ Vector DB å†…å®¹")
        docs_by_type = analyze_vector_db_content(vector_db_client)

        # 3. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        display_statistics(docs_by_type)

        # 4. æ˜¾ç¤ºå„ç±»å‹æ–‡æ¡£
        if docs_by_type["context"]:
            display_context_documents(docs_by_type["context"])

        if docs_by_type["title"]:
            display_title_documents(docs_by_type["title"])

        if docs_by_type["structure"]:
            display_structure_documents(docs_by_type["structure"])

        if docs_by_type["other"]:
            display_other_documents(docs_by_type["other"])

        # 5. å¯¼å‡ºä¸ºJSONï¼ˆå¯é€‰ï¼‰
        if export_json:
            print_section("å¯¼å‡ºæ•°æ®")
            export_to_json(docs_by_type, doc_name)

        print_section("æµ‹è¯•å®Œæˆ")
        logger.info("âœ… Vector DB å†…å®¹æŸ¥çœ‹å®Œæˆï¼")

    except FileNotFoundError as e:
        logger.error(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {e}")
        logger.info("\nğŸ’¡ æç¤º: è¯·å…ˆè¿è¡Œ test_indexing_agent.py å¯¹æ–‡æ¡£è¿›è¡Œç´¢å¼•")

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    main()
