"""
LLM Client åŸºç¡€åŠŸèƒ½æµ‹è¯•

è¿™ä¸ªæµ‹è¯•å±•ç¤ºï¼š
1. åˆå§‹åŒ– LLM Client
2. å‘é€ç®€å•æ¶ˆæ¯å¹¶æŸ¥çœ‹LLMè¿”å›
3. å¤šè½®å¯¹è¯æµ‹è¯•
4. æŸ¥çœ‹ä¼šè¯ä¿¡æ¯

è¿è¡Œæ–¹å¼ï¼š
    python tests/test_llm_client.py
"""
import sys
import os
import logging

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.core.llm.client import LLMBase
from src.config.prompts.common_prompts import CommonRole

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s'
)
logger = logging.getLogger(__name__)


def print_section(title):
    """æ‰“å°åˆ†éš”çº¿å’Œæ ‡é¢˜"""
    print("\n" + "="*70)
    print(f"  {title}")
    print("="*70 + "\n")


def test_basic_llm_call():
    """æµ‹è¯•1ï¼šåŸºç¡€LLMè°ƒç”¨"""
    print_section("æµ‹è¯•1ï¼šåŸºç¡€LLMè°ƒç”¨")

    try:
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨OpenAI providerï¼‰
        print("ğŸ“Œ åˆå§‹åŒ– LLM Client (provider=openai)...")
        llm_client = LLMBase(provider="openai")

        # è·å–providerä¿¡æ¯
        provider_info = llm_client.get_provider_info()
        print(f"âœ… Providerä¿¡æ¯:")
        print(f"   - Provider: {provider_info['provider']}")
        print(f"   - Chat Model: {provider_info['chat_model_type']}")
        print(f"   - Embedding Model: {provider_info['embedding_model_type']}")

        # æµ‹è¯•ç®€å•å¯¹è¯
        print("\nğŸ“Œ å‘é€æµ‹è¯•æ¶ˆæ¯...")
        session_id = "test_session_1"
        user_input = "ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±ã€‚"

        print(f"ğŸ‘¤ ç”¨æˆ·: {user_input}")

        response = llm_client.call_llm_chain(
            role=CommonRole.CHAPTER_MATCHER,
            input_prompt=user_input,
            session_id=session_id
        )

        print(f"ğŸ¤– AIå›å¤: {response}")

        # æŸ¥çœ‹ä¼šè¯ä¿¡æ¯
        session_info = llm_client.get_session_info(session_id)
        print(f"\nğŸ“Š ä¼šè¯ä¿¡æ¯:")
        print(f"   - Session ID: {session_info['session_id']}")
        print(f"   - Message Count: {session_info['message_count']}")

        print("\nâœ… æµ‹è¯•1é€šè¿‡ï¼šLLMæˆåŠŸå“åº”ï¼")
        return True

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•1å¤±è´¥: {e}")
        logger.exception("æµ‹è¯•1è¯¦ç»†é”™è¯¯:")
        return False


def test_multi_turn_conversation():
    """æµ‹è¯•2ï¼šå¤šè½®å¯¹è¯"""
    print_section("æµ‹è¯•2ï¼šå¤šè½®å¯¹è¯")

    try:
        llm_client = LLMBase(provider="openai")
        session_id = "test_session_2"

        # å¯¹è¯è½®æ¬¡
        conversations = [
            "è¯·è®°ä½è¿™ä¸ªæ•°å­—ï¼š42",
            "æˆ‘åˆšæ‰å‘Šè¯‰ä½ çš„æ•°å­—æ˜¯å¤šå°‘ï¼Ÿ",
            "æŠŠè¿™ä¸ªæ•°å­—ä¹˜ä»¥2æ˜¯å¤šå°‘ï¼Ÿ"
        ]

        print("ğŸ“Œ å¼€å§‹å¤šè½®å¯¹è¯æµ‹è¯•...\n")

        for i, user_input in enumerate(conversations, 1):
            print(f"--- ç¬¬{i}è½®å¯¹è¯ ---")
            print(f"ğŸ‘¤ ç”¨æˆ·: {user_input}")

            response = llm_client.call_llm_chain(
                role=CommonRole.CHAPTER_MATCHER,
                input_prompt=user_input,
                session_id=session_id
            )

            print(f"ğŸ¤– AIå›å¤: {response}")

            # æ˜¾ç¤ºå½“å‰æ¶ˆæ¯æ•°
            session_info = llm_client.get_session_info(session_id)
            print(f"ğŸ“Š å½“å‰æ¶ˆæ¯æ•°: {session_info['message_count']}\n")

        print("âœ… æµ‹è¯•2é€šè¿‡ï¼šå¤šè½®å¯¹è¯æˆåŠŸï¼")
        return True

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•2å¤±è´¥: {e}")
        logger.exception("æµ‹è¯•2è¯¦ç»†é”™è¯¯:")
        return False


def test_session_management():
    """æµ‹è¯•3ï¼šä¼šè¯ç®¡ç†"""
    print_section("æµ‹è¯•3ï¼šä¼šè¯ç®¡ç†")

    try:
        llm_client = LLMBase(provider="openai")

        # åˆ›å»ºå¤šä¸ªä¼šè¯
        sessions = ["session_A", "session_B", "session_C"]

        print("ğŸ“Œ åˆ›å»ºå¤šä¸ªç‹¬ç«‹ä¼šè¯...\n")

        for session_id in sessions:
            message = f"è¿™æ˜¯ä¼šè¯ {session_id} çš„æ¶ˆæ¯"
            print(f"å‘ {session_id} å‘é€: {message}")

            llm_client.call_llm_chain(
                role=CommonRole.CHAPTER_MATCHER,
                input_prompt=message,
                session_id=session_id
            )

        # æŸ¥çœ‹æ‰€æœ‰ä¼šè¯ä¿¡æ¯
        all_sessions_info = llm_client.get_session_info()
        print(f"\nğŸ“Š æ€»ä¼šè¯ä¿¡æ¯:")
        print(f"   - æ€»ä¼šè¯æ•°: {all_sessions_info['total_sessions']}")
        print(f"   - ä¼šè¯åˆ—è¡¨: {all_sessions_info['sessions']}")

        # æŸ¥çœ‹å•ä¸ªä¼šè¯è¯¦æƒ…
        print(f"\nğŸ“Š å•ä¸ªä¼šè¯è¯¦æƒ…:")
        for session_id in sessions:
            info = llm_client.get_session_info(session_id)
            print(f"   - {session_id}: {info['message_count']} æ¡æ¶ˆæ¯")

        print("\nâœ… æµ‹è¯•3é€šè¿‡ï¼šä¼šè¯ç®¡ç†æˆåŠŸï¼")
        return True

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•3å¤±è´¥: {e}")
        logger.exception("æµ‹è¯•3è¯¦ç»†é”™è¯¯:")
        return False


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "ğŸš€"*35)
    print("  AgenticReader - LLM Client åŠŸèƒ½æµ‹è¯•")
    print("ğŸš€"*35)

    results = []

    # è¿è¡Œæµ‹è¯•
    results.append(("åŸºç¡€LLMè°ƒç”¨", test_basic_llm_call()))
    results.append(("å¤šè½®å¯¹è¯", test_multi_turn_conversation()))
    results.append(("ä¼šè¯ç®¡ç†", test_session_management()))

    # æ±‡æ€»ç»“æœ
    print_section("æµ‹è¯•æ±‡æ€»")

    passed = sum(1 for _, result in results if result)
    total = len(results)

    for test_name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"{status} - {test_name}")

    print(f"\næ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")

    if passed == total:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼")
    else:
        print(f"\nâš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")


if __name__ == "__main__":
    main()
