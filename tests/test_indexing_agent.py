"""
IndexingAgent æµ‹è¯•æ–‡ä»¶

åŠŸèƒ½ï¼š
1. æ£€æŸ¥PDFæ–‡æ¡£æ˜¯å¦å·²ç»æ³¨å†Œåˆ°DocumentRegistry
2. å¦‚æœæœªæ³¨å†Œï¼Œè°ƒç”¨IndexingAgentè¿›è¡Œè§£æå’Œç´¢å¼•
3. æ”¯æŒæ‰¹é‡å¤„ç†PDFåˆ—è¡¨

è¿è¡Œæ–¹å¼ï¼š
    python tests/test_indexing_agent.py
"""
import sys
import os
import logging
import asyncio
from typing import List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

from src.agents.indexing import IndexingAgent, DocumentRegistry
from src.config.settings import PDF_PATH

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,  # Changed to DEBUG to see detailed stage_status tracking
    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s'
)
logger = logging.getLogger(__name__)


def print_section(title):
    """æ‰“å°åˆ†éš”çº¿å’Œæ ‡é¢˜"""
    print("\n" + "="*80)
    print(f"  {title}")
    print("="*80 + "\n")


def print_subsection(title):
    """æ‰“å°å­æ ‡é¢˜"""
    print("\n" + "-"*80)
    print(f"  {title}")
    print("-"*80 + "\n")


def check_pdf_exists(pdf_name: str) -> bool:
    """
    æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨äºdata/pdf/ç›®å½•

    Args:
        pdf_name: PDFæ–‡ä»¶åï¼ˆä¸å«.pdfæ‰©å±•åï¼‰

    Returns:
        bool: æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    """
    pdf_path = os.path.join(PDF_PATH, f"{pdf_name}.pdf")
    exists = os.path.exists(pdf_path)

    if exists:
        # è·å–æ–‡ä»¶å¤§å°
        size_bytes = os.path.getsize(pdf_path)
        size_mb = size_bytes / (1024 * 1024)
        logger.info(f"âœ… PDFæ–‡ä»¶å­˜åœ¨: {pdf_name}.pdf ({size_mb:.2f} MB)")
    else:
        logger.warning(f"âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

    return exists


def is_document_registered(doc_registry: DocumentRegistry, pdf_name: str) -> bool:
    """
    æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²ç»æ³¨å†Œ

    Args:
        doc_registry: DocumentRegistryå®ä¾‹
        pdf_name: PDFæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰

    Returns:
        bool: æ˜¯å¦å·²æ³¨å†Œ
    """
    doc_info = doc_registry.get_by_name(pdf_name)

    if doc_info:
        logger.info(f"ğŸ“‹ æ–‡æ¡£å·²æ³¨å†Œ: {pdf_name}")
        logger.info(f"   - æ–‡æ¡£ID: {doc_info['doc_id']}")
        logger.info(f"   - ç´¢å¼•è·¯å¾„: {doc_info.get('index_path', 'N/A')}")
        logger.info(f"   - åˆ›å»ºæ—¶é—´: {doc_info.get('created_at', 'N/A')}")
        logger.info(f"   - ç®€è¦æ‘˜è¦: {doc_info.get('brief_summary', 'N/A')[:100]}...")

        # æ˜¾ç¤ºå¤„ç†é˜¶æ®µçŠ¶æ€
        if "processing_stages" in doc_info:
            stages = doc_info["processing_stages"]
            logger.info(f"   - å¤„ç†é˜¶æ®µ:")
            for stage_name, stage_info in stages.items():
                status_emoji = "âœ…" if stage_info.get("status") == "completed" else "âŒ"
                output_count = len(stage_info.get("output_files", []))
                logger.info(f"     {status_emoji} {stage_name}: {stage_info.get('status')} ({output_count} ä¸ªæ–‡ä»¶)")

        return True
    else:
        logger.info(f"ğŸ†• æ–‡æ¡£æœªæ³¨å†Œ: {pdf_name}")
        return False


async def process_single_pdf(
    indexing_agent: IndexingAgent,
    doc_registry: DocumentRegistry,
    pdf_name: str,
    force_reindex: bool = False
) -> dict:
    """
    å¤„ç†å•ä¸ªPDFæ–‡æ¡£

    Args:
        indexing_agent: IndexingAgentå®ä¾‹
        doc_registry: DocumentRegistryå®ä¾‹
        pdf_name: PDFæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        force_reindex: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç´¢å¼•ï¼ˆå³ä½¿å·²æ³¨å†Œï¼‰

    Returns:
        dict: å¤„ç†ç»“æœ
    """
    print_subsection(f"å¤„ç†æ–‡æ¡£: {pdf_name}")

    # 1. æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not check_pdf_exists(pdf_name):
        return {
            "pdf_name": pdf_name,
            "status": "error",
            "message": "PDFæ–‡ä»¶ä¸å­˜åœ¨"
        }

    # 2. æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²æ³¨å†Œ
    is_registered = is_document_registered(doc_registry, pdf_name)

    if is_registered and not force_reindex:
        logger.info(f"â­ï¸  è·³è¿‡å·²æ³¨å†Œçš„æ–‡æ¡£: {pdf_name}")
        return {
            "pdf_name": pdf_name,
            "status": "skipped",
            "message": "æ–‡æ¡£å·²æ³¨å†Œ"
        }

    # 3. è°ƒç”¨IndexingAgentè¿›è¡Œç´¢å¼•
    if is_registered and force_reindex:
        logger.warning(f"ğŸ”„ å¼ºåˆ¶é‡æ–°ç´¢å¼•: {pdf_name}")
    else:
        logger.info(f"ğŸš€ å¼€å§‹ç´¢å¼•æ–°æ–‡æ¡£: {pdf_name}")

    try:
        # æ„å»ºPDFå®Œæ•´è·¯å¾„
        pdf_path = os.path.join(PDF_PATH, f"{pdf_name}.pdf")

        # è°ƒç”¨IndexingAgentçš„graph
        logger.info(f"ğŸ“‘ è°ƒç”¨IndexingAgentå¤„ç†æ–‡æ¡£...")
        result = await indexing_agent.graph.ainvoke({
            "doc_name": pdf_name,
            "doc_path": pdf_path,
            "doc_type": "pdf",
            "status": "pending"
        })

        # æ£€æŸ¥å¤„ç†ç»“æœ
        final_status = result.get("status")

        if final_status == "completed":
            logger.info(f"âœ… æ–‡æ¡£ç´¢å¼•æˆåŠŸ: {pdf_name}")
            logger.info(f"   - æ–‡æ¡£ID: {result.get('doc_id')}")
            logger.info(f"   - ç´¢å¼•è·¯å¾„: {result.get('index_path')}")
            logger.info(f"   - ç®€è¦æ‘˜è¦: {result.get('brief_summary', '')[:150]}...")

            # æ˜¾ç¤ºç”Ÿæˆçš„æ–‡ä»¶
            generated_files = result.get("generated_files", {})
            logger.info(f"   - ç”Ÿæˆçš„æ–‡ä»¶:")
            logger.info(f"     * å›¾ç‰‡: {len(generated_files.get('images', []))} ä¸ª")
            logger.info(f"     * JSON: {generated_files.get('json_data', 'N/A')}")
            logger.info(f"     * å‘é‡DB: {generated_files.get('vector_db', 'N/A')}")
            logger.info(f"     * æ‘˜è¦: {len(generated_files.get('summaries', []))} ä¸ª")

            # æ˜¾ç¤ºå¤„ç†é˜¶æ®µçŠ¶æ€
            doc_info = doc_registry.get_by_name(pdf_name)
            if doc_info and "processing_stages" in doc_info:
                stages = doc_info["processing_stages"]
                logger.info(f"   - å¤„ç†é˜¶æ®µ:")
                for stage_name, stage_info in stages.items():
                    status_emoji = "âœ…" if stage_info.get("status") == "completed" else "âŒ"
                    logger.info(f"     {status_emoji} {stage_name}: {stage_info.get('status')}")

            return {
                "pdf_name": pdf_name,
                "status": "success",
                "doc_id": result.get("doc_id"),
                "index_path": result.get("index_path"),
                "message": "ç´¢å¼•å®Œæˆ"
            }
        else:
            error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
            logger.error(f"âŒ æ–‡æ¡£ç´¢å¼•å¤±è´¥: {pdf_name}")
            logger.error(f"   - é”™è¯¯: {error_msg}")

            return {
                "pdf_name": pdf_name,
                "status": "error",
                "message": error_msg
            }

    except Exception as e:
        logger.error(f"âŒ å¤„ç†æ–‡æ¡£æ—¶å‘ç”Ÿå¼‚å¸¸: {pdf_name}")
        logger.error(f"   - å¼‚å¸¸: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())

        return {
            "pdf_name": pdf_name,
            "status": "error",
            "message": str(e)
        }


async def batch_process_pdfs(
    pdf_name_list: List[str],
    provider: str = "openai",
    pdf_preset: str = "high",
    force_reindex: bool = False
):
    """
    æ‰¹é‡å¤„ç†PDFæ–‡æ¡£åˆ—è¡¨

    Args:
        pdf_name_list: PDFæ–‡ä»¶ååˆ—è¡¨ï¼ˆä¸å«æ‰©å±•åï¼‰
        provider: LLMæä¾›å•† ('openai', 'azure', 'ollama')
        pdf_preset: PDFè½¬å›¾ç‰‡è´¨é‡é¢„è®¾ ('fast', 'balanced', 'high', 'ultra')
        force_reindex: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç´¢å¼•å·²æ³¨å†Œçš„æ–‡æ¡£
    """
    print_section(f"æ‰¹é‡å¤„ç†PDFæ–‡æ¡£ - å…± {len(pdf_name_list)} ä¸ª")

    # åˆå§‹åŒ–IndexingAgent
    logger.info(f"ğŸ”§ åˆå§‹åŒ–IndexingAgent (provider={provider}, pdf_preset={pdf_preset})...")
    indexing_agent = IndexingAgent(provider=provider, pdf_preset=pdf_preset)

    # åˆå§‹åŒ–DocumentRegistry
    logger.info(f"ğŸ“‹ åˆå§‹åŒ–DocumentRegistry...")
    doc_registry = indexing_agent.doc_registry

    # æ˜¾ç¤ºå½“å‰æ³¨å†Œè¡¨ç»Ÿè®¡
    stats = doc_registry.get_statistics()
    logger.info(f"ğŸ“Š å½“å‰æ³¨å†Œè¡¨ç»Ÿè®¡:")
    logger.info(f"   - æ€»æ–‡æ¡£æ•°: {stats['total_documents']}")
    logger.info(f"   - æŒ‰ç±»å‹åˆ†å¸ƒ: {stats['by_type']}")

    # å¤„ç†ç»“æœç»Ÿè®¡
    results = {
        "success": [],
        "skipped": [],
        "error": []
    }

    # é€ä¸ªå¤„ç†PDF
    for idx, pdf_name in enumerate(pdf_name_list, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"è¿›åº¦: {idx}/{len(pdf_name_list)}")
        logger.info(f"{'='*80}")

        result = await process_single_pdf(
            indexing_agent,
            doc_registry,
            pdf_name,
            force_reindex
        )

        # ç»Ÿè®¡ç»“æœ
        status = result["status"]
        if status == "success":
            results["success"].append(result)
        elif status == "skipped":
            results["skipped"].append(result)
        else:
            results["error"].append(result)

    # æ‰“å°æœ€ç»ˆç»Ÿè®¡
    print_section("å¤„ç†å®Œæˆ - ç»Ÿè®¡æŠ¥å‘Š")

    logger.info(f"âœ… æˆåŠŸç´¢å¼•: {len(results['success'])} ä¸ª")
    for r in results['success']:
        logger.info(f"   - {r['pdf_name']}")

    logger.info(f"\nâ­ï¸  è·³è¿‡ï¼ˆå·²æ³¨å†Œï¼‰: {len(results['skipped'])} ä¸ª")
    for r in results['skipped']:
        logger.info(f"   - {r['pdf_name']}")

    logger.info(f"\nâŒ å¤±è´¥: {len(results['error'])} ä¸ª")
    for r in results['error']:
        logger.info(f"   - {r['pdf_name']}: {r['message']}")

    # æœ€ç»ˆæ³¨å†Œè¡¨ç»Ÿè®¡
    final_stats = doc_registry.get_statistics()
    logger.info(f"\nğŸ“Š æœ€ç»ˆæ³¨å†Œè¡¨ç»Ÿè®¡:")
    logger.info(f"   - æ€»æ–‡æ¡£æ•°: {final_stats['total_documents']}")
    logger.info(f"   - æŒ‰ç±»å‹åˆ†å¸ƒ: {final_stats['by_type']}")

    return results


def get_pdf_list_from_user() -> List[str]:
    """
    ä»ç”¨æˆ·è¾“å…¥è·å–PDFåˆ—è¡¨

    Returns:
        List[str]: PDFåç§°åˆ—è¡¨ï¼ˆä¸å«.pdfæ‰©å±•åï¼‰
    """
    print("\n" + "="*80)
    print("  è¯·è¾“å…¥è¦å¤„ç†çš„PDFæ–‡ä»¶å")
    print("="*80)
    print(f"\nPDFæ–‡ä»¶åº”å­˜æ”¾åœ¨: {PDF_PATH}")
    print("\nä½¿ç”¨è¯´æ˜:")
    print("  - è¾“å…¥PDFæ–‡ä»¶åï¼ˆä¸å«.pdfæ‰©å±•åï¼‰")
    print("  - å¤šä¸ªæ–‡ä»¶ç”¨é€—å·åˆ†éš”")
    print("  - ä¾‹å¦‚: document1, document2, research_paper")
    print("  - è¾“å…¥ 'q' æˆ– 'quit' é€€å‡º\n")

    try:
        user_input = input("è¯·è¾“å…¥PDFåç§°: ").strip()

        # æ£€æŸ¥é€€å‡ºå‘½ä»¤
        if user_input.lower() in ['q', 'quit', 'exit', 'é€€å‡º']:
            logger.info("ç”¨æˆ·å–æ¶ˆæ“ä½œ")
            return []

        # æ£€æŸ¥ç©ºè¾“å…¥
        if not user_input:
            logger.warning("è¾“å…¥ä¸ºç©ºï¼Œè¯·é‡æ–°è¿è¡Œå¹¶è¾“å…¥PDFåç§°")
            return []

        # è§£æé€—å·åˆ†éš”çš„PDFåç§°
        pdf_names = [name.strip() for name in user_input.split(',')]
        # è¿‡æ»¤æ‰ç©ºå­—ç¬¦ä¸²
        pdf_names = [name for name in pdf_names if name]

        if not pdf_names:
            logger.warning("æœªè¯†åˆ«åˆ°æœ‰æ•ˆçš„PDFåç§°")
            return []

        logger.info(f"âœ… å·²è¯†åˆ« {len(pdf_names)} ä¸ªPDF:")
        for name in pdf_names:
            logger.info(f"   - {name}")

        return pdf_names

    except EOFError:
        logger.warning("\næ£€æµ‹åˆ°EOFï¼Œä½¿ç”¨ç©ºåˆ—è¡¨")
        return []
    except KeyboardInterrupt:
        logger.warning("\nç”¨æˆ·ä¸­æ–­è¾“å…¥")
        return []


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print_section("IndexingAgent æµ‹è¯•")

    # ==================== é…ç½®æµ‹è¯•å‚æ•° ====================

    # ğŸ“ åœ¨è¿™é‡Œé…ç½®è¦æµ‹è¯•çš„PDFåˆ—è¡¨ï¼ˆä¸å«.pdfæ‰©å±•åï¼‰
    pdf_name_list = [
        "1706.03762v7"
        # "example_document_1",
        # "example_document_2",
        # "research_paper",
    ]

    # å¦‚æœåˆ—è¡¨ä¸ºç©ºï¼Œæç¤ºç”¨æˆ·è¾“å…¥
    if not pdf_name_list:
        logger.info("ğŸ’¬ PDFåˆ—è¡¨æœªé¢„è®¾ï¼Œå¯åŠ¨äº¤äº’å¼è¾“å…¥...")
        pdf_name_list = get_pdf_list_from_user()

        if not pdf_name_list:
            logger.warning("âš ï¸  æœªè·å–åˆ°PDFåˆ—è¡¨ï¼Œé€€å‡ºæµ‹è¯•")
            logger.info("\næç¤ºï¼šä½ ä¹Ÿå¯ä»¥ç›´æ¥åœ¨ä»£ç ä¸­é…ç½®pdf_name_list:")
            logger.info('    pdf_name_list = ["document1", "document2"]')
            return

    # LLMæä¾›å•†é…ç½®
    provider = "openai"  # å¯é€‰: "openai", "azure", "ollama"

    # PDFè½¬å›¾ç‰‡è´¨é‡é¢„è®¾
    pdf_preset = "high"  # å¯é€‰: "fast", "balanced", "high", "ultra"

    # æ˜¯å¦å¼ºåˆ¶é‡æ–°ç´¢å¼•å·²æ³¨å†Œçš„æ–‡æ¡£
    force_reindex = False

    # ==================== æ‰§è¡Œæµ‹è¯• ====================

    logger.info("ğŸ“‹ æµ‹è¯•é…ç½®:")
    logger.info(f"   - PDFåˆ—è¡¨: {pdf_name_list}")
    logger.info(f"   - LLM Provider: {provider}")
    logger.info(f"   - PDFè´¨é‡é¢„è®¾: {pdf_preset}")
    logger.info(f"   - å¼ºåˆ¶é‡æ–°ç´¢å¼•: {force_reindex}")
    logger.info(f"   - PDFç›®å½•: {PDF_PATH}")

    # è¿è¡Œå¼‚æ­¥æ‰¹å¤„ç†
    try:
        results = asyncio.run(batch_process_pdfs(
            pdf_name_list=pdf_name_list,
            provider=provider,
            pdf_preset=pdf_preset,
            force_reindex=force_reindex
        ))

        print_section("æµ‹è¯•å®Œæˆ")
        logger.info("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        logger.info(f"ğŸ“Š å¤„ç†ç»“æœ: æˆåŠŸ {len(results['success'])} | è·³è¿‡ {len(results['skipped'])} | å¤±è´¥ {len(results['error'])}")

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
    except Exception as e:
        logger.error(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸: {e}")
        import traceback
        logger.error(traceback.format_exc())


if __name__ == "__main__":
    main()
