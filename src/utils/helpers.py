import fitz  # å¯¼å…¥pymupdfåº“ï¼Œå®ƒåœ¨å¯¼å…¥æ—¶åˆ«åä¸ºfitz
import json
import os
import unicodedata
import re
import logging
from typing import List, Optional, Dict, Any, Union

# å¯¼å…¥æ–°çš„å·¥å…·åº“
from .file_operations import SafeFileOperations, AdvancedFileOperations
from .error_handler import retry_on_error, safe_execute, error_context
from .exceptions import FileProcessingError

logger = logging.getLogger(__name__)

def list_pdf_files(folder_path: str = "data/pdf") -> List[str]:
    """
    è¯»å–æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰æ–‡ä»¶ï¼Œè¿”å›æ–‡ä»¶ååˆ—è¡¨

    Args:
        folder_path (str): ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º"data/pdf"

    Returns:
        List[str]: æ–‡ä»¶ååˆ—è¡¨
    """
    if not os.path.exists(folder_path):
        logging.warning(f"æ–‡ä»¶å¤¹ä¸å­˜åœ¨: {folder_path}")
        return []
    return [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]

@retry_on_error(max_retries=3, exceptions=(IOError, OSError))
def load_json_file(file_path: str) -> Optional[Union[Dict[str, Any], List[Any]]]:
    """
    è¯»å–JSONæ–‡ä»¶å¹¶è¿”å›è§£æåçš„æ•°æ®

    ä½¿ç”¨æ–°çš„SafeFileOperationsè¿›è¡Œå®‰å…¨æ–‡ä»¶è¯»å–

    Args:
        file_path (str): JSONæ–‡ä»¶çš„è·¯å¾„

    Returns:
        Optional[Union[Dict[str, Any], List[Any]]]: è§£æåçš„JSONæ•°æ®ï¼Œå¦‚æœå‡ºé”™åˆ™è¿”å›None
    """
    return safe_execute(
        SafeFileOperations.read_json_file,
        file_path,
        default_value=None,
        context=f"åŠ è½½JSONæ–‡ä»¶: {file_path}",
        log_errors=True
    )

@retry_on_error(max_retries=3, exceptions=(IOError, OSError))
def load_md_file(file_path: str) -> Optional[str]:
    """
    è¯»å–æœ¬åœ°Markdownæ–‡ä»¶çš„å†…å®¹

    ä½¿ç”¨æ–°çš„AdvancedFileOperationsè¿›è¡Œç¼–ç æ£€æµ‹å’Œå®‰å…¨è¯»å–

    Args:
        file_path (str): Markdownæ–‡ä»¶çš„è·¯å¾„

    Returns:
        Optional[str]: æ–‡ä»¶å†…å®¹ï¼Œå¦‚æœå‡ºé”™åˆ™è¿”å›None
    """
    return safe_execute(
        AdvancedFileOperations.read_file_with_encoding_detection,
        file_path,
        default_value=None,
        context=f"åŠ è½½Markdownæ–‡ä»¶: {file_path}",
        log_errors=True
    )

def read_images_in_directory(directory_path: str) -> List[str]:
    """
    è¯»å–æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰æ”¯æŒæ ¼å¼çš„å›¾ç‰‡æ–‡ä»¶è·¯å¾„ã€‚
    Read all supported image files in a directory.
    Args:
        directory_path (str): ç›®å½•è·¯å¾„ã€‚
    Returns:
        List[str]: å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚
    """
    # å¯¼å…¥å¸¸é‡é…ç½®
    from src.config.constants import PDFConstants

    image_files = []
    valid_image_extensions = PDFConstants.SUPPORTED_IMAGE_EXTENSIONS
    for root, dirs, files in os.walk(directory_path):
        for file in files:
            file_extension = os.path.splitext(file)[1].lower()
            if file_extension in valid_image_extensions:
                image_path = os.path.join(root, file)
                image_files.append(image_path)
    logger.info(f"è¯»å–åˆ°{len(image_files)}å¼ å›¾ç‰‡ in {directory_path}")
    return image_files

def makedir(path: str) -> None:
    """
    åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰

    Args:
        path (str): è¦åˆ›å»ºçš„ç›®å½•è·¯å¾„

    Returns:
        None
    """
    try:
        if not os.path.exists(path):
            os.makedirs(path, exist_ok=True)
            logger.info(f"ç›®å½•å·²åˆ›å»º: {path}")
        else:
            logger.debug(f"ç›®å½•å·²å­˜åœ¨: {path}")
    except OSError as e:
        logger.error(f"åˆ›å»ºç›®å½•å¤±è´¥ {path}: {e}")
        raise
 
def get_pdf_name(file_name: str) -> str:
    """
    è·å–å»é™¤æ‰©å±•ååçš„æ–‡ä»¶åã€‚
    Get file name without extension.
    Args:
        file_name (str): æ–‡ä»¶åã€‚
    Returns:
        str: å»é™¤æ‰©å±•ååçš„æ–‡ä»¶åã€‚
    """
    dot_index = file_name.rfind('.')
    if dot_index != -1:
        file_name_without_ext = file_name[:dot_index]
        logger.debug(f"PDFæ–‡ä»¶åå»åç¼€: {file_name_without_ext}")
    else:
        file_name_without_ext = file_name
        logger.debug(f"PDFæ–‡ä»¶åæ— åç¼€: {file_name}")
    return file_name_without_ext

@retry_on_error(max_retries=2, exceptions=(Exception,))
def pdf_to_images(pdf_path: str, output_folder: str, dpi: int = 300, quality: str = "high") -> Dict[str, Any]:
    """
    å°† PDF æ–‡ä»¶æ¯ä¸€é¡µè½¬æ¢ä¸ºé«˜è´¨é‡å›¾ç‰‡å¹¶ä¿å­˜åˆ°æŒ‡å®šæ–‡ä»¶å¤¹ã€‚

    å¢å¼ºåŠŸèƒ½ï¼š
    - æ·»åŠ é”™è¯¯é‡è¯•æœºåˆ¶
    - æ”¹è¿›å†…å­˜ç®¡ç†
    - è¿”å›è½¬æ¢ç»Ÿè®¡ä¿¡æ¯
    - æ·»åŠ è¿›åº¦è·Ÿè¸ª

    Args:
        pdf_path (str): PDF æ–‡ä»¶è·¯å¾„
        output_folder (str): å›¾ç‰‡ä¿å­˜æ–‡ä»¶å¤¹
        dpi (int): å›¾ç‰‡åˆ†è¾¨ç‡ï¼Œé»˜è®¤300 DPIã€‚å»ºè®®å€¼ï¼š150(å¿«é€Ÿ), 300(é«˜è´¨é‡), 600(è¶…é«˜è´¨é‡)
        quality (str): å›¾ç‰‡è´¨é‡çº§åˆ« - "low"(2x), "medium"(3x), "high"(4x), "ultra"(5x)

    Returns:
        Dict[str, Any]: è½¬æ¢ç»“æœç»Ÿè®¡ä¿¡æ¯

    Raises:
        FileProcessingError: PDFå¤„ç†å¤±è´¥
    """
    import time
    start_time = time.time()

    try:
        # éªŒè¯è¾“å…¥æ–‡ä»¶
        if not os.path.exists(pdf_path):
            raise FileProcessingError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}", "FILE_NOT_FOUND")

        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        logger.debug(f"å‡†å¤‡åˆ›å»ºè¾“å‡ºç›®å½•: {output_folder}")
        try:
            # ä½¿ç”¨ is_directory=True å‚æ•°æ˜ç¡®æŒ‡å®šè¿™æ˜¯ç›®å½•è·¯å¾„
            SafeFileOperations.ensure_directory(output_folder)
            logger.info(f"è¾“å‡ºç›®å½•å·²å‡†å¤‡: {output_folder}")
        except Exception as e:
            logger.error(f"åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {output_folder}, é”™è¯¯: {e}")
            # å°è¯•ç›´æ¥ä½¿ç”¨os.makedirsä½œä¸ºå¤‡ç”¨æ–¹æ¡ˆ
            try:
                os.makedirs(output_folder, exist_ok=True)
                logger.info(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ³•åˆ›å»ºç›®å½•æˆåŠŸ: {output_folder}")
            except Exception as backup_e:
                raise FileProcessingError(f"æ— æ³•åˆ›å»ºè¾“å‡ºç›®å½•: {backup_e}", "DIRECTORY_CREATION_ERROR")

        # å¯¼å…¥å¸¸é‡é…ç½®
        from src.config.constants import PDFConstants

        # è®¡ç®—ç¼©æ”¾å› å­
        dpi_scale = dpi / 72.0
        quality_scale_factor = PDFConstants.QUALITY_LEVELS.get(quality, {}).get("scale", 4.0)
        final_scale = max(dpi_scale, quality_scale_factor)

        logger.info(f"å¼€å§‹PDFè½¬å›¾ç‰‡: {pdf_path}")
        logger.info(f"å‚æ•°: DPI={dpi}, è´¨é‡={quality}, ç¼©æ”¾={final_scale:.1f}x")

        # è½¬æ¢ç»Ÿè®¡
        conversion_stats = {
            "total_pages": 0,
            "successful_pages": 0,
            "failed_pages": 0,
            "total_size_mb": 0,
            "processing_time": 0,
            "output_files": []
        }

        doc = None
        try:
            doc = fitz.open(pdf_path)
            conversion_stats["total_pages"] = doc.page_count

            for page_num in range(doc.page_count):
                try:
                    page = doc[page_num]

                    # åˆ›å»ºå˜æ¢çŸ©é˜µ
                    matrix = fitz.Matrix(final_scale, final_scale)

                    # è·å–pixmapå¹¶ç«‹å³å¤„ç†
                    pix = page.get_pixmap(
                        matrix=matrix,
                        alpha=False,
                        annots=True,
                        clip=None
                    )

                    # æ„å»ºè¾“å‡ºè·¯å¾„
                    image_path = os.path.join(output_folder, f"page_{page_num + 1}.png")

                    # ä¿å­˜å›¾ç‰‡
                    pix.save(image_path)

                    # è®°å½•æ–‡ä»¶ä¿¡æ¯
                    file_size = os.path.getsize(image_path)
                    conversion_stats["total_size_mb"] += file_size / (1024 * 1024)
                    conversion_stats["output_files"].append({
                        "page": page_num + 1,
                        "path": image_path,
                        "size_mb": file_size / (1024 * 1024),
                        "dimensions": f"{pix.width}x{pix.height}"
                    })

                    conversion_stats["successful_pages"] += 1

                    # ç«‹å³é‡Šæ”¾å†…å­˜
                    pix = None
                    page = None

                    # æ¯10é¡µè®°å½•ä¸€æ¬¡è¿›åº¦
                    if (page_num + 1) % 10 == 0:
                        logger.info(f"å·²å¤„ç† {page_num + 1}/{doc.page_count} é¡µ")

                except Exception as e:
                    conversion_stats["failed_pages"] += 1
                    logger.error(f"ç¬¬{page_num + 1}é¡µè½¬æ¢å¤±è´¥: {e}")
                    continue

        finally:
            # ç¡®ä¿æ–‡æ¡£è¢«å…³é—­
            if doc:
                doc.close()
                doc = None

        # è®¡ç®—å¤„ç†æ—¶é—´
        conversion_stats["processing_time"] = time.time() - start_time

        logger.info(f"PDFè½¬æ¢å®Œæˆ: æˆåŠŸ{conversion_stats['successful_pages']}é¡µ, "
                   f"å¤±è´¥{conversion_stats['failed_pages']}é¡µ, "
                   f"æ€»å¤§å°{conversion_stats['total_size_mb']:.2f}MB, "
                   f"è€—æ—¶{conversion_stats['processing_time']:.2f}ç§’")

        return conversion_stats

    except Exception as e:
        raise FileProcessingError(f"PDFè½¬å›¾ç‰‡å¤±è´¥: {e}", "PDF_CONVERSION_ERROR")

def extract_page_num(path: str) -> Optional[str]:
    """
    ä»å›¾ç‰‡è·¯å¾„ä¸­æå–é¡µç æ•°å­—ã€‚
    Extract page number from image file path.
    Args:
        path (str): å›¾ç‰‡æ–‡ä»¶è·¯å¾„ã€‚
    Returns:
        Optional[str]: æå–åˆ°çš„é¡µç æ•°å­—ï¼Œæœªæ‰¾åˆ°åˆ™ä¸º Noneã€‚
    """
    file_name = os.path.basename(path)
    pattern = r'\d+'
    match = re.search(pattern, file_name)

    if match:
        number = match.group(0)
        logger.debug(f"Extracted page number {number} from {file_name}")
        return number
    else:
        logger.warning(f"æœªæ‰¾åˆ°æ•°å­— in {file_name}ã€‚")
        return None

def extract_data_from_LLM_res(content: str) -> Optional[Dict[str, Any]]:
    """
    ä»LLMå“åº”ä¸­æå–JSONæ•°æ®ï¼Œå®‰å…¨åœ°è§£æå„ç§æ ¼å¼

    Args:
        content: LLMå“åº”å†…å®¹ï¼Œå¯èƒ½åŒ…å«JSONæ ¼å¼æˆ–markdownä»£ç å—ä¸­çš„JSON

    Returns:
        dict: è§£æåçš„æ•°æ®ï¼Œå¤±è´¥æ—¶è¿”å›None
    """
    data = None

    # å¦‚æœè¾“å…¥ä¸ºç©ºæˆ–Noneï¼Œç›´æ¥è¿”å›None
    if not content:
        logger.warning("extract_data_from_LLM_res: è¾“å…¥å†…å®¹ä¸ºç©º")
        return None

    try:
        # é¦–å…ˆå°è¯•ç›´æ¥è§£æJSON
        data = json.loads(content)
        logger.debug("æˆåŠŸç›´æ¥è§£æJSONå†…å®¹")
        return data
    except json.JSONDecodeError as e:
        logger.debug(f"ç›´æ¥JSONè§£æå¤±è´¥: {e}ï¼Œå°è¯•ä»markdownä»£ç å—ä¸­æå–")

        # å°è¯•ä»markdownä»£ç å—ä¸­æå–JSON
        # æ”¯æŒå¤šç§å¯èƒ½çš„ä»£ç å—æ ¼å¼
        patterns = [
            r'```json\n(.*?)```',      # æ ‡å‡†jsonä»£ç å—
            r'```\n(.*?)```',          # æ™®é€šä»£ç å—
            r'`([^`]*)`',              # å•è¡Œä»£ç å—
        ]

        for pattern in patterns:
            try:
                match = re.search(pattern, content, re.DOTALL)
                if match:
                    json_str = match.group(1).strip()
                    logger.debug(f"ä»ä»£ç å—ä¸­æå–åˆ°å†…å®¹: {json_str[:100]}...")

                    # å®‰å…¨åœ°å°è¯•è§£ææå–çš„JSONå­—ç¬¦ä¸²
                    data = json.loads(json_str)
                    logger.debug("æˆåŠŸä»ä»£ç å—ä¸­è§£æJSON")
                    return data

            except json.JSONDecodeError as parse_error:
                logger.warning(f"ä»£ç å—JSONè§£æå¤±è´¥ (pattern: {pattern}): {parse_error}")
                continue
            except Exception as unexpected_error:
                logger.error(f"æå–JSONæ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {unexpected_error}")
                continue

        # å¦‚æœæ‰€æœ‰æ–¹æ³•éƒ½å¤±è´¥äº†ï¼Œè®°å½•è­¦å‘Šå¹¶è¿”å›None
        logger.warning("æ— æ³•ä»å†…å®¹ä¸­æå–æœ‰æ•ˆçš„JSONæ•°æ®")
        logger.debug(f"åŸå§‹å†…å®¹: {content[:200]}...")

    except Exception as e:
        logger.error(f"extract_data_from_LLM_reså‘ç”Ÿæ„å¤–é”™è¯¯: {e}")

    return None

def parse_latest_plugin_call(text: str) -> tuple[str, str, str]:  
    i = text.rfind('\nAction:')  
    j = text.rfind('\nAction Input:')  
    k = text.rfind('\nObservation:')  
    final_answer_index = text.rfind('\nFinal Answer:')  
  
    if 0 <= i < j:  # If the text has `Action` and `Action input`,  
        if k < j:  # but does not contain `Observation`,  
            # then it is likely that `Observation` is omitted by the LLM,  
            # because the output text may have discarded the stop word.  
            text = text.rstrip() + '\nObservation:'  # Add it back.  
            k = text.rfind('\nObservation:')  
  
    plugin_name, plugin_args, final_answer = '', '', ''  
  
    if 0 <= i < j < k:  
        plugin_name = text[i + len('\nAction:'):j].strip()  
        plugin_args = text[j + len('\nAction Input:'):k].strip()  
  
    if final_answer_index != -1:  
        final_answer = text[final_answer_index + len('\nFinal Answer:'):].strip()  
  
    return plugin_name, plugin_args, final_answer  

def full_to_half(text: str) -> str:
    """
    å°†å…¨è§’å­—ç¬¦è½¬æ¢ä¸ºåŠè§’

    Args:
        text (str): è¾“å…¥çš„æ–‡æœ¬å­—ç¬¦ä¸²

    Returns:
        str: è½¬æ¢åçš„åŠè§’å­—ç¬¦ä¸²
    """
    normalized = []
    for char in text:
        # å…¨è§’è½¬åŠè§’
        if unicodedata.east_asian_width(char) == 'F':
            normalized_char = unicodedata.normalize('NFKC', char)
            normalized.append(normalized_char)
        else:
            normalized.append(char)
    return ''.join(normalized)

def normalize_chapter(name: str) -> str:
    """
    æ ‡å‡†åŒ–ç« èŠ‚åç§°

    Args:
        name (str): åŸå§‹ç« èŠ‚åç§°

    Returns:
        str: æ ‡å‡†åŒ–åçš„ç« èŠ‚åç§°
    """
    # 1. å…¨è§’è½¬åŠè§’
    name = full_to_half(name)
    # 2. ç§»é™¤æ‰€æœ‰æ ‡ç‚¹å’Œç©ºç™½
    name = re.sub(r'[^\w\u4e00-\u9fa5]', '', name)
    # 3. è½¬ä¸ºå°å†™ï¼ˆå¦‚æœæœ‰è‹±æ–‡ï¼‰
    return name.lower()

def deduplicate_by_title(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    æ ¹æ®æ ‡é¢˜å»é‡æ•°æ®é¡¹

    Args:
        data (List[Dict[str, Any]]): åŒ…å«æ ‡é¢˜çš„æ•°æ®é¡¹åˆ—è¡¨

    Returns:
        List[Dict[str, Any]]: å»é‡åçš„æ•°æ®é¡¹åˆ—è¡¨
    """
    seen = set()
    result = []
    for item in data:
        title = normalize_chapter(item.get('title', ''))
        if title not in seen:
            seen.add(title)
            result.append(item)
    return result

def group_data_by_sections_with_titles(total_sections: List[Dict[str, Any]], raw_data: List[Dict[str, Any]]) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    æ ¹æ®ç« èŠ‚ä¿¡æ¯å¯¹æ•°æ®è¿›è¡Œåˆ†ç»„

    å¢å¼ºåŠŸèƒ½ï¼š
    - æ”¹è¿›é”™è¯¯å¤„ç†
    - æ·»åŠ æ•°æ®éªŒè¯
    - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
    - å¢å¼ºæ—¥å¿—è®°å½•

    Args:
        total_sections: ç« èŠ‚ä¿¡æ¯åˆ—è¡¨
        raw_data: åŸå§‹æ•°æ®åˆ—è¡¨

    Returns:
        tuple: (æ•°æ®ç»“æœ, è®®ç¨‹ç»“æœ)

    Raises:
        ValidationError: æ•°æ®éªŒè¯å¤±è´¥
    """
    logger.info(f"å¼€å§‹åˆ†ç»„æ•°æ®ï¼Œè¾“å…¥ç« èŠ‚æ•°: {len(total_sections)}, åŸå§‹æ•°æ®æ¡æ•°: {len(raw_data)}")

    # è¾“å…¥éªŒè¯
    if not total_sections:
        logger.warning("ç« èŠ‚åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
        return [], []

    if not raw_data:
        logger.warning("åŸå§‹æ•°æ®ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
        return [], []

    try:
        with error_context("æ•°æ®åˆ†ç»„å¤„ç†", reraise=True) as error_handler:
            # å»é‡ç« èŠ‚
            unsort_sections = deduplicate_by_title(total_sections)
            logger.info(f"å»é‡åç« èŠ‚æ•°: {len(unsort_sections)}")

            if not unsort_sections:
                logger.warning("å»é‡åç« èŠ‚åˆ—è¡¨ä¸ºç©º")
                return [], []

            # å®‰å…¨æ’åºç« èŠ‚
            sections = _safe_sort_sections(unsort_sections)
            logger.debug(f"ç« èŠ‚æ’åºå®Œæˆï¼Œå…± {len(sections)} ä¸ªç« èŠ‚")

            # æ„å»ºé¡µç åˆ°å†…å®¹çš„æ˜ å°„
            page_to_data = _build_page_mapping(raw_data)
            logger.debug(f"æ„å»ºé¡µç æ˜ å°„å®Œæˆï¼Œå…± {len(page_to_data)} é¡µ")

            # æ‰¾åˆ°æœ€å¤§é¡µç 
            max_page = max((int(item['page']) for item in raw_data), default=0)
            logger.debug(f"æœ€å¤§é¡µç : {max_page}")

            # æŒ‰ç›¸åŒé¡µç åˆ†ç»„
            groups = _group_sections_by_page(sections)
            logger.debug(f"ç« èŠ‚åˆ†ç»„å®Œæˆï¼Œå…± {len(groups)} ç»„")

            # å¤„ç†æ¯ä¸ªç»„å¹¶ç”Ÿæˆç»“æœ
            data_result, agenda_result = _process_section_groups(
                groups, page_to_data, max_page
            )

            logger.info(f"æ•°æ®åˆ†ç»„å®Œæˆ: æ•°æ®ç»“æœ {len(data_result)} é¡¹, è®®ç¨‹ç»“æœ {len(agenda_result)} é¡¹")
            return data_result, agenda_result

    except Exception as e:
        from .exceptions import ValidationError
        raise ValidationError(f"æ•°æ®åˆ†ç»„å¤±è´¥: {e}")


def _safe_sort_sections(sections: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """å®‰å…¨åœ°å¯¹ç« èŠ‚è¿›è¡Œæ’åº"""
    try:
        return sorted(
            sections,
            key=lambda x: int(x.get("page", float('inf'))) if x.get("page") else float('inf')
        )
    except (ValueError, TypeError) as e:
        logger.warning(f"ç« èŠ‚æ’åºå¤±è´¥ï¼Œä½¿ç”¨åŸå§‹é¡ºåº: {e}")
        return sections


def _build_page_mapping(raw_data: List[Dict[str, Any]]) -> Dict[int, Any]:
    """æ„å»ºé¡µç åˆ°å†…å®¹çš„æ˜ å°„"""
    page_to_data = {}

    for item in raw_data:
        try:
            page = int(item['page'])
            page_to_data[page] = item.get('data', "")
        except (ValueError, KeyError) as e:
            logger.warning(f"è·³è¿‡æ— æ•ˆæ•°æ®é¡¹: {e}")
            continue

    return page_to_data


def _group_sections_by_page(sections: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
    """æŒ‰ç›¸åŒé¡µç å¯¹ç« èŠ‚è¿›è¡Œåˆ†ç»„"""
    if not sections:
        return []

    groups = []
    current_group = [sections[0]]
    current_page = int(sections[0].get('page', 0))

    for sec in sections[1:]:
        try:
            page = int(sec.get('page', 0))
            if page == current_page:
                current_group.append(sec)
            else:
                groups.append(current_group)
                current_group = [sec]
                current_page = page
        except (ValueError, TypeError):
            logger.warning(f"è·³è¿‡é¡µç æ— æ•ˆçš„ç« èŠ‚: {sec}")
            continue

    if current_group:
        groups.append(current_group)

    return groups


def _process_section_groups(groups: List[List[Dict[str, Any]]],
                          page_to_data: Dict[int, Any],
                          max_page: int) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """å¤„ç†ç« èŠ‚ç»„å¹¶ç”Ÿæˆæœ€ç»ˆç»“æœ"""
    data_result = []
    agenda_result = []

    for group_idx, group in enumerate(groups):
        # è·å–ä¸‹ä¸€ç»„çš„èµ·å§‹é¡µç 
        if group_idx < len(groups) - 1:
            next_group_start = int(groups[group_idx + 1][0].get('page', max_page))
        else:
            next_group_start = max_page

        # å¤„ç†ç»„å†…æ¯ä¸ªç« èŠ‚
        for sec_idx, sec in enumerate(group):
            try:
                start = int(sec.get('page', 0))

                # ç¡®å®šç»“æŸé¡µç 
                if sec_idx == len(group) - 1:  # ç»„å†…æœ€åä¸€ä¸ªç« èŠ‚
                    end = next_group_start if group_idx < len(groups) - 1 else max_page
                else:
                    end = start  # ç»„å†…éæœ€åç« èŠ‚åªåŒ…å«å½“å‰é¡µ

                # ç”Ÿæˆé¡µç èŒƒå›´
                pages = [start] if start == end else list(range(start, end + 1))

                # æ”¶é›†ç« èŠ‚æ•°æ®
                section_data = {}
                for page in pages:
                    section_data[page] = page_to_data.get(page, "")

                # æ·»åŠ åˆ°ç»“æœ
                data_result.append({
                    'title': sec.get('title', f'æœªå‘½åç« èŠ‚_{len(data_result)}'),
                    'pages': pages,
                    'data': section_data
                })

                agenda_result.append({
                    'title': sec.get('title', f'æœªå‘½åç« èŠ‚_{len(agenda_result)}'),
                    'pages': pages,
                })

            except (ValueError, TypeError) as e:
                logger.error(f"å¤„ç†ç« èŠ‚æ—¶å‡ºé”™: {e}, ç« èŠ‚: {sec}")
                continue

    return data_result, agenda_result

def add_data_keep_order(total_dict: Dict[Any, Any], add_dict: Dict[Any, Any]) -> Dict[Any, Any]:
    """
    åˆå¹¶ add_dict åˆ° total_dictï¼Œè‹¥ key é‡å¤åˆ™ç”¨ add_dict çš„ value è¦†ç›–ï¼Œ
    æœ€ç»ˆè¿”å›ä¸€ä¸ª dictï¼Œkey æŒ‰ç…§æ•°å­—ä»å°åˆ°å¤§æ’åºã€‚

    Args:
        total_dict (Dict[Any, Any]): ç›®æ ‡å­—å…¸
        add_dict (Dict[Any, Any]): è¦åˆå¹¶çš„å­—å…¸

    Returns:
        Dict[Any, Any]: åˆå¹¶å¹¶æ’åºåçš„å­—å…¸
    """
    # æ›´æ–° total_dict
    total_dict.update(add_dict)
    # æŒ‰ key å‡åºæ’åºå¹¶è¿”å›æ–°çš„ dict
    return dict(sorted(total_dict.items(), key=lambda x: x[0]))

def is_file_exists(file_path: str) -> bool:
    """
    åˆ¤æ–­æŒ‡å®šè·¯å¾„ä¸‹çš„æ–‡ä»¶æ˜¯å¦å­˜åœ¨

    å‚æ•°:
        file_path (str): å®Œæ•´çš„æ–‡ä»¶è·¯å¾„ï¼ŒåŒ…æ‹¬æ–‡ä»¶åå’Œæ‰©å±•å

    è¿”å›å€¼:
        bool: å¦‚æœæ–‡ä»¶å­˜åœ¨ä¸”æ˜¯ä¸€ä¸ªæ–‡ä»¶ï¼ˆä¸æ˜¯ç›®å½•ï¼‰åˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False

    å¼‚å¸¸å¤„ç†:
        æ•è·å¹¶å¤„ç†è·¯å¾„è§£æè¿‡ç¨‹ä¸­å¯èƒ½å‡ºç°çš„å¼‚å¸¸ï¼ˆå¦‚æƒé™é—®é¢˜ã€æ— æ•ˆè·¯å¾„ç­‰ï¼‰
    """
    try:
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ä¸”æ˜¯ä¸€ä¸ªæ–‡ä»¶
        if os.path.exists(file_path) and os.path.isfile(file_path):
            logger.info(f"æ–‡ä»¶å­˜åœ¨: {file_path}")
            return True
        else:
            # è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶
            if not os.path.exists(file_path):
                logger.warning(f"è·¯å¾„ä¸å­˜åœ¨: {file_path}")
            else:
                logger.warning(f"è·¯å¾„æŒ‡å‘çš„æ˜¯ç›®å½•è€Œéæ–‡ä»¶: {file_path}")
            return False
    except Exception as e:
        # å¤„ç†å…¶ä»–å¯èƒ½çš„å¼‚å¸¸ï¼ˆå¦‚æƒé™é”™è¯¯ã€è·¯å¾„æ ¼å¼é”™è¯¯ç­‰ï¼‰
        logger.error(f"æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
        return False

def extract_name_from_url(url: str) -> str:
    """
    ä»URLä¸­æå–å¯èƒ½ä½œä¸ºnameçš„å…³é”®ä¿¡æ¯ï¼Œè§„èŒƒåŒ–å¤„ç†ä»¥ç¡®ä¿æ–‡ä»¶ç³»ç»Ÿå…¼å®¹

    è§„èŒƒåŒ–è§„åˆ™ï¼š
    1. å»é™¤æŸ¥è¯¢å‚æ•°ï¼ˆ? åŠå…¶åçš„å†…å®¹ï¼‰
    2. å»é™¤é”šç‚¹ï¼ˆ# åŠå…¶åçš„å†…å®¹ï¼‰
    3. å°†è¿å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
    4. ç§»é™¤ä¸å®‰å…¨çš„æ–‡ä»¶åå­—ç¬¦

    Args:
        url (str): å¾…æå–ä¿¡æ¯çš„URLå­—ç¬¦ä¸²

    Returns:
        str: æå–åˆ°çš„è§„èŒƒåŒ–æ–‡ä»¶åï¼ˆé€‚ç”¨äºæ‰€æœ‰æ“ä½œç³»ç»Ÿï¼‰
    """
    import re
    
    # å»é™¤URLä¸­çš„åè®®éƒ¨åˆ†ï¼ˆå¦‚http://ã€https://ï¼‰
    url_without_protocol = url.split('://')[-1]
    
    # å»é™¤åŸŸåéƒ¨åˆ†ï¼ˆå¦‚medium.comã€@lucknitelolç­‰ï¼‰
    path_parts = url_without_protocol.split('/')[-2:]  # é’ˆå¯¹è¯¥URLç»“æ„ï¼Œå–åŸŸååçš„è·¯å¾„éƒ¨åˆ†
    
    # æå–URLä¸­ä»¥è¿å­—ç¬¦è¿æ¥çš„å…³é”®å†…å®¹éƒ¨åˆ†
    if path_parts:
        if path_parts[-1]:
            name_part = path_parts[-1]
        else:
            name_part = path_parts[-2]

        # ğŸ”¥ è§„èŒƒåŒ–å¤„ç†ï¼š
        # 1. ç§»é™¤æŸ¥è¯¢å‚æ•°ï¼ˆ? åŠå…¶åçš„æ‰€æœ‰å†…å®¹ï¼‰
        name_part = name_part.split('?')[0]
        
        # 2. ç§»é™¤é”šç‚¹ï¼ˆ# åŠå…¶åçš„æ‰€æœ‰å†…å®¹ï¼‰
        name_part = name_part.split('#')[0]
        
        # 3. å°†è¿å­—ç¬¦æ›¿æ¢ä¸ºç©ºæ ¼
        name = name_part.replace('-', ' ')
        
        # 4. ç§»é™¤å…¶ä»–ä¸å®‰å…¨çš„æ–‡ä»¶åå­—ç¬¦ï¼ˆä¿ç•™å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼ã€ä¸‹åˆ’çº¿ã€ç‚¹ï¼‰
        name = re.sub(r'[^\w\s\.]', '', name)
        
        # 5. å»é™¤é¦–å°¾ç©ºæ ¼å¹¶å‹ç¼©å¤šä½™ç©ºæ ¼
        name = ' '.join(name.split())
        
        # 6. å¦‚æœç»“æœä¸ºç©ºï¼Œè¿”å›é»˜è®¤å€¼
        if not name:
            return "web_content"

        return name
    else:
        return "æ— æ³•ä»URLä¸­æå–æœ‰æ•ˆä¿¡æ¯"

@retry_on_error(max_retries=3, exceptions=(IOError, OSError))
def save_data(path: str, content: Union[Dict[str, Any], List[Any]]) -> None:
    """
    å°†ç»™å®šå†…å®¹ä¿å­˜ä¸ºJSONæ–‡ä»¶åˆ°æŒ‡å®šè·¯å¾„

    ä½¿ç”¨æ–°çš„SafeFileOperationsè¿›è¡Œå®‰å…¨æ–‡ä»¶å†™å…¥

    Args:
        path (str): JSONæ–‡ä»¶ä¿å­˜è·¯å¾„
        content (Union[Dict[str, Any], List[Any]]): è¦ä¿å­˜çš„å†…å®¹

    Returns:
        None

    Raises:
        FileProcessingError: æ–‡ä»¶ä¿å­˜å¤±è´¥
    """
    try:
        SafeFileOperations.write_json_file(path, content)
        logger.info(f"æ•°æ®å·²å®‰å…¨ä¿å­˜åˆ°: {path}")
    except Exception as e:
        raise FileProcessingError(f"ä¿å­˜æ•°æ®å¤±è´¥: {e}", "SAVE_DATA_ERROR")