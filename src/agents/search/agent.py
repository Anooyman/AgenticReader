"""
SearchAgent - ç½‘ç»œæœç´¢ä¸URLå†…å®¹åˆ†æAgent

æ”¯æŒä¸¤ç§ä½¿ç”¨åœºæ™¯ï¼š
1. æœç´¢å¼•æ“æ£€ç´¢ï¼šé€šè¿‡æœç´¢å¼•æ“è·å–æœ€æ–°ä¿¡æ¯
2. æŒ‡å®šURLåˆ†æï¼šåˆ†æç‰¹å®šç½‘é¡µå†…å®¹ï¼Œæ”¯æŒæ™ºèƒ½ç´¢å¼•å†³ç­–
"""

from langgraph.graph import StateGraph, END
from typing import Optional, Dict
import logging

from ..base import AgentBase
from .state import SearchState
from .tools import SearchTools
from .nodes import SearchNodes
from .utils import SearchUtils

logger = logging.getLogger(__name__)


class SearchAgent(AgentBase):
    """
    SearchAgent - ç½‘ç»œæœç´¢ä¸å†…å®¹åˆ†æAgent

    å·¥ä½œæµç¨‹ï¼š
    1. initialize â†’ åˆå§‹åŒ–ç¯å¢ƒ
    2. analyze_query â†’ åˆ†ææŸ¥è¯¢ç±»å‹ï¼ˆsearch vs url_analysisï¼‰
    3. route_by_use_case â†’ æ ¹æ®ç±»å‹è·¯ç”±

    Use Case 1 (æœç´¢å¼•æ“æ£€ç´¢):
        web_search â†’ select_urls â†’ scrape_content â†’ extract_and_merge â†’ evaluate â†’ format

    Use Case 2 (URLåˆ†æ):
        scrape_content â†’ evaluate_content_size â†’ extract_and_merge â†’ evaluate â†’ format
                                â†“ (å¦‚æœéœ€è¦ç´¢å¼•)
                        call_indexing_agent â†’ åŸºäºç´¢å¼•å¯¹è¯
    """

    def __init__(self, provider: str = 'openai', progress_callback=None):
        """
        åˆå§‹åŒ– SearchAgent

        Args:
            provider: LLM provider ("openai", "azure", "ollama")
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼ˆå¯é€‰ï¼‰
        """
        super().__init__(name="SearchAgent", provider=provider)

        # è¿›åº¦å›è°ƒå‡½æ•°
        self.progress_callback = progress_callback

        # åˆå§‹åŒ–åŠŸèƒ½æ¨¡å—ï¼ˆä½¿ç”¨ä¾èµ–æ³¨å…¥ï¼‰
        self.utils = SearchUtils(self)
        self.tools = SearchTools(self)
        self.nodes = SearchNodes(self)

        # æ„å»º workflow
        self.graph = self.build_graph()

        logger.info("âœ… SearchAgent åˆå§‹åŒ–å®Œæˆ")

    # ========== Workflow æ„å»º ==========

    def build_graph(self) -> StateGraph:
        """æ„å»ºåŒ use case workflow"""
        workflow = StateGraph(SearchState)

        # ========== æ·»åŠ èŠ‚ç‚¹ ==========

        # é€šç”¨èŠ‚ç‚¹
        workflow.add_node("initialize", self.nodes.initialize)
        workflow.add_node("analyze_query", self.nodes.analyze_query)
        workflow.add_node("extract_and_merge", self.nodes.extract_and_merge)
        workflow.add_node("evaluate_completeness", self.nodes.evaluate_completeness)
        workflow.add_node("format_answer", self.nodes.format_answer)

        # Use Case 1: æœç´¢å¼•æ“æ£€ç´¢ä¸“ç”¨èŠ‚ç‚¹
        workflow.add_node("web_search", self.nodes.web_search)
        workflow.add_node("select_urls", self.nodes.select_urls)

        # å…±ç”¨èŠ‚ç‚¹
        workflow.add_node("scrape_content", self.nodes.scrape_content)

        # Use Case 2: URLåˆ†æä¸“ç”¨èŠ‚ç‚¹
        workflow.add_node("evaluate_content_size", self.nodes.evaluate_content_size)

        # ========== æ·»åŠ è¾¹ ==========

        # åˆå§‹åŒ–æµç¨‹
        workflow.add_edge("initialize", "analyze_query")

        # æ ¹æ® use case è·¯ç”±
        workflow.add_conditional_edges(
            "analyze_query",
            self.nodes.route_by_use_case,
            {
                "search": "web_search",        # Use Case 1: æœç´¢å¼•æ“æ£€ç´¢
                "url_analysis": "scrape_content"  # Use Case 2: URLåˆ†æ
            }
        )

        # ========== Use Case 1 è·¯å¾„ ==========
        workflow.add_edge("web_search", "select_urls")
        workflow.add_edge("select_urls", "scrape_content")

        # ========== Use Case 2 è·¯å¾„ ==========
        # scrape_content ååˆ¤æ–­ï¼š
        # - å¦‚æœæ˜¯ search æ¨¡å¼ â†’ extract_and_merge
        # - å¦‚æœæ˜¯ url_analysis æ¨¡å¼ â†’ evaluate_content_size
        workflow.add_conditional_edges(
            "scrape_content",
            self._route_after_scrape,
            {
                "extract": "extract_and_merge",  # search æ¨¡å¼ç›´æ¥æå–
                "evaluate_size": "evaluate_content_size"  # url_analysis éœ€è¦è¯„ä¼°å¤§å°
            }
        )

        # å†…å®¹é‡è¯„ä¼°å â†’ æå–å†…å®¹
        # NOTE: å®é™…çš„ç´¢å¼•è°ƒç”¨ä¼šåœ¨è¿™é‡Œå¤„ç†ï¼ˆæœªæ¥æ‰©å±•ï¼‰
        workflow.add_edge("evaluate_content_size", "extract_and_merge")

        # ========== é€šç”¨åç»­æµç¨‹ ==========
        workflow.add_edge("extract_and_merge", "evaluate_completeness")

        # è¯„ä¼°å®Œæ•´æ€§åå†³å®šï¼šç»§ç»­ or ç»“æŸ
        workflow.add_conditional_edges(
            "evaluate_completeness",
            self.nodes.should_continue,
            {
                "continue": "scrape_content",  # ç»§ç»­æ£€ç´¢ï¼ˆé‡æ–°çˆ¬å–ï¼‰
                "format": "format_answer"       # ç”Ÿæˆç­”æ¡ˆ
            }
        )

        # ç”Ÿæˆç­”æ¡ˆåç»“æŸ
        workflow.add_edge("format_answer", END)

        # è®¾ç½®å…¥å£
        workflow.set_entry_point("initialize")

        return workflow.compile()

    def _route_after_scrape(self, state: SearchState) -> str:
        """
        çˆ¬å–å†…å®¹åçš„è·¯ç”±é€»è¾‘

        Args:
            state: å½“å‰çŠ¶æ€

        Returns:
            ä¸‹ä¸€ä¸ªèŠ‚ç‚¹åç§°
        """
        detected_use_case = state.get('detected_use_case', 'search')

        if detected_use_case == "url_analysis":
            # URLåˆ†ææ¨¡å¼ï¼šéœ€è¦è¯„ä¼°å†…å®¹å¤§å°
            return "evaluate_size"
        else:
            # æœç´¢æ¨¡å¼ï¼šç›´æ¥æå–å†…å®¹
            return "extract"

    # ========== å…¬å…±æ¥å£ ==========

    async def search(
        self,
        query: str,
        target_urls: Optional[list] = None,
        use_case: Optional[str] = None,
        max_iterations: int = 3
    ) -> Dict:
        """
        æ‰§è¡Œæœç´¢ä»»åŠ¡

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢/é—®é¢˜
            target_urls: æŒ‡å®šURLåˆ—è¡¨ï¼ˆå¯é€‰ï¼Œç”¨äº url_analysis æ¨¡å¼ï¼‰
            use_case: ä½¿ç”¨åœºæ™¯ ("search" æˆ– "url_analysis"ï¼Œå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°

        Returns:
            åŒ…å« final_answer, sources ç­‰çš„ç»“æœå­—å…¸
        """
        logger.info(f"ğŸš€ [SearchAgent] å¼€å§‹æœç´¢ä»»åŠ¡: {query}")

        # æ„å»ºåˆå§‹çŠ¶æ€
        initial_state: SearchState = {
            "query": query,
            "max_iterations": max_iterations,
            "current_iteration": 0,
            "is_complete": False
        }

        # å¯é€‰å‚æ•°
        if target_urls:
            initial_state["target_urls"] = target_urls

        if use_case:
            initial_state["use_case"] = use_case

        try:
            # æ‰§è¡Œ workflow
            final_state = await self.graph.ainvoke(initial_state)

            # æå–ç»“æœ
            result = {
                "success": True,
                "query": query,
                "use_case": final_state.get('detected_use_case', 'unknown'),
                "answer": final_state.get('final_answer', ''),
                "sources": final_state.get('sources', []),
                "processing_strategy": final_state.get('processing_strategy', ''),
                "scraped_count": len(final_state.get('scraped_results', [])),
                "content_size": final_state.get('content_size', 0),
                "warnings": final_state.get('warnings', [])
            }

            # å¦‚æœæœ‰é”™è¯¯
            if 'error' in final_state:
                result['success'] = False
                result['error'] = final_state['error']

            logger.info("âœ… [SearchAgent] æœç´¢ä»»åŠ¡å®Œæˆ")
            logger.info(f"   - Use Case: {result['use_case']}")
            logger.info(f"   - ç­”æ¡ˆé•¿åº¦: {len(result['answer'])} å­—ç¬¦")
            logger.info(f"   - æ¥æºæ•°: {len(result['sources'])}")

            return result

        except Exception as e:
            logger.error(f"âŒ [SearchAgent] æœç´¢ä»»åŠ¡å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "query": query,
                "error": str(e),
                "answer": "",
                "sources": []
            }

        finally:
            # æ¸…ç† MCP clients
            await self.utils.cleanup_mcp_clients()

    async def analyze_url(
        self,
        url: str,
        question: Optional[str] = None,
        auto_index: bool = True
    ) -> Dict:
        """
        åˆ†æå•ä¸ªURLçš„å†…å®¹

        Args:
            url: ç›®æ ‡URL
            question: ç”¨æˆ·é—®é¢˜ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä¸º"æ€»ç»“è¿™ä¸ªç½‘é¡µçš„å†…å®¹"ï¼‰
            auto_index: æ˜¯å¦è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦ç´¢å¼•ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            åˆ†æç»“æœ
        """
        query = question or "æ€»ç»“è¿™ä¸ªç½‘é¡µçš„å†…å®¹"

        result = await self.search(
            query=query,
            target_urls=[url],
            use_case="url_analysis",
            max_iterations=1
        )

        return result

    # ========== è¾…åŠ©æ–¹æ³• ==========

    async def call_indexing_agent(
        self,
        content: Optional[str] = None,
        source_url: str = "",
        doc_name: Optional[str] = None,
        json_path: Optional[str] = None
    ) -> Dict:
        """
        è°ƒç”¨ IndexingAgent å¯¹å†…å®¹è¿›è¡Œç´¢å¼•

        è¿™ä¸ªæ–¹æ³•å°†åœ¨ Use Case 2 ä¸­ä½¿ç”¨ï¼Œå½“å†…å®¹é‡è¶…è¿‡é˜ˆå€¼æ—¶è°ƒç”¨ã€‚

        Args:
            content: è¦ç´¢å¼•çš„æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœ json_path æœªæä¾›ï¼‰
            source_url: å†…å®¹æ¥æºURL
            doc_name: æ–‡æ¡£åç§°ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨URLç”Ÿæˆï¼‰
            json_path: JSON æ–‡ä»¶è·¯å¾„ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼Œå¦‚æœæä¾›ï¼‰

        Returns:
            ç´¢å¼•ç»“æœ
        """
        logger.info("ğŸ“š [CallIndexingAgent] å‡†å¤‡è°ƒç”¨ IndexingAgent...")

        try:
            from ..indexing import IndexingAgent
            import tempfile
            import os
            import json

            # å¦‚æœæä¾›äº† JSON è·¯å¾„ï¼Œä» JSON è¯»å–å†…å®¹
            if json_path and os.path.exists(json_path):
                logger.info(f"ğŸ“„ [CallIndexingAgent] ä» JSON æ–‡ä»¶è¯»å–å†…å®¹: {json_path}")
                with open(json_path, 'r', encoding='utf-8') as f:
                    web_data = json.load(f)

                content = web_data.get('content', {}).get('text', '')
                if not source_url:
                    source_url = web_data.get('url', '')

                logger.info(f"   - å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")

            # å¦‚æœæ²¡æœ‰å†…å®¹ï¼ŒæŠ¥é”™
            if not content:
                raise ValueError("æ²¡æœ‰å¯ç´¢å¼•çš„å†…å®¹")

            # ç”Ÿæˆæ–‡æ¡£åç§°
            if not doc_name and source_url:
                doc_name = self.utils.generate_doc_name_from_url(source_url)

            logger.info(f"ğŸ“š [CallIndexingAgent] æ–‡æ¡£å: {doc_name}")

            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶ä¿å­˜å†…å®¹
            temp_file = tempfile.NamedTemporaryFile(
                mode='w',
                encoding='utf-8',
                suffix='.txt',
                delete=False
            )

            try:
                temp_file.write(content)
                temp_file.close()

                # è°ƒç”¨ IndexingAgent
                indexing_agent = IndexingAgent(provider=self.provider)

                # æ‰§è¡Œç´¢å¼•
                # æ³¨æ„ï¼šIndexingAgent çš„ process æ–¹æ³•éœ€è¦ pdf_path å’Œ pdf_name
                # ä½†æˆ‘ä»¬æ˜¯ web å†…å®¹ï¼Œæ‰€ä»¥éœ€è¦é€‚é…
                # æš‚æ—¶ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                logger.info(f"ğŸ”„ [CallIndexingAgent] å¼€å§‹ç´¢å¼•æ–‡æ¡£...")

                # TODO: è¿™é‡Œéœ€è¦è°ƒç”¨ IndexingAgent çš„æ­£ç¡®æ–¹æ³•
                # ç°åœ¨æš‚æ—¶è¿”å›å ä½ç¬¦ï¼Œç­‰å¾…å®é™…é›†æˆ
                logger.warning("âš ï¸  [CallIndexingAgent] IndexingAgent é›†æˆå°šæœªå®Œæˆï¼Œè¿”å›å ä½ç¬¦")

                return {
                    "success": True,
                    "doc_name": doc_name,
                    "index_path": "",  # å¾…å®ç°
                    "indexed": False,  # å¾…å®ç°
                    "message": "IndexingAgent é›†æˆå¾…å®Œæˆ"
                }

            finally:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if os.path.exists(temp_file.name):
                    os.unlink(temp_file.name)

        except Exception as e:
            logger.error(f"âŒ [CallIndexingAgent] ç´¢å¼•å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "error": str(e),
                "indexed": False
            }

    def __del__(self):
        """æ¸…ç†èµ„æº"""
        import asyncio

        try:
            # æ¸…ç† MCP clients
            loop = asyncio.get_event_loop()
            if loop.is_running():
                loop.create_task(self.utils.cleanup_mcp_clients())
            else:
                asyncio.run(self.utils.cleanup_mcp_clients())
        except Exception:
            pass
