"""
SearchAgent è¾…åŠ©å·¥å…·å‡½æ•°

æä¾›ï¼š
- MCP Client åˆå§‹åŒ–å’Œç®¡ç†
- å†…å®¹é‡åˆ¤æ–­
- URL éªŒè¯
- æ•°æ®æ ¼å¼åŒ–
"""

from __future__ import annotations
from typing import TYPE_CHECKING, Dict, List, Optional
import logging
import re
import json
from urllib.parse import urlparse
from contextlib import AsyncExitStack

if TYPE_CHECKING:
    from .agent import SearchAgent

logger = logging.getLogger(__name__)


# ========== JSON æå–è¾…åŠ©å‡½æ•° ==========

def extract_json_from_llm_response(response: str) -> Dict:
    """
    ä»LLMå“åº”ä¸­æå–JSONå¯¹è±¡

    æ”¯æŒä»¥ä¸‹æ ¼å¼ï¼š
    1. çº¯JSON: {"key": "value"}
    2. Markdownä»£ç å—: ```json\n{"key": "value"}\n```
    3. å¸¦è¯´æ˜çš„JSON: Some text\n```json\n{"key": "value"}\n```\nMore text

    Args:
        response: LLMå“åº”æ–‡æœ¬

    Returns:
        è§£æåçš„JSONå­—å…¸

    Raises:
        json.JSONDecodeError: JSONè§£æå¤±è´¥
    """
    json_text = response.strip()

    # å¦‚æœåŒ…å«markdownä»£ç å—ï¼Œæå–JSONéƒ¨åˆ†
    if "```json" in json_text:
        # æå– ```json å’Œ ``` ä¹‹é—´çš„å†…å®¹
        start = json_text.find("```json") + 7
        end = json_text.find("```", start)
        if end != -1:
            json_text = json_text[start:end].strip()
    elif "```" in json_text:
        # æå–ç¬¬ä¸€ä¸ªä»£ç å—
        start = json_text.find("```") + 3
        # è·³è¿‡å¯èƒ½çš„è¯­è¨€æ ‡è¯†ç¬¦ï¼ˆå¦‚json, pythonç­‰ï¼‰
        newline_pos = json_text.find("\n", start)
        if newline_pos != -1:
            start = newline_pos + 1
        end = json_text.find("```", start)
        if end != -1:
            json_text = json_text[start:end].strip()

    # è§£æJSON
    return json.loads(json_text)


class SimpleMCPClient:
    """
    ç®€åŒ–çš„ MCP Clientï¼Œç”¨äºç›´æ¥è°ƒç”¨ MCP å·¥å…·
    ä¸ä¾èµ– LLMBaseï¼Œåªè´Ÿè´£å·¥å…·è°ƒç”¨
    """

    def __init__(self, service_name: str, config: Dict):
        """
        åˆå§‹åŒ–ç®€åŒ–çš„ MCP Client

        Args:
            service_name: æœåŠ¡åç§°
            config: MCP æœåŠ¡é…ç½®
        """
        self.service_name = service_name
        self.config = config
        self.session = None
        self.exit_stack = AsyncExitStack()

    async def initialize(self):
        """åˆå§‹åŒ– MCP è¿æ¥"""
        try:
            from mcp import ClientSession, StdioServerParameters
            from mcp.client.stdio import stdio_client

            connection_type = self.config.get("type", "stdio")

            if connection_type == "stdio":
                server_params = StdioServerParameters(
                    command=self.config.get("command"),
                    args=self.config.get("args", []),
                    env=self.config.get("env", {}),
                )

                stdio_transport = await self.exit_stack.enter_async_context(
                    stdio_client(server_params)
                )
                read_stream, write_stream = stdio_transport

                self.session = await self.exit_stack.enter_async_context(
                    ClientSession(read_stream, write_stream)
                )

                await self.session.initialize()
                logger.info(f"âœ… [SimpleMCP] {self.service_name} session åˆå§‹åŒ–æˆåŠŸ")

            else:
                logger.error(f"âŒ [SimpleMCP] ä¸æ”¯æŒçš„è¿æ¥ç±»å‹: {connection_type}")
                raise ValueError(f"ä¸æ”¯æŒçš„è¿æ¥ç±»å‹: {connection_type}")

        except Exception as e:
            logger.error(f"âŒ [SimpleMCP] åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            raise

    async def call_tool(self, tool_name: str, arguments: Dict) -> List:
        """
        è°ƒç”¨ MCP å·¥å…·

        Args:
            tool_name: å·¥å…·åç§°
            arguments: å·¥å…·å‚æ•°

        Returns:
            å·¥å…·è°ƒç”¨ç»“æœ
        """
        if not self.session:
            raise RuntimeError("MCP session æœªåˆå§‹åŒ–")

        try:
            result = await self.session.call_tool(tool_name, arguments)

            # æå–å†…å®¹
            if hasattr(result, 'content') and result.content:
                return result.content

            return []

        except Exception as e:
            logger.error(f"âŒ [SimpleMCP] è°ƒç”¨å·¥å…· {tool_name} å¤±è´¥: {e}", exc_info=True)
            raise

    async def disconnect(self):
        """æ–­å¼€è¿æ¥"""
        try:
            await self.exit_stack.aclose()
            logger.info(f"âœ… [SimpleMCP] {self.service_name} è¿æ¥å·²å…³é—­")
        except RuntimeError as e:
            # é™é»˜å¿½ç•¥ cancel scope åœ¨ä¸åŒä»»åŠ¡ä¸­çš„é”™è¯¯ï¼ˆèµ„æºå·²æ­£ç¡®æ¸…ç†ï¼‰
            error_msg = str(e).lower()
            if "cancel scope" in error_msg or "different task" in error_msg:
                logger.debug(f"[SimpleMCP] {self.service_name} è·¨ä»»åŠ¡æ¸…ç†å®Œæˆï¼ˆæ­£å¸¸ï¼‰")
            else:
                logger.warning(f"âš ï¸  [SimpleMCP] å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")
        except Exception as e:
            logger.warning(f"âš ï¸  [SimpleMCP] å…³é—­è¿æ¥æ—¶å‡ºé”™: {e}")


class SearchUtils:
    """SearchAgent è¾…åŠ©å·¥å…·ç±»"""

    def __init__(self, agent: 'SearchAgent'):
        """
        Args:
            agent: SearchAgent å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.agent = agent
        self.mcp_clients: Dict[str, any] = {}  # MCP client ç¼“å­˜

    # ========== MCP Client ç®¡ç† ==========

    async def init_mcp_client(self, service_name: str) -> Optional[any]:
        """
        åˆå§‹åŒ–å¹¶ç¼“å­˜ MCP Clientï¼ˆä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ï¼Œç›´æ¥è°ƒç”¨ MCP å·¥å…·ï¼‰

        Args:
            service_name: MCP æœåŠ¡åç§°ï¼ˆ"web_scraper" æˆ– "duckduckgo"ï¼‰

        Returns:
            SimpleMCPClient å®ä¾‹ï¼Œå¤±è´¥è¿”å› None
        """
        # æ£€æŸ¥ç¼“å­˜
        if service_name in self.mcp_clients:
            logger.info(f"ğŸ”Œ [MCP] å¤ç”¨å·²æœ‰çš„ {service_name} client")
            return self.mcp_clients[service_name]

        try:
            from src.config.settings import MCP_CONFIG

            # è·å–é…ç½®
            config = MCP_CONFIG.get(service_name)
            if not config:
                logger.error(f"âŒ [MCP] æœªæ‰¾åˆ° {service_name} çš„é…ç½®")
                return None

            logger.info(f"ğŸ”Œ [MCP] åˆå§‹åŒ– {service_name} client...")

            # åˆ›å»ºç®€åŒ–çš„ MCP client
            client = SimpleMCPClient(
                service_name=service_name,
                config=config
            )

            # åˆå§‹åŒ–è¿æ¥
            await client.initialize()

            # ç¼“å­˜
            self.mcp_clients[service_name] = client
            logger.info(f"âœ… [MCP] {service_name} client åˆå§‹åŒ–æˆåŠŸ")

            return client

        except Exception as e:
            logger.error(f"âŒ [MCP] åˆå§‹åŒ– {service_name} client å¤±è´¥: {e}", exc_info=True)
            return None

    async def cleanup_mcp_clients(self):
        """æ¸…ç†æ‰€æœ‰ MCP clients"""
        for service_name, client in self.mcp_clients.items():
            try:
                if hasattr(client, 'disconnect'):
                    await client.disconnect()
                logger.info(f"ğŸ”Œ [MCP] {service_name} client å·²æ–­å¼€")
            except Exception as e:
                logger.warning(f"âš ï¸  [MCP] æ–­å¼€ {service_name} client æ—¶å‡ºé”™: {e}")

        self.mcp_clients.clear()

    # ========== URL éªŒè¯ ==========

    @staticmethod
    def is_valid_url(url: str) -> bool:
        """
        éªŒè¯URLæ˜¯å¦åˆæ³•

        Args:
            url: å¾…éªŒè¯çš„URLå­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦ä¸ºåˆæ³•URL
        """
        if not url or not isinstance(url, str):
            return False

        try:
            result = urlparse(url.strip())
            # å¿…é¡»æœ‰scheme (http/https) å’Œ netloc (åŸŸå)
            return all([result.scheme in ['http', 'https'], result.netloc])
        except Exception:
            return False

    @staticmethod
    def extract_urls_from_text(text: str) -> List[str]:
        """
        ä»æ–‡æœ¬ä¸­æå–æ‰€æœ‰URL

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            URLåˆ—è¡¨
        """
        # URLæ­£åˆ™è¡¨è¾¾å¼
        url_pattern = r'https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+'
        urls = re.findall(url_pattern, text)

        # éªŒè¯å¹¶å»é‡
        valid_urls = []
        seen = set()

        for url in urls:
            if SearchUtils.is_valid_url(url) and url not in seen:
                valid_urls.append(url)
                seen.add(url)

        return valid_urls

    @staticmethod
    def normalize_url(url: str) -> str:
        """
        æ ‡å‡†åŒ–URLï¼ˆå»é™¤æœ«å°¾æ–œæ ã€æŸ¥è¯¢å‚æ•°ç­‰ï¼‰

        Args:
            url: åŸå§‹URL

        Returns:
            æ ‡å‡†åŒ–åçš„URL
        """
        url = url.strip()

        # å»é™¤æœ«å°¾æ–œæ 
        if url.endswith('/'):
            url = url[:-1]

        return url

    # ========== å†…å®¹é‡åˆ¤æ–­ ==========

    @staticmethod
    def calculate_content_size(text: str) -> Dict[str, int]:
        """
        è®¡ç®—æ–‡æœ¬å†…å®¹çš„å¤§å°æŒ‡æ ‡

        Args:
            text: æ–‡æœ¬å†…å®¹

        Returns:
            åŒ…å«å¤šä¸ªæŒ‡æ ‡çš„å­—å…¸ï¼š
            {
                "chars": å­—ç¬¦æ•°,
                "words": å•è¯æ•°ï¼ˆä¸­æ–‡æŒ‰å­—ç¬¦ï¼Œè‹±æ–‡æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰,
                "estimated_tokens": ä¼°ç®—çš„tokenæ•°
            }
        """
        if not text:
            return {"chars": 0, "words": 0, "estimated_tokens": 0}

        chars = len(text)

        # ç®€å•çš„å•è¯ç»Ÿè®¡ï¼ˆä¸­æ–‡æ¯ä¸ªå­—ç®—ä¸€ä¸ªè¯ï¼Œè‹±æ–‡æŒ‰ç©ºæ ¼åˆ†å‰²ï¼‰
        # æ£€æµ‹æ˜¯å¦åŒ…å«ä¸­æ–‡
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', text))

        words = chinese_chars + english_words

        # Tokenä¼°ç®—ï¼ˆç²—ç•¥ä¼°è®¡ï¼šä¸­æ–‡1å­—=1tokenï¼Œè‹±æ–‡1è¯=1.3tokenï¼‰
        estimated_tokens = chinese_chars + int(english_words * 1.3)

        return {
            "chars": chars,
            "words": words,
            "estimated_tokens": estimated_tokens
        }

    @staticmethod
    def should_index_content(
        content_size: int,
        threshold: int = 5000,
        num_sources: int = 1
    ) -> tuple[bool, str]:
        """
        åˆ¤æ–­å†…å®¹æ˜¯å¦åº”è¯¥ç´¢å¼•

        Args:
            content_size: å†…å®¹å­—ç¬¦æ•°
            threshold: å­—ç¬¦æ•°é˜ˆå€¼ï¼ˆé»˜è®¤5000ï¼‰
            num_sources: æ¥æºæ•°é‡

        Returns:
            (æ˜¯å¦åº”è¯¥ç´¢å¼•, ç†ç”±)
        """
        # ç­–ç•¥1: å†…å®¹é‡è¶…è¿‡é˜ˆå€¼
        if content_size >= threshold:
            return True, f"å†…å®¹é‡å¤§ ({content_size} å­—ç¬¦ â‰¥ {threshold} é˜ˆå€¼)"

        # ç­–ç•¥2: å¤šæºå†…å®¹ï¼ˆè¶…è¿‡3ä¸ªæ¥æºï¼‰
        if num_sources > 3:
            return True, f"å¤šæºå†…å®¹ ({num_sources} ä¸ªæ¥æº > 3)"

        # ç­–ç•¥3: æ¥è¿‘é˜ˆå€¼ï¼ˆ80%ä»¥ä¸Šï¼‰
        if content_size >= threshold * 0.8:
            return True, f"æ¥è¿‘é˜ˆå€¼ ({content_size} å­—ç¬¦ â‰¥ {threshold * 0.8})"

        # é»˜è®¤ï¼šç›´æ¥å¯¹è¯
        return False, f"å†…å®¹é‡å° ({content_size} å­—ç¬¦ < {threshold})"

    # ========== æ•°æ®æ ¼å¼åŒ– ==========

    @staticmethod
    def format_search_results(raw_results: List[Dict]) -> List[Dict]:
        """
        æ ¼å¼åŒ–æœç´¢å¼•æ“ç»“æœä¸ºç»Ÿä¸€æ ¼å¼

        Args:
            raw_results: åŸå§‹æœç´¢ç»“æœ

        Returns:
            æ ¼å¼åŒ–åçš„ç»“æœåˆ—è¡¨
            [{"title": str, "url": str, "snippet": str}, ...]
        """
        formatted = []

        for item in raw_results:
            # æå–å…³é”®å­—æ®µï¼ˆå…¼å®¹ä¸åŒæœç´¢å¼•æ“çš„è¿”å›æ ¼å¼ï¼‰
            formatted_item = {
                "title": item.get("title") or item.get("name") or "æ— æ ‡é¢˜",
                "url": item.get("url") or item.get("link") or "",
                "snippet": item.get("snippet") or item.get("description") or item.get("content") or ""
            }

            # éªŒè¯URL
            if SearchUtils.is_valid_url(formatted_item["url"]):
                formatted.append(formatted_item)

        return formatted

    @staticmethod
    def merge_scraped_content(scraped_results: List[Dict]) -> str:
        """
        åˆå¹¶å¤šä¸ªçˆ¬å–ç»“æœçš„æ–‡æœ¬å†…å®¹

        Args:
            scraped_results: çˆ¬å–ç»“æœåˆ—è¡¨

        Returns:
            åˆå¹¶åçš„æ–‡æœ¬
        """
        merged_parts = []

        for idx, result in enumerate(scraped_results, 1):
            if not result.get("success"):
                continue

            url = result.get("url", "")
            content = result.get("content", {})
            text = content.get("text", "")

            if text and text.strip():
                # æ·»åŠ æ¥æºæ ‡è¯†
                merged_parts.append(f"=== æ¥æº {idx}: {url} ===\n\n{text}\n")

        return "\n\n".join(merged_parts)

    @staticmethod
    def extract_key_info_from_html(html: str, max_length: int = 10000) -> str:
        """
        ä»HTMLä¸­æå–å…³é”®æ–‡æœ¬ï¼ˆå»é™¤æ ‡ç­¾ã€è„šæœ¬ã€æ ·å¼ç­‰ï¼‰

        Args:
            html: HTMLå†…å®¹
            max_length: æœ€å¤§ä¿ç•™é•¿åº¦

        Returns:
            æå–çš„æ–‡æœ¬
        """
        if not html:
            return ""

        # ç®€å•çš„HTMLæ¸…ç†ï¼ˆå®é™…é¡¹ç›®ä¸­åº”ä½¿ç”¨ BeautifulSoup ç­‰åº“ï¼‰
        # å»é™¤ script å’Œ style æ ‡ç­¾
        text = re.sub(r'<script[^>]*>.*?</script>', '', html, flags=re.DOTALL | re.IGNORECASE)
        text = re.sub(r'<style[^>]*>.*?</style>', '', text, flags=re.DOTALL | re.IGNORECASE)

        # å»é™¤æ‰€æœ‰HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)

        # å»é™¤å¤šä½™ç©ºç™½
        text = re.sub(r'\s+', ' ', text)

        # æˆªæ–­
        if len(text) > max_length:
            text = text[:max_length] + "..."

        return text.strip()

    # ========== Use Case æ£€æµ‹ ==========

    @staticmethod
    def auto_detect_use_case(query: str, target_urls: Optional[List[str]] = None) -> tuple[str, str]:
        """
        è‡ªåŠ¨æ£€æµ‹ä½¿ç”¨åœºæ™¯

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            target_urls: ç”¨æˆ·æä¾›çš„URLåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰

        Returns:
            (use_case, reason)
        """
        # åœºæ™¯1: ç”¨æˆ·æ˜ç¡®æä¾›äº†URL
        if target_urls and len(target_urls) > 0:
            return "url_analysis", f"ç”¨æˆ·æä¾›äº† {len(target_urls)} ä¸ªURL"

        # åœºæ™¯2: æŸ¥è¯¢ä¸­åŒ…å«URL
        urls_in_query = SearchUtils.extract_urls_from_text(query)
        if urls_in_query:
            return "url_analysis", f"æŸ¥è¯¢ä¸­åŒ…å« {len(urls_in_query)} ä¸ªURL"

        # åœºæ™¯3: æŸ¥è¯¢ä¸­åŒ…å«"è¿™ä¸ªç½‘é¡µ"ã€"è¿™ç¯‡æ–‡ç« "ç­‰æŒ‡ç¤ºè¯
        analysis_keywords = ["è¿™ä¸ªç½‘é¡µ", "è¿™ç¯‡æ–‡ç« ", "è¯¥é¡µé¢", "æ­¤é“¾æ¥", "åˆ†æç½‘é¡µ", "ç½‘é¡µå†…å®¹"]
        if any(keyword in query for keyword in analysis_keywords):
            return "url_analysis", "æŸ¥è¯¢åŒ…å«URLåˆ†æç›¸å…³çš„æŒ‡ç¤ºè¯"

        # é»˜è®¤ï¼šæœç´¢å¼•æ“æ£€ç´¢
        return "search", "æœªæ£€æµ‹åˆ°URLï¼Œé»˜è®¤ä½¿ç”¨æœç´¢å¼•æ“æ£€ç´¢"

    # ========== çŠ¶æ€éªŒè¯ ==========

    def validate_state(self, state: Dict) -> None:
        """
        éªŒè¯çŠ¶æ€æ˜¯å¦åŒ…å«å¿…éœ€å­—æ®µ

        Args:
            state: SearchState

        Raises:
            ValueError: å¦‚æœç¼ºå°‘å¿…éœ€å­—æ®µ
        """
        required_fields = ["query"]

        for field in required_fields:
            if field not in state or not state[field]:
                raise ValueError(f"SearchState ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")

        logger.debug(f"âœ… [Utils] State éªŒè¯é€šè¿‡")

    # ========== æœç´¢ç¼“å­˜ç®¡ç† ==========

    @staticmethod
    def generate_query_hash(query: str) -> str:
        """
        ç”ŸæˆæŸ¥è¯¢çš„å“ˆå¸Œå€¼ï¼ˆç”¨äºç¼“å­˜æ–‡ä»¶åï¼‰

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            MD5 å“ˆå¸Œå€¼
        """
        import hashlib
        return hashlib.md5(query.strip().lower().encode('utf-8')).hexdigest()

    @staticmethod
    def save_search_cache(
        query: str,
        sources: List[Dict],
        answer: Optional[str] = None
    ) -> str:
        """
        ä¿å­˜æœç´¢ç»“æœåˆ°ç¼“å­˜

        Args:
            query: åŸå§‹æŸ¥è¯¢
            sources: æ¥æºåˆ—è¡¨ [{"url": str, "title": str, "content": Dict}, ...]
            answer: ç”Ÿæˆçš„ç­”æ¡ˆï¼ˆå¯é€‰ï¼‰

        Returns:
            ç¼“å­˜æ–‡ä»¶è·¯å¾„
        """
        import json
        from datetime import datetime
        from pathlib import Path
        from src.config.constants import PathConstants

        # ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œ
        query_hash = SearchUtils.generate_query_hash(query)

        # ç¼“å­˜ç›®å½•
        cache_dir = Path(PathConstants.DATA_ROOT) / PathConstants.SEARCH_CACHE_DIR
        cache_dir.mkdir(parents=True, exist_ok=True)

        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_file = cache_dir / f"{query_hash}.json"

        # æ„å»ºç¼“å­˜æ•°æ®
        cache_data = {
            "query": query,
            "query_hash": query_hash,
            "timestamp": datetime.now().isoformat(),
            "sources": sources,
            "answer": answer
        }

        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(cache_data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ [Cache] æœç´¢ç»“æœå·²ä¿å­˜: {cache_file}")
        logger.info(f"   - æŸ¥è¯¢: {query[:50]}...")
        logger.info(f"   - æ¥æºæ•°é‡: {len(sources)}")

        return str(cache_file)

    @staticmethod
    def load_search_cache(query: str) -> Optional[Dict]:
        """
        åŠ è½½æœç´¢ç¼“å­˜

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            ç¼“å­˜æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        import json
        from pathlib import Path
        from src.config.constants import PathConstants

        # ç”ŸæˆæŸ¥è¯¢å“ˆå¸Œ
        query_hash = SearchUtils.generate_query_hash(query)

        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        cache_dir = Path(PathConstants.DATA_ROOT) / PathConstants.SEARCH_CACHE_DIR
        cache_file = cache_dir / f"{query_hash}.json"

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not cache_file.exists():
            return None

        # è¯»å–ç¼“å­˜
        try:
            with open(cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)

            logger.info(f"ğŸ“¦ [Cache] æ‰¾åˆ°ç¼“å­˜: {cache_file}")
            logger.info(f"   - æŸ¥è¯¢: {cache_data['query'][:50]}...")
            logger.info(f"   - æ—¶é—´: {cache_data['timestamp']}")
            logger.info(f"   - æ¥æºæ•°é‡: {len(cache_data['sources'])}")

            return cache_data

        except Exception as e:
            logger.warning(f"âš ï¸  [Cache] è¯»å–ç¼“å­˜å¤±è´¥: {e}")
            return None

    @staticmethod
    def has_search_cache(query: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦æœ‰ç¼“å­˜

        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            æ˜¯å¦å­˜åœ¨ç¼“å­˜
        """
        from pathlib import Path
        from src.config.constants import PathConstants

        query_hash = SearchUtils.generate_query_hash(query)
        cache_dir = Path(PathConstants.DATA_ROOT) / PathConstants.SEARCH_CACHE_DIR
        cache_file = cache_dir / f"{query_hash}.json"

        return cache_file.exists()

    # ========== Use Case 2: Web å†…å®¹ä¿å­˜ ==========

    @staticmethod
    def save_web_content(url: str, content: Dict, metadata: Optional[Dict] = None) -> str:
        """
        ä¿å­˜ URL å†…å®¹åˆ° JSON æ–‡ä»¶ï¼ˆUse Case 2ï¼‰

        Args:
            url: URL åœ°å€
            content: å†…å®¹æ•°æ® {"text": str, "html": str, "json": Dict}
            metadata: å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰

        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        import json
        from datetime import datetime
        from pathlib import Path
        from src.config.constants import PathConstants

        # ç”Ÿæˆ URL å“ˆå¸Œä½œä¸ºæ–‡ä»¶å
        url_hash = SearchUtils.generate_query_hash(url)

        # ç›®å½•
        web_dir = Path(PathConstants.DATA_ROOT) / PathConstants.WEB_CONTENT_DIR
        web_dir.mkdir(parents=True, exist_ok=True)

        # æ–‡ä»¶è·¯å¾„
        json_file = web_dir / f"{url_hash}.json"

        # æ„å»ºæ•°æ®
        data = {
            "url": url,
            "url_hash": url_hash,
            "timestamp": datetime.now().isoformat(),
            "content": content,
            "metadata": metadata or {}
        }

        # ä¿å­˜
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)

        logger.info(f"ğŸ’¾ [WebContent] URL å†…å®¹å·²ä¿å­˜: {json_file}")
        logger.info(f"   - URL: {url}")
        logger.info(f"   - æ–‡æœ¬é•¿åº¦: {len(content.get('text', ''))} å­—ç¬¦")

        return str(json_file)

    @staticmethod
    def load_web_content(url: str) -> Optional[Dict]:
        """
        åŠ è½½å·²ä¿å­˜çš„ URL å†…å®¹

        Args:
            url: URL åœ°å€

        Returns:
            å†…å®¹æ•°æ®ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        import json
        from pathlib import Path
        from src.config.constants import PathConstants

        url_hash = SearchUtils.generate_query_hash(url)
        web_dir = Path(PathConstants.DATA_ROOT) / PathConstants.WEB_CONTENT_DIR
        json_file = web_dir / f"{url_hash}.json"

        if not json_file.exists():
            return None

        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            logger.info(f"ğŸ“¦ [WebContent] æ‰¾åˆ°å·²ä¿å­˜å†…å®¹: {json_file}")
            return data

        except Exception as e:
            logger.warning(f"âš ï¸  [WebContent] è¯»å–å†…å®¹å¤±è´¥: {e}")
            return None

    @staticmethod
    def generate_doc_name_from_url(url: str) -> str:
        """
        ä» URL ç”Ÿæˆæ–‡æ¡£åï¼ˆç”¨äº IndexingAgentï¼‰

        Args:
            url: URL åœ°å€

        Returns:
            æ–‡æ¡£åç§°ï¼Œæ ¼å¼ï¼šweb_{domain}_{hash[:8]}
        """
        from urllib.parse import urlparse

        parsed = urlparse(url)
        domain = parsed.netloc.replace('.', '_').replace(':', '_')
        url_hash = SearchUtils.generate_query_hash(url)[:8]

        return f"web_{domain}_{url_hash}"
