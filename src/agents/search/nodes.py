"""
SearchAgent Workflow èŠ‚ç‚¹å®ç°

åŒ…å«æ‰€æœ‰ workflow èŠ‚ç‚¹çš„å®ç°é€»è¾‘
"""

from __future__ import annotations
from typing import Dict, TYPE_CHECKING
import logging
import json

from .state import SearchState
from .prompts import SearchRole
from .utils import extract_json_from_llm_response
from src.config.constants import ProcessingLimits

if TYPE_CHECKING:
    from .agent import SearchAgent

logger = logging.getLogger(__name__)


class SearchNodes:
    """SearchAgent Workflow èŠ‚ç‚¹é›†åˆ"""

    def __init__(self, agent: 'SearchAgent'):
        """
        Args:
            agent: SearchAgent å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.agent = agent

    # ========== åˆå§‹åŒ–èŠ‚ç‚¹ ==========

    async def initialize(self, state: SearchState) -> Dict:
        """åˆå§‹åŒ–èŠ‚ç‚¹ï¼šéªŒè¯è¾“å…¥ï¼Œè®¾ç½®é»˜è®¤å€¼"""
        logger.info("ğŸ”§ [Initialize] ========== SearchAgent åˆå§‹åŒ– ==========")

        try:
            # éªŒè¯state
            self.agent.utils.validate_state(state)

            # è®¾ç½®é»˜è®¤å€¼
            if 'max_iterations' not in state:
                state['max_iterations'] = 3

            if 'current_iteration' not in state:
                state['current_iteration'] = 0

            # åˆå§‹åŒ–åˆ—è¡¨å­—æ®µ
            for field in ['thoughts', 'actions', 'observations', 'warnings']:
                if field not in state:
                    state[field] = []

            for field in ['search_engine_results', 'selected_urls', 'scraped_results', 'extracted_content', 'sources']:
                if field not in state:
                    state[field] = []

            # æ—¥å¿—è¾“å‡º
            logger.info(f"ğŸ”§ [Initialize] ç”¨æˆ·æŸ¥è¯¢: {state['query']}")
            logger.info(f"ğŸ”§ [Initialize] æœ€å¤§è¿­ä»£: {state['max_iterations']}")

            if 'target_urls' in state and state['target_urls']:
                logger.info(f"ğŸ”§ [Initialize] æŒ‡å®šURL: {state['target_urls']}")

            logger.info("âœ… [Initialize] åˆå§‹åŒ–å®Œæˆ")
            return state

        except Exception as e:
            logger.error(f"âŒ [Initialize] åˆå§‹åŒ–å¤±è´¥: {e}", exc_info=True)
            state['error'] = str(e)
            state['is_complete'] = True
            return state

    # ========== Use Case åˆ†æèŠ‚ç‚¹ ==========

    async def analyze_query(self, state: SearchState) -> Dict:
        """åˆ†ææŸ¥è¯¢å¹¶åˆ¤æ–­ä½¿ç”¨åœºæ™¯"""
        logger.info("ğŸ¤” [AnalyzeQuery] ========== åˆ†ææŸ¥è¯¢ç±»å‹ ==========")

        try:
            query = state['query']
            target_urls = state.get('target_urls')

            # å¦‚æœç”¨æˆ·å·²æŒ‡å®š use_caseï¼Œç›´æ¥ä½¿ç”¨
            if 'use_case' in state and state['use_case']:
                detected_use_case = state['use_case']
                reason = "ç”¨æˆ·æ‰‹åŠ¨æŒ‡å®š"
                logger.info(f"âœ… [AnalyzeQuery] ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„åœºæ™¯: {detected_use_case}")
            else:
                # è‡ªåŠ¨æ£€æµ‹
                detected_use_case, reason = self.agent.utils.auto_detect_use_case(query, target_urls)

            state['detected_use_case'] = detected_use_case
            state['use_case_reason'] = reason

            logger.info(f"ğŸ¤” [AnalyzeQuery] æ£€æµ‹ç»“æœ:")
            logger.info(f"   - Use Case: {detected_use_case}")
            logger.info(f"   - ç†ç”±: {reason}")

            # å¦‚æœæ˜¯ URL åˆ†ææ¨¡å¼ï¼Œæå– URL
            if detected_use_case == "url_analysis":
                if not target_urls:
                    # å°è¯•ä»æŸ¥è¯¢ä¸­æå–URL
                    extracted_urls = self.agent.utils.extract_urls_from_text(query)
                    if extracted_urls:
                        state['target_urls'] = extracted_urls
                        logger.info(f"ğŸ¤” [AnalyzeQuery] ä»æŸ¥è¯¢ä¸­æå–äº† {len(extracted_urls)} ä¸ªURL")
                    else:
                        logger.warning("âš ï¸  [AnalyzeQuery] URLåˆ†ææ¨¡å¼ä½†æœªæ‰¾åˆ°URLï¼Œåˆ‡æ¢åˆ°æœç´¢æ¨¡å¼")
                        state['detected_use_case'] = "search"
                        state['use_case_reason'] = "URLåˆ†ææ¨¡å¼ä½†æœªæ‰¾åˆ°URL"

            return state

        except Exception as e:
            logger.error(f"âŒ [AnalyzeQuery] åˆ†æå¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶é»˜è®¤ä½¿ç”¨æœç´¢æ¨¡å¼
            state['detected_use_case'] = "search"
            state['use_case_reason'] = f"åˆ†æå¤±è´¥ï¼Œé»˜è®¤æœç´¢æ¨¡å¼: {str(e)}"
            return state

    # ========== Use Case 1: æœç´¢å¼•æ“æŸ¥è¯¢èŠ‚ç‚¹ ==========

    async def web_search(self, state: SearchState) -> Dict:
        """æœç´¢å¼•æ“æŸ¥è¯¢èŠ‚ç‚¹"""
        logger.info("ğŸ” [WebSearch] ========== æœç´¢å¼•æ“æŸ¥è¯¢ ==========")

        try:
            query = state['query']

            # æ­¥éª¤1: ä¼˜åŒ–æŸ¥è¯¢ï¼ˆå¯é€‰ï¼‰
            # è¿™é‡Œå¯ä»¥è°ƒç”¨ LLM ä¼˜åŒ–æŸ¥è¯¢ï¼Œæš‚æ—¶ç›´æ¥ä½¿ç”¨åŸå§‹æŸ¥è¯¢
            search_query = query
            state['search_query'] = search_query

            logger.info(f"ğŸ” [WebSearch] æœç´¢æŸ¥è¯¢: {search_query}")

            # æ­¥éª¤2: è°ƒç”¨æœç´¢å¼•æ“å·¥å…·
            search_result = await self.agent.tools.web_search(
                query=search_query,
                max_results=10
            )

            if search_result.get('success'):
                results = search_result.get('results', [])
                state['search_engine_results'] = results

                logger.info(f"âœ… [WebSearch] è·å–åˆ° {len(results)} ä¸ªæœç´¢ç»“æœ")

                # è®°å½•observation
                state['observations'] = state.get('observations', []) + [
                    f"æœç´¢åˆ° {len(results)} ä¸ªç»“æœ"
                ]
            else:
                error_msg = search_result.get('error', 'æœªçŸ¥é”™è¯¯')
                logger.error(f"âŒ [WebSearch] æœç´¢å¤±è´¥: {error_msg}")

                state['search_engine_results'] = []
                state['warnings'] = state.get('warnings', []) + [f"æœç´¢å¤±è´¥: {error_msg}"]

            return state

        except Exception as e:
            logger.error(f"âŒ [WebSearch] æœç´¢å¼‚å¸¸: {e}", exc_info=True)
            state['search_engine_results'] = []
            state['warnings'] = state.get('warnings', []) + [f"æœç´¢å¼‚å¸¸: {str(e)}"]
            return state

    # ========== Use Case 1: URLç­›é€‰èŠ‚ç‚¹ ==========

    async def select_urls(self, state: SearchState) -> Dict:
        """ä»æœç´¢ç»“æœä¸­ç­›é€‰ç›¸å…³URL"""
        logger.info("ğŸ“‹ [SelectURLs] ========== ç­›é€‰ç›¸å…³URL ==========")

        try:
            search_results = state.get('search_engine_results', [])

            if not search_results:
                logger.warning("âš ï¸  [SelectURLs] æ²¡æœ‰æœç´¢ç»“æœå¯ä¾›ç­›é€‰")
                state['selected_urls'] = []
                return state

            # æ­¥éª¤1: ä½¿ç”¨ LLM ç­›é€‰ URL
            # æ„å»º prompt
            query = state['query']
            max_urls = 5  # æœ€å¤šé€‰æ‹©5ä¸ªURL

            # æ ¼å¼åŒ–æœç´¢ç»“æœ
            results_text = "\n".join([
                f"{idx}. {item['title']}\n   URL: {item['url']}\n   æ‘˜è¦: {item['snippet']}\n"
                for idx, item in enumerate(search_results, 1)
            ])

            prompt = SearchRole.URL_SELECTOR.format(
                query=query,
                search_results=results_text,
                max_urls=max_urls
            )

            logger.info("ğŸ“‹ [SelectURLs] è°ƒç”¨ LLM ç­›é€‰URL...")
            response = await self.agent.llm.async_call_llm_chain(
                role="",  # ä½¿ç”¨ç©ºè§’è‰²ï¼Œpromptå·²ç»åŒ…å«å®Œæ•´æŒ‡ä»¤
                input_prompt=prompt,
                session_id="select_urls"
            )

            # è§£æ JSON å“åº”
            try:
                # ä½¿ç”¨å·¥å…·å‡½æ•°ä»LLMå“åº”ä¸­æå–JSON
                selection_data = extract_json_from_llm_response(response)
                selected_items = selection_data.get('selected_urls', [])
                overall_reason = selection_data.get('overall_reason', '')

                # æå– URL åˆ—è¡¨
                selected_urls = [item['url'] for item in selected_items if 'url' in item]

                state['selected_urls'] = selected_urls
                state['selection_reason'] = overall_reason

                logger.info(f"âœ… [SelectURLs] ç­›é€‰å‡º {len(selected_urls)} ä¸ªç›¸å…³URL")
                for idx, item in enumerate(selected_items, 1):
                    logger.info(f"   {idx}. {item.get('url', '')} - {item.get('reason', '')}")

            except (json.JSONDecodeError, KeyError, ValueError) as e:
                logger.warning(f"âš ï¸  [SelectURLs] JSONè§£æå¤±è´¥ ({e})ï¼Œä½¿ç”¨å‰5ä¸ªURL")
                logger.debug(f"ğŸ“‹ [SelectURLs] åŸå§‹å“åº”: {response[:500]}")
                state['selected_urls'] = [item['url'] for item in search_results[:max_urls]]
                state['selection_reason'] = "LLMå“åº”è§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥"

            return state

        except Exception as e:
            logger.error(f"âŒ [SelectURLs] ç­›é€‰å¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶ä½¿ç”¨å‰3ä¸ªURL
            search_results = state.get('search_engine_results', [])
            state['selected_urls'] = [item['url'] for item in search_results[:3]]
            state['selection_reason'] = f"ç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨å‰3ä¸ªURL: {str(e)}"
            return state

    # ========== å†…å®¹çˆ¬å–èŠ‚ç‚¹ï¼ˆä¸¤ç§ use case å…±ç”¨ï¼‰==========

    async def scrape_content(self, state: SearchState) -> Dict:
        """çˆ¬å–ç½‘é¡µå†…å®¹"""
        logger.info("ğŸŒ [ScrapeContent] ========== çˆ¬å–ç½‘é¡µå†…å®¹ ==========")

        try:
            detected_use_case = state.get('detected_use_case', 'search')

            # ç¡®å®šè¦çˆ¬å–çš„ URL åˆ—è¡¨
            if detected_use_case == "search":
                urls_to_scrape = state.get('selected_urls', [])
            else:  # url_analysis
                urls_to_scrape = state.get('target_urls', [])

            if not urls_to_scrape:
                logger.warning("âš ï¸  [ScrapeContent] æ²¡æœ‰URLéœ€è¦çˆ¬å–")
                state['scraped_results'] = []
                return state

            logger.info(f"ğŸŒ [ScrapeContent] å¾…çˆ¬å–URLæ•°: {len(urls_to_scrape)}")

            # é€‰æ‹©çˆ¬å–æ–¹å¼ï¼šå•ä¸ª vs æ‰¹é‡
            if len(urls_to_scrape) == 1:
                # å•ä¸ªURLï¼šä½¿ç”¨ scrape_single_url
                url = urls_to_scrape[0]
                logger.info(f"ğŸŒ [ScrapeContent] å•ä¸ªURLçˆ¬å–: {url}")

                result = await self.agent.tools.scrape_single_url(
                    url=url,
                    content_types=["html", "text", "json"],
                    timeout=30000
                )

                state['scraped_results'] = [result]

            else:
                # å¤šä¸ªURLï¼šä½¿ç”¨ scrape_batch_urls
                logger.info(f"ğŸŒ [ScrapeContent] æ‰¹é‡çˆ¬å– {len(urls_to_scrape)} ä¸ªURL")

                batch_result = await self.agent.tools.scrape_batch_urls(
                    urls=urls_to_scrape,
                    content_types=["text"],  # æ‰¹é‡çˆ¬å–é»˜è®¤åªæå–æ–‡æœ¬
                    concurrent_limit=3,
                    delay_between=2000
                )

                if batch_result.get('success'):
                    state['scraped_results'] = batch_result.get('results', [])

                    logger.info(f"âœ… [ScrapeContent] æ‰¹é‡çˆ¬å–å®Œæˆ")
                    logger.info(f"   - æˆåŠŸ: {batch_result.get('succeeded', 0)}")
                    logger.info(f"   - å¤±è´¥: {batch_result.get('failed', 0)}")
                else:
                    state['scraped_results'] = []
                    error_msg = batch_result.get('error', 'æœªçŸ¥é”™è¯¯')
                    state['warnings'] = state.get('warnings', []) + [f"æ‰¹é‡çˆ¬å–å¤±è´¥: {error_msg}"]

            # è®°å½•observation
            successful_count = sum(1 for r in state.get('scraped_results', []) if r.get('success'))
            state['observations'] = state.get('observations', []) + [
                f"çˆ¬å–äº† {len(urls_to_scrape)} ä¸ªURLï¼ŒæˆåŠŸ {successful_count} ä¸ª"
            ]

            return state

        except Exception as e:
            logger.error(f"âŒ [ScrapeContent] çˆ¬å–å¼‚å¸¸: {e}", exc_info=True)
            state['scraped_results'] = []
            state['warnings'] = state.get('warnings', []) + [f"çˆ¬å–å¼‚å¸¸: {str(e)}"]
            return state

    # ========== Use Case 2: å†…å®¹é‡è¯„ä¼°èŠ‚ç‚¹ ==========

    async def evaluate_content_size(self, state: SearchState) -> Dict:
        """è¯„ä¼°å†…å®¹é‡å¹¶å†³å®šå¤„ç†ç­–ç•¥"""
        logger.info("âš–ï¸ [EvaluateSize] ========== è¯„ä¼°å†…å®¹é‡ ==========")

        try:
            scraped_results = state.get('scraped_results', [])

            # åˆå¹¶æ‰€æœ‰çˆ¬å–çš„æ–‡æœ¬
            merged_text = self.agent.utils.merge_scraped_content(scraped_results)
            state['merged_text'] = merged_text

            # è®¡ç®—å†…å®¹å¤§å°
            size_metrics = self.agent.utils.calculate_content_size(merged_text)
            content_size = size_metrics['chars']
            state['content_size'] = content_size

            logger.info(f"âš–ï¸ [EvaluateSize] å†…å®¹ç»Ÿè®¡:")
            logger.info(f"   - å­—ç¬¦æ•°: {size_metrics['chars']}")
            logger.info(f"   - å•è¯æ•°: {size_metrics['words']}")
            logger.info(f"   - ä¼°ç®—tokens: {size_metrics['estimated_tokens']}")

            # å†³å®šå¤„ç†ç­–ç•¥
            threshold = 5000  # å­—ç¬¦æ•°é˜ˆå€¼
            num_sources = len([r for r in scraped_results if r.get('success')])

            should_index, reason = self.agent.utils.should_index_content(
                content_size=content_size,
                threshold=threshold,
                num_sources=num_sources
            )

            if should_index:
                state['processing_strategy'] = "index_then_chat"
                state['should_call_indexing'] = True
                logger.info(f"âš–ï¸ [EvaluateSize] ç­–ç•¥: ç´¢å¼•åå¯¹è¯")
                logger.info(f"âš–ï¸ [EvaluateSize] ç†ç”±: {reason}")
            else:
                state['processing_strategy'] = "direct_chat"
                state['should_call_indexing'] = False
                logger.info(f"âš–ï¸ [EvaluateSize] ç­–ç•¥: ç›´æ¥å¯¹è¯")
                logger.info(f"âš–ï¸ [EvaluateSize] ç†ç”±: {reason}")

            state['strategy_reason'] = reason

            # Use Case 2: ä¿å­˜ URL å†…å®¹åˆ° JSON æ–‡ä»¶
            use_case = state.get('use_case', '')
            if use_case == 'url_analysis' and scraped_results:
                try:
                    # è·å–ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœï¼ˆUse Case 2 é€šå¸¸åªæœ‰ä¸€ä¸ª URLï¼‰
                    for result in scraped_results:
                        if result.get('success'):
                            url = result.get('url', '')
                            content = result.get('content', {})

                            # ä¿å­˜å†…å®¹
                            json_path = self.agent.utils.save_web_content(
                                url=url,
                                content=content,
                                metadata={
                                    "content_size": content_size,
                                    "processing_strategy": state['processing_strategy'],
                                    "strategy_reason": reason
                                }
                            )

                            # å°†æ–‡ä»¶è·¯å¾„ä¿å­˜åˆ° state ä¸­ï¼Œæ–¹ä¾¿åç»­ IndexingAgent ä½¿ç”¨
                            state['web_content_json'] = json_path

                            # ç”Ÿæˆæ–‡æ¡£åï¼ˆç”¨äº IndexingAgentï¼‰
                            doc_name = self.agent.utils.generate_doc_name_from_url(url)
                            state['generated_doc_name'] = doc_name

                            logger.info(f"ğŸ“„ [EvaluateSize] ç”Ÿæˆæ–‡æ¡£å: {doc_name}")

                            break  # åªå¤„ç†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ç»“æœ

                except Exception as save_error:
                    logger.warning(f"âš ï¸  [EvaluateSize] ä¿å­˜ web å†…å®¹å¤±è´¥: {save_error}")

            return state

        except Exception as e:
            logger.error(f"âŒ [EvaluateSize] è¯„ä¼°å¤±è´¥: {e}", exc_info=True)
            # å¤±è´¥æ—¶é»˜è®¤ç›´æ¥å¯¹è¯
            state['processing_strategy'] = "direct_chat"
            state['should_call_indexing'] = False
            state['strategy_reason'] = f"è¯„ä¼°å¤±è´¥ï¼Œé»˜è®¤ç›´æ¥å¯¹è¯: {str(e)}"
            return state

    # ========== å†…å®¹æå–èŠ‚ç‚¹ ==========

    async def extract_and_merge(self, state: SearchState) -> Dict:
        """æå–å¹¶åˆå¹¶çˆ¬å–çš„å†…å®¹"""
        logger.info("ğŸ“ [ExtractMerge] ========== æå–å¹¶åˆå¹¶å†…å®¹ ==========")

        try:
            scraped_results = state.get('scraped_results', [])

            extracted_content = []
            sources = []

            for result in scraped_results:
                if not result.get('success'):
                    continue

                url = result.get('url', '')
                content = result.get('content', {})

                # æå–æ–‡æœ¬
                text = content.get('text', '')
                html = content.get('html', '')
                json_data = content.get('json', {})

                if text and text.strip():
                    extracted_content.append({
                        "url": url,
                        "text": text,
                        "html": html,
                        "json": json_data
                    })

                    sources.append(url)

            state['extracted_content'] = extracted_content
            state['sources'] = sources

            logger.info(f"âœ… [ExtractMerge] æå–äº† {len(extracted_content)} ä¸ªå†…å®¹ç‰‡æ®µ")

            # å¯¹äº Use Case 1ï¼ˆæœç´¢å¼•æ“æ£€ç´¢ï¼‰ï¼Œä¿å­˜åˆ°ç¼“å­˜
            use_case = state.get('use_case', '')
            if use_case == 'search' and extracted_content:
                try:
                    query = state.get('query', '')
                    # æ„å»ºç¼“å­˜æ•°æ®æ ¼å¼
                    cache_sources = []
                    for item in extracted_content:
                        cache_sources.append({
                            "url": item.get('url', ''),
                            "title": item.get('url', '').split('/')[-1],  # ç®€å•ä»URLæå–æ ‡é¢˜
                            "content": {
                                "text": item.get('text', ''),
                                "html": item.get('html', ''),
                                "json": item.get('json', {})
                            }
                        })

                    # ä¿å­˜ç¼“å­˜ï¼ˆç­”æ¡ˆç¨ååœ¨ format_answer èŠ‚ç‚¹æ·»åŠ ï¼‰
                    self.agent.utils.save_search_cache(
                        query=query,
                        sources=cache_sources,
                        answer=None  # ç­”æ¡ˆç¨åæ›´æ–°
                    )
                except Exception as cache_error:
                    logger.warning(f"âš ï¸  [ExtractMerge] ä¿å­˜ç¼“å­˜å¤±è´¥: {cache_error}")

            return state

        except Exception as e:
            logger.error(f"âŒ [ExtractMerge] æå–å¤±è´¥: {e}", exc_info=True)
            state['extracted_content'] = []
            state['sources'] = []
            return state

    # ========== å®Œæ•´æ€§è¯„ä¼°èŠ‚ç‚¹ ==========

    async def evaluate_completeness(self, state: SearchState) -> Dict:
        """è¯„ä¼°æ£€ç´¢ç»“æœçš„å®Œæ•´æ€§"""
        logger.info("âš–ï¸ [Evaluate] ========== è¯„ä¼°å®Œæ•´æ€§ ==========")

        try:
            current_iteration = state.get('current_iteration', 0)
            max_iterations = state.get('max_iterations', 3)
            extracted_content = state.get('extracted_content', [])

            logger.info(f"âš–ï¸ [Evaluate] è¿­ä»£è¿›åº¦: {current_iteration + 1}/{max_iterations}")
            logger.info(f"âš–ï¸ [Evaluate] å·²æå–å†…å®¹æ•°: {len(extracted_content)}")

            # ç®€å•ç­–ç•¥ï¼šå¦‚æœæœ‰å†…å®¹å°±è®¤ä¸ºå®Œæ•´
            if len(extracted_content) > 0:
                state['is_complete'] = True
                logger.info("âœ… [Evaluate] è¯„ä¼°å®Œæˆï¼šå·²è·å–è¶³å¤Ÿå†…å®¹")
            elif current_iteration >= max_iterations - 1:
                # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
                state['is_complete'] = True
                logger.warning("âš ï¸  [Evaluate] è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œå¼ºåˆ¶å®Œæˆ")
            else:
                state['is_complete'] = False
                logger.info("ğŸ”„ [Evaluate] å†…å®¹ä¸è¶³ï¼Œç»§ç»­æ£€ç´¢")

            # æ›´æ–°è¿­ä»£è®¡æ•°
            state['current_iteration'] = current_iteration + 1

            return state

        except Exception as e:
            logger.error(f"âŒ [Evaluate] è¯„ä¼°å¤±è´¥: {e}", exc_info=True)
            state['is_complete'] = True  # å¤±è´¥æ—¶å¼ºåˆ¶å®Œæˆ
            return state

    # ========== ç­”æ¡ˆç”ŸæˆèŠ‚ç‚¹ ==========

    async def format_answer(self, state: SearchState) -> Dict:
        """ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ"""
        logger.info("ğŸ¯ [FormatAnswer] ========== ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ ==========")

        try:
            query = state['query']
            extracted_content = state.get('extracted_content', [])
            sources = state.get('sources', [])
            detected_use_case = state.get('detected_use_case', 'search')

            if not extracted_content:
                state['final_answer'] = "æŠ±æ­‰ï¼Œæœªèƒ½è·å–åˆ°ç›¸å…³å†…å®¹ã€‚è¯·å°è¯•è°ƒæ•´æŸ¥è¯¢æˆ–æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚"
                logger.warning("âš ï¸  [FormatAnswer] æ— å†…å®¹å¯ç”¨ï¼Œè¿”å›é»˜è®¤ç­”æ¡ˆ")
                return state

            # åˆå¹¶å†…å®¹
            merged_content = "\n\n".join([
                f"=== æ¥æº: {item['url']} ===\n{item['text'][:2000]}"  # æ¯ä¸ªæ¥æºæœ€å¤š2000å­—ç¬¦
                for item in extracted_content
            ])

            # æ„å»º prompt
            prompt = SearchRole.CONTENT_SUMMARIZER.format(
                query=query,
                scraped_content=merged_content
            )

            logger.info("ğŸ¯ [FormatAnswer] è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ...")
            final_answer = await self.agent.llm.async_call_llm_chain(
                role="",
                input_prompt=prompt,
                session_id="format_answer"
            )

            # æ·»åŠ ä¿¡æ¯æ¥æº
            if sources:
                final_answer += "\n\n## ä¿¡æ¯æ¥æº\n"
                for idx, source_url in enumerate(sources, 1):
                    final_answer += f"{idx}. {source_url}\n"

            state['final_answer'] = final_answer

            logger.info(f"âœ… [FormatAnswer] ç­”æ¡ˆç”Ÿæˆå®Œæˆ")
            logger.info(f"   - ç­”æ¡ˆé•¿åº¦: {len(final_answer)} å­—ç¬¦")
            logger.info(f"   - æ¥æºæ•°: {len(sources)}")

            # å¯¹äº Use Case 1ï¼Œæ›´æ–°ç¼“å­˜æ·»åŠ ç­”æ¡ˆ
            use_case = state.get('use_case', '')
            if use_case == 'search' and extracted_content:
                try:
                    # æ„å»ºå®Œæ•´çš„ç¼“å­˜æ•°æ®
                    cache_sources = []
                    for item in extracted_content:
                        cache_sources.append({
                            "url": item.get('url', ''),
                            "title": item.get('url', '').split('/')[-1],
                            "content": {
                                "text": item.get('text', ''),
                                "html": item.get('html', ''),
                                "json": item.get('json', {})
                            }
                        })

                    # ä¿å­˜å®Œæ•´ç¼“å­˜ï¼ˆåŒ…å«ç­”æ¡ˆï¼‰
                    self.agent.utils.save_search_cache(
                        query=query,
                        sources=cache_sources,
                        answer=final_answer
                    )
                    logger.info("ğŸ’¾ [FormatAnswer] æœç´¢ç»“æœå·²ä¿å­˜åˆ°ç¼“å­˜")
                except Exception as cache_error:
                    logger.warning(f"âš ï¸  [FormatAnswer] æ›´æ–°ç¼“å­˜å¤±è´¥: {cache_error}")

            return state

        except Exception as e:
            logger.error(f"âŒ [FormatAnswer] ç”Ÿæˆç­”æ¡ˆå¤±è´¥: {e}", exc_info=True)
            state['final_answer'] = f"ç”Ÿæˆç­”æ¡ˆæ—¶å‡ºé”™: {str(e)}"
            return state

    # ========== æ¡ä»¶è·¯ç”±èŠ‚ç‚¹ ==========

    def should_continue(self, state: SearchState) -> str:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­æ£€ç´¢"""
        is_complete = state.get('is_complete', False)

        if is_complete:
            logger.info("âœ… [ShouldContinue] æ£€ç´¢å®Œæˆï¼Œç”Ÿæˆç­”æ¡ˆ")
            return "format"

        logger.info("ğŸ”„ [ShouldContinue] ç»§ç»­æ£€ç´¢")
        return "continue"

    def route_by_use_case(self, state: SearchState) -> str:
        """æ ¹æ® use case è·¯ç”±åˆ°ä¸åŒåˆ†æ”¯"""
        detected_use_case = state.get('detected_use_case', 'search')

        if detected_use_case == "search":
            logger.info("ğŸ” [Route] è·¯ç”±åˆ°: æœç´¢å¼•æ“æ¨¡å¼")
            return "search"
        else:  # url_analysis
            logger.info("ğŸ“„ [Route] è·¯ç”±åˆ°: URLåˆ†ææ¨¡å¼")
            return "url_analysis"
