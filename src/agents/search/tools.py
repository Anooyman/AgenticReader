"""
SearchAgent å·¥å…·å®ç°

å°è£… MCP å·¥å…·è°ƒç”¨ï¼š
- web_scraper MCP: ç½‘é¡µçˆ¬å–
- DuckDuckGo MCP: æœç´¢å¼•æ“ï¼ˆæˆ–å…¶ä»–æœç´¢APIï¼‰
"""

from __future__ import annotations
from typing import TYPE_CHECKING, Dict, List, Any, Optional
import logging
import json

if TYPE_CHECKING:
    from .agent import SearchAgent

logger = logging.getLogger(__name__)


class SearchTools:
    """SearchAgent å·¥å…·æ–¹æ³•é›†åˆ"""

    def __init__(self, agent: 'SearchAgent'):
        """
        Args:
            agent: SearchAgent å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.agent = agent

    # ========== è¾…åŠ©æ–¹æ³• ==========

    def _parse_text_search_results(self, text: str) -> Dict:
        """
        è§£ææ–‡æœ¬æ ¼å¼çš„æœç´¢ç»“æœ

        DuckDuckGo MCP è¿”å›æ ¼å¼:
        Found 10 search results:

        1. Title
           URL: https://...
           Summary: ...

        2. Title
           URL: https://...
           Summary: ...

        Args:
            text: åŸå§‹æ–‡æœ¬

        Returns:
            {"results": [{"title": str, "url": str, "snippet": str}, ...]}
        """
        import re

        results = []

        # ä½¿ç”¨æ­£åˆ™æå–æ¯ä¸ªç»“æœå—
        # åŒ¹é…æ¨¡å¼: æ•°å­—. æ ‡é¢˜\n   URL: ...\n   Summary: ...
        pattern = r'(\d+)\.\s+(.+?)\s+URL:\s+(\S+)\s+Summary:\s+(.+?)(?=\n\d+\.|$)'

        matches = re.findall(pattern, text, re.DOTALL)

        for match in matches:
            idx, title, url, summary = match
            results.append({
                "title": title.strip(),
                "url": url.strip(),
                "snippet": summary.strip()
            })

        logger.info(f"ğŸ“ [TextParser] ä»æ–‡æœ¬ä¸­è§£æå‡º {len(results)} ä¸ªç»“æœ")

        return {"results": results}

    # ========== æœç´¢å¼•æ“å·¥å…· ==========

    async def web_search(self, query: str, max_results: int = 10) -> Dict:
        """
        ä½¿ç”¨æœç´¢å¼•æ“æŸ¥æ‰¾ç›¸å…³ç½‘é¡µ

        è°ƒç”¨ DuckDuckGo MCPï¼ˆæˆ–å…¶ä»–æœç´¢APIï¼‰

        Args:
            query: æœç´¢æŸ¥è¯¢
            max_results: æœ€å¤§ç»“æœæ•°

        Returns:
            æ ‡å‡†æ ¼å¼å“åº”ï¼š
            {
                "success": bool,
                "results": [{"title": str, "url": str, "snippet": str}, ...],
                "total": int,
                "error": str (å¦‚æœå¤±è´¥)
            }
        """
        logger.info(f"ğŸ” [Tool:web_search] æœç´¢å¼•æ“æŸ¥è¯¢: {query}")
        logger.info(f"ğŸ” [Tool:web_search] æœ€å¤§ç»“æœæ•°: {max_results}")

        try:
            # åˆå§‹åŒ– MCP clientï¼ˆDuckDuckGo æˆ–å…¶ä»–æœç´¢æœåŠ¡ï¼‰
            mcp_client = await self.agent.utils.init_mcp_client("duckduckgo")

            if not mcp_client:
                logger.error("âŒ [Tool:web_search] æœç´¢å¼•æ“ MCP client æœªåˆå§‹åŒ–")
                return {
                    "success": False,
                    "results": [],
                    "total": 0,
                    "error": "æœç´¢å¼•æ“æœåŠ¡ä¸å¯ç”¨"
                }

            # è°ƒç”¨ MCP å·¥å…·
            logger.info("ğŸ” [Tool:web_search] è°ƒç”¨ DuckDuckGo MCP...")
            result = await mcp_client.call_tool(
                tool_name="search",  # DuckDuckGo MCP çš„å·¥å…·åç§°
                arguments={
                    "query": query,
                    "max_results": max_results
                }
            )

            # è§£æç»“æœ
            if result and isinstance(result, list) and len(result) > 0:
                # MCP è¿”å› TextContent å¯¹è±¡ï¼Œç›´æ¥è®¿é—® .text å±æ€§
                result_text = result[0].text

                # è°ƒè¯•ï¼šè¾“å‡ºåŸå§‹è¿”å›å†…å®¹ï¼ˆä½¿ç”¨ INFO çº§åˆ«ç¡®ä¿å¯è§ï¼‰
                logger.info(f"ğŸ” [Tool:web_search] DuckDuckGo åŸå§‹è¿”å›ï¼ˆå‰500å­—ç¬¦ï¼‰:")
                logger.info(f"   {result_text[:500]}")
                if len(result_text) > 500:
                    logger.info(f"   ... (å…± {len(result_text)} å­—ç¬¦)")

                # å°è¯•è§£æ JSON
                try:
                    parsed_result = json.loads(result_text)
                    logger.debug(f"ğŸ” [Tool:web_search] è§£æåçš„ JSON keys: {list(parsed_result.keys())}")
                except json.JSONDecodeError as e:
                    # å¦‚æœä¸æ˜¯ JSONï¼Œå°è¯•æ–‡æœ¬è§£æ
                    #logger.warning(f"âš ï¸  [Tool:web_search] JSON è§£æå¤±è´¥: {e}")
                    logger.info(f"ğŸ” [Tool:web_search] å°è¯•ä½¿ç”¨æ–‡æœ¬è§£æå™¨...")
                    parsed_result = self._parse_text_search_results(result_text)

                # æ ¼å¼åŒ–ç»“æœ - å°è¯•å¤šç§å¯èƒ½çš„ key
                search_results = []

                # å°è¯•ä¸åŒçš„ç»“æœå­—æ®µ
                if "results" in parsed_result:
                    search_results = parsed_result["results"]
                elif "data" in parsed_result:
                    search_results = parsed_result["data"]
                elif "items" in parsed_result:
                    search_results = parsed_result["items"]
                elif isinstance(parsed_result, list):
                    # å¦‚æœæ•´ä¸ªç»“æœå°±æ˜¯ä¸€ä¸ªåˆ—è¡¨
                    search_results = parsed_result
                else:
                    # å¦‚æœæ˜¯å•ä¸ªå¯¹è±¡ï¼ŒåŒ…è£…æˆåˆ—è¡¨
                    logger.warning(f"âš ï¸  [Tool:web_search] æœªè¯†åˆ«çš„ç»“æœæ ¼å¼ï¼Œkeys: {list(parsed_result.keys())}")
                    search_results = []

                logger.info(f"ğŸ” [Tool:web_search] æå–åˆ° {len(search_results)} ä¸ªåŸå§‹ç»“æœ")

                # ä½¿ç”¨ utils ç»Ÿä¸€æ ¼å¼åŒ–
                formatted_results = self.agent.utils.format_search_results(search_results)

                logger.info(f"âœ… [Tool:web_search] è·å–åˆ° {len(formatted_results)} ä¸ªç»“æœ")

                # æ˜¾ç¤ºç»“æœé¢„è§ˆ
                for idx, item in enumerate(formatted_results[:3], 1):
                    logger.info(f"   {idx}. {item['title'][:50]}... ({item['url']})")

                return {
                    "success": True,
                    "results": formatted_results,
                    "total": len(formatted_results)
                }
            else:
                logger.warning("âš ï¸  [Tool:web_search] æœç´¢å¼•æ“è¿”å›ç©ºç»“æœ")
                return {
                    "success": False,
                    "results": [],
                    "total": 0,
                    "error": "æœªæ‰¾åˆ°ç›¸å…³ç»“æœ"
                }

        except Exception as e:
            logger.error(f"âŒ [Tool:web_search] æœç´¢å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "results": [],
                "total": 0,
                "error": str(e)
            }

    # ========== ç½‘é¡µçˆ¬å–å·¥å…· ==========

    async def scrape_single_url(
        self,
        url: str,
        content_types: Optional[List[str]] = None,
        wait_for: Optional[str] = None,
        timeout: int = 30000
    ) -> Dict:
        """
        çˆ¬å–å•ä¸ªç½‘é¡µå†…å®¹

        è°ƒç”¨ web_scraper MCP çš„ scrape_url å·¥å…·

        Args:
            url: ç›®æ ‡URL
            content_types: å†…å®¹ç±»å‹åˆ—è¡¨ ["html", "text", "json", "screenshot"]
            wait_for: CSSé€‰æ‹©å™¨ï¼ˆç­‰å¾…åŠ¨æ€åŠ è½½ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰

        Returns:
            æ ‡å‡†æ ¼å¼å“åº”ï¼š
            {
                "success": bool,
                "url": str,
                "content": {"html": str, "text": str, "json": dict},
                "files": [str],
                "metadata": {"timestamp": str, "duration_ms": int},
                "error": str (å¦‚æœå¤±è´¥)
            }
        """
        logger.info(f"ğŸŒ [Tool:scrape_single_url] çˆ¬å–URL: {url}")

        # é»˜è®¤å†…å®¹ç±»å‹
        if content_types is None:
            content_types = ["html", "text"]

        logger.info(f"ğŸŒ [Tool:scrape_single_url] å†…å®¹ç±»å‹: {content_types}")

        try:
            # éªŒè¯URL
            if not self.agent.utils.is_valid_url(url):
                logger.error(f"âŒ [Tool:scrape_single_url] æ— æ•ˆçš„URL: {url}")
                return {
                    "success": False,
                    "url": url,
                    "content": {},
                    "files": [],
                    "error": "æ— æ•ˆçš„URLæ ¼å¼"
                }

            # åˆå§‹åŒ– web_scraper MCP client
            mcp_client = await self.agent.utils.init_mcp_client("web_scraper")

            if not mcp_client:
                logger.error("âŒ [Tool:scrape_single_url] web_scraper MCP client æœªåˆå§‹åŒ–")
                return {
                    "success": False,
                    "url": url,
                    "content": {},
                    "files": [],
                    "error": "ç½‘é¡µçˆ¬å–æœåŠ¡ä¸å¯ç”¨"
                }

            # æ„å»ºå‚æ•°
            tool_args = {
                "url": url,
                "content_types": content_types,
                "timeout": timeout
            }

            if wait_for:
                tool_args["wait_for"] = wait_for

            # è°ƒç”¨ MCP å·¥å…·
            logger.info("ğŸŒ [Tool:scrape_single_url] è°ƒç”¨ web_scraper MCP...")
            result = await mcp_client.call_tool(
                tool_name="scrape_url",
                arguments=tool_args
            )

            # è§£æç»“æœ
            if result and isinstance(result, list) and len(result) > 0:
                # result[0] æ˜¯ TextContent å¯¹è±¡ï¼Œç›´æ¥è®¿é—® .text å±æ€§
                result_text = result[0].text

                # è§£æ JSON
                try:
                    parsed_result = json.loads(result_text)
                except json.JSONDecodeError:
                    logger.error("âŒ [Tool:scrape_single_url] è§£æMCPç»“æœå¤±è´¥")
                    return {
                        "success": False,
                        "url": url,
                        "content": {},
                        "files": [],
                        "error": "è§£æçˆ¬å–ç»“æœå¤±è´¥"
                    }

                # æ£€æŸ¥æˆåŠŸæ ‡å¿—
                if parsed_result.get("success"):
                    data = parsed_result.get("data", {})
                    content = data.get("content", {})

                    # æå–æ–‡æœ¬é•¿åº¦ç”¨äºæ—¥å¿—
                    text_length = len(content.get("text", ""))
                    html_length = len(content.get("html", ""))

                    logger.info(f"âœ… [Tool:scrape_single_url] çˆ¬å–æˆåŠŸ")
                    logger.info(f"   - æ–‡æœ¬é•¿åº¦: {text_length} å­—ç¬¦")
                    logger.info(f"   - HTMLé•¿åº¦: {html_length} å­—ç¬¦")
                    logger.info(f"   - ä¿å­˜æ–‡ä»¶: {len(data.get('files', []))} ä¸ª")

                    return {
                        "success": True,
                        "url": data.get("url", url),
                        "content": content,
                        "files": data.get("files", []),
                        "metadata": data.get("metadata", {})
                    }
                else:
                    error_msg = parsed_result.get("error", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ [Tool:scrape_single_url] çˆ¬å–å¤±è´¥: {error_msg}")
                    return {
                        "success": False,
                        "url": url,
                        "content": {},
                        "files": [],
                        "error": error_msg
                    }
            else:
                logger.error("âŒ [Tool:scrape_single_url] MCP è¿”å›ç©ºç»“æœ")
                return {
                    "success": False,
                    "url": url,
                    "content": {},
                    "files": [],
                    "error": "çˆ¬å–æœåŠ¡è¿”å›ç©ºç»“æœ"
                }

        except Exception as e:
            logger.error(f"âŒ [Tool:scrape_single_url] çˆ¬å–å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "url": url,
                "content": {},
                "files": [],
                "error": str(e)
            }

    async def scrape_batch_urls(
        self,
        urls: List[str],
        content_types: Optional[List[str]] = None,
        concurrent_limit: int = 3,
        delay_between: int = 2000
    ) -> Dict:
        """
        æ‰¹é‡çˆ¬å–å¤šä¸ªç½‘é¡µ

        è°ƒç”¨ web_scraper MCP çš„ scrape_batch å·¥å…·

        Args:
            urls: URLåˆ—è¡¨
            content_types: å†…å®¹ç±»å‹
            concurrent_limit: å¹¶å‘æ•°é™åˆ¶
            delay_between: è¯·æ±‚é—´å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰

        Returns:
            æ ‡å‡†æ ¼å¼å“åº”ï¼š
            {
                "success": bool,
                "total": int,
                "succeeded": int,
                "failed": int,
                "results": [
                    {"url": str, "success": bool, "content": {...}, "files": [...], "error": str},
                    ...
                ]
            }
        """
        logger.info(f"ğŸŒ [Tool:scrape_batch_urls] æ‰¹é‡çˆ¬å– {len(urls)} ä¸ªURL")
        logger.info(f"ğŸŒ [Tool:scrape_batch_urls] å¹¶å‘é™åˆ¶: {concurrent_limit}, å»¶è¿Ÿ: {delay_between}ms")

        # é»˜è®¤å†…å®¹ç±»å‹
        if content_types is None:
            content_types = ["text"]  # æ‰¹é‡çˆ¬å–é»˜è®¤åªæå–æ–‡æœ¬

        try:
            # éªŒè¯URLåˆ—è¡¨
            valid_urls = [url for url in urls if self.agent.utils.is_valid_url(url)]

            if len(valid_urls) == 0:
                logger.error("âŒ [Tool:scrape_batch_urls] æ²¡æœ‰æœ‰æ•ˆçš„URL")
                return {
                    "success": False,
                    "total": 0,
                    "succeeded": 0,
                    "failed": 0,
                    "results": [],
                    "error": "æ²¡æœ‰æœ‰æ•ˆçš„URL"
                }

            if len(valid_urls) < len(urls):
                logger.warning(f"âš ï¸  [Tool:scrape_batch_urls] è¿‡æ»¤äº† {len(urls) - len(valid_urls)} ä¸ªæ— æ•ˆURL")

            # åˆå§‹åŒ– MCP client
            mcp_client = await self.agent.utils.init_mcp_client("web_scraper")

            if not mcp_client:
                logger.error("âŒ [Tool:scrape_batch_urls] web_scraper MCP client æœªåˆå§‹åŒ–")
                return {
                    "success": False,
                    "total": 0,
                    "succeeded": 0,
                    "failed": 0,
                    "results": [],
                    "error": "ç½‘é¡µçˆ¬å–æœåŠ¡ä¸å¯ç”¨"
                }

            # è°ƒç”¨ MCP å·¥å…·
            logger.info("ğŸŒ [Tool:scrape_batch_urls] è°ƒç”¨ web_scraper MCP æ‰¹é‡çˆ¬å–...")
            result = await mcp_client.call_tool(
                tool_name="scrape_batch",
                arguments={
                    "urls": valid_urls,
                    "content_types": content_types,
                    "concurrent_limit": concurrent_limit,
                    "delay_between": delay_between
                }
            )

            # è§£æç»“æœ
            if result and isinstance(result, list) and len(result) > 0:
                # result[0] æ˜¯ TextContent å¯¹è±¡ï¼Œç›´æ¥è®¿é—® .text å±æ€§
                result_text = result[0].text

                try:
                    parsed_result = json.loads(result_text)
                except json.JSONDecodeError:
                    logger.error("âŒ [Tool:scrape_batch_urls] è§£æMCPç»“æœå¤±è´¥")
                    return {
                        "success": False,
                        "total": 0,
                        "succeeded": 0,
                        "failed": 0,
                        "results": [],
                        "error": "è§£ææ‰¹é‡çˆ¬å–ç»“æœå¤±è´¥"
                    }

                # æå–ç»Ÿè®¡ä¿¡æ¯
                data = parsed_result.get("data", {})
                total = data.get("total", 0)
                succeeded = data.get("succeeded", 0)
                failed = data.get("failed", 0)
                results = data.get("results", [])

                logger.info(f"âœ… [Tool:scrape_batch_urls] æ‰¹é‡çˆ¬å–å®Œæˆ")
                logger.info(f"   - æ€»æ•°: {total}, æˆåŠŸ: {succeeded}, å¤±è´¥: {failed}")

                # æ˜¾ç¤ºæˆåŠŸçš„ç»“æœé¢„è§ˆ
                successful_results = [r for r in results if r.get("success")]
                for idx, item in enumerate(successful_results[:3], 1):
                    url = item.get("url", "")
                    logger.info(f"   âœ“ {idx}. {url}")

                return {
                    "success": True,
                    "total": total,
                    "succeeded": succeeded,
                    "failed": failed,
                    "results": results
                }
            else:
                logger.error("âŒ [Tool:scrape_batch_urls] MCP è¿”å›ç©ºç»“æœ")
                return {
                    "success": False,
                    "total": 0,
                    "succeeded": 0,
                    "failed": 0,
                    "results": [],
                    "error": "æ‰¹é‡çˆ¬å–æœåŠ¡è¿”å›ç©ºç»“æœ"
                }

        except Exception as e:
            logger.error(f"âŒ [Tool:scrape_batch_urls] æ‰¹é‡çˆ¬å–å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "total": 0,
                "succeeded": 0,
                "failed": 0,
                "results": [],
                "error": str(e)
            }

    # ========== èµ„æºä¸‹è½½å·¥å…·ï¼ˆå¯é€‰ï¼‰==========

    async def download_resources(
        self,
        url: str,
        resource_types: Optional[List[str]] = None,
        selector: Optional[str] = None,
        max_files: int = 50
    ) -> Dict:
        """
        ä»ç½‘é¡µä¸‹è½½èµ„æºæ–‡ä»¶

        è°ƒç”¨ web_scraper MCP çš„ download_resources å·¥å…·

        Args:
            url: ç›®æ ‡URL
            resource_types: èµ„æºç±»å‹ ["images", "pdfs", "videos"]
            selector: CSSé€‰æ‹©å™¨
            max_files: æœ€å¤§ä¸‹è½½æ•°é‡

        Returns:
            æ ‡å‡†æ ¼å¼å“åº”ï¼š
            {
                "success": bool,
                "url": str,
                "downloaded_files": [str],
                "count": int,
                "metadata": {...},
                "error": str (å¦‚æœå¤±è´¥)
            }
        """
        logger.info(f"ğŸ“¥ [Tool:download_resources] ä¸‹è½½èµ„æº: {url}")

        # é»˜è®¤èµ„æºç±»å‹
        if resource_types is None:
            resource_types = ["images"]

        logger.info(f"ğŸ“¥ [Tool:download_resources] èµ„æºç±»å‹: {resource_types}, æœ€å¤§æ•°é‡: {max_files}")

        try:
            # éªŒè¯URL
            if not self.agent.utils.is_valid_url(url):
                logger.error(f"âŒ [Tool:download_resources] æ— æ•ˆçš„URL: {url}")
                return {
                    "success": False,
                    "url": url,
                    "downloaded_files": [],
                    "count": 0,
                    "error": "æ— æ•ˆçš„URLæ ¼å¼"
                }

            # åˆå§‹åŒ– MCP client
            mcp_client = await self.agent.utils.init_mcp_client("web_scraper")

            if not mcp_client:
                logger.error("âŒ [Tool:download_resources] web_scraper MCP client æœªåˆå§‹åŒ–")
                return {
                    "success": False,
                    "url": url,
                    "downloaded_files": [],
                    "count": 0,
                    "error": "èµ„æºä¸‹è½½æœåŠ¡ä¸å¯ç”¨"
                }

            # æ„å»ºå‚æ•°
            tool_args = {
                "url": url,
                "resource_types": resource_types,
                "max_files": max_files
            }

            if selector:
                tool_args["selector"] = selector

            # è°ƒç”¨ MCP å·¥å…·
            logger.info("ğŸ“¥ [Tool:download_resources] è°ƒç”¨ web_scraper MCP...")
            result = await mcp_client.call_tool(
                tool_name="download_resources",
                arguments=tool_args
            )

            # è§£æç»“æœï¼ˆä¸ scrape_single_url ç±»ä¼¼çš„å¤„ç†é€»è¾‘ï¼‰
            if result and isinstance(result, list) and len(result) > 0:
                # result[0] æ˜¯ TextContent å¯¹è±¡ï¼Œç›´æ¥è®¿é—® .text å±æ€§
                result_text = result[0].text

                try:
                    parsed_result = json.loads(result_text)
                except json.JSONDecodeError:
                    logger.error("âŒ [Tool:download_resources] è§£æMCPç»“æœå¤±è´¥")
                    return {
                        "success": False,
                        "url": url,
                        "downloaded_files": [],
                        "count": 0,
                        "error": "è§£æä¸‹è½½ç»“æœå¤±è´¥"
                    }

                if parsed_result.get("success"):
                    data = parsed_result.get("data", {})
                    downloaded_files = data.get("downloaded_files", [])
                    count = data.get("count", 0)

                    logger.info(f"âœ… [Tool:download_resources] ä¸‹è½½æˆåŠŸ: {count} ä¸ªæ–‡ä»¶")

                    return {
                        "success": True,
                        "url": data.get("url", url),
                        "downloaded_files": downloaded_files,
                        "count": count,
                        "metadata": data.get("metadata", {})
                    }
                else:
                    error_msg = parsed_result.get("error", "æœªçŸ¥é”™è¯¯")
                    logger.error(f"âŒ [Tool:download_resources] ä¸‹è½½å¤±è´¥: {error_msg}")
                    return {
                        "success": False,
                        "url": url,
                        "downloaded_files": [],
                        "count": 0,
                        "error": error_msg
                    }
            else:
                logger.error("âŒ [Tool:download_resources] MCP è¿”å›ç©ºç»“æœ")
                return {
                    "success": False,
                    "url": url,
                    "downloaded_files": [],
                    "count": 0,
                    "error": "èµ„æºä¸‹è½½æœåŠ¡è¿”å›ç©ºç»“æœ"
                }

        except Exception as e:
            logger.error(f"âŒ [Tool:download_resources] ä¸‹è½½å¤±è´¥: {e}", exc_info=True)
            return {
                "success": False,
                "url": url,
                "downloaded_files": [],
                "count": 0,
                "error": str(e)
            }
