"""
IndexingAgent å·¥å…·æ–¹æ³•å®ç°

æ‰€æœ‰å¯å¤ç”¨çš„å·¥å…·æ–¹æ³•ï¼ˆ*_implï¼‰
"""

from typing import Dict, List, Any, Optional, TYPE_CHECKING
import logging
import json
import os
import re
from pathlib import Path

if TYPE_CHECKING:
    from .agent import IndexingAgent

logger = logging.getLogger(__name__)


class IndexingTools:
    """IndexingAgent å·¥å…·æ–¹æ³•é›†åˆ"""

    def __init__(self, agent: 'IndexingAgent'):
        """
        Args:
            agent: IndexingAgentå®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.agent = agent

    async def generate_summary_impl(
        self,
        context_data: Dict[str, Any],
        doc_name: str,
        session_id: str = "summary_generation"
    ) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£ç®€è¦æ‘˜è¦ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            context_data: ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå¯ä»¥æ˜¯å…¨æ–‡å†…å®¹æˆ–ç« èŠ‚æ‘˜è¦å­—å…¸ï¼‰
            doc_name: æ–‡æ¡£åç§°
            session_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒè°ƒç”¨åœºæ™¯

        Returns:
            ç®€è¦æ‘˜è¦æ–‡æœ¬
        """
        logger.info(f"ğŸ“ [Tool:generate_summary] ç”Ÿæˆæ‘˜è¦: {doc_name}")

        try:
            from src.config.prompts.common_prompts import CommonRole

            query = (
                "è¯·æŒ‰ç…§æ–‡ç« æœ¬èº«çš„ç« èŠ‚ä¿¡æ¯å’Œå™äº‹ç»“æ„ï¼Œæ•´ç†è¿™ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹ï¼Œ"
                "æ¯ä¸ªç« èŠ‚éƒ½éœ€è¦æœ‰ä¸€å®šçš„ç®€å•ä»‹ç»ã€‚å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸­æœ‰ä¸€äº›æ–‡ç« çš„åŸºæœ¬ä¿¡æ¯ä¹Ÿéœ€è¦ä¸€å¹¶æ€»ç»“ã€‚"
                "ä»…éœ€è¦è¿”å›ç›¸å…³å†…å®¹ï¼Œå¤šä½™çš„è¯æ— éœ€è¿”å›ã€‚è¿”å›ä¸­æ–‡ã€‚"
            )

            # æ„å»ºè¾“å…¥ promptï¼ˆä¸ ReaderBase.get_answer ç›¸åŒçš„æ ¼å¼ï¼‰
            input_prompt = (
                f"è¯·ç»“åˆæ£€ç´¢å›æ¥çš„ä¸Šä¸‹æ–‡ä¿¡æ¯(Context data)å›ç­”å®¢æˆ·é—®é¢˜\n\n"
                f"===== \n\nQuestion: {query}\n\n"
                f"===== \n\nContext data: {context_data}"
            )

            # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼ˆç¦ç”¨å†å²æ€»ç»“ï¼Œæ‘˜è¦ç”Ÿæˆä¸éœ€è¦ä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
            answer = await self.agent.llm.async_call_llm_chain(
                CommonRole.CONTEXT_QA,
                input_prompt,
                session_id,
                enable_llm_summary=False
            )

            if not answer or not answer.strip():
                logger.warning("ç”Ÿæˆçš„ç®€è¦æ‘˜è¦ä¸ºç©º")
                return f"æ–‡æ¡£ {doc_name} çš„ç®€è¦æ‘˜è¦ï¼ˆç”Ÿæˆå¤±è´¥ï¼‰"

            logger.info(f"âœ… [Tool:generate_summary] æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
            return answer

        except Exception as e:
            logger.error(f"âŒ [Tool:generate_summary] ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return f"æ–‡æ¡£ {doc_name} çš„ç®€è¦æ‘˜è¦ï¼ˆç”Ÿæˆé”™è¯¯: {str(e)}ï¼‰"

    async def build_vector_index_impl(
        self,
        doc_name: str,
        chunks: List[Dict[str, str]],
        metadata: Optional[Dict] = None
    ) -> str:
        """
        æ„å»ºå‘é‡ç´¢å¼•ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            doc_name: æ–‡æ¡£åç§°
            chunks: æ–‡æœ¬åˆ†å—åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"data": str, "page": str}, ...]
            metadata: å…ƒæ•°æ®ï¼ˆåŒ…å«tags, summaryç­‰ï¼‰

        Returns:
            ç´¢å¼•è·¯å¾„
        """
        logger.info(f"ğŸ”¨ [Tool:build_index] æ„å»ºå‘é‡ç´¢å¼•: {doc_name}, åˆ†å—æ•°: {len(chunks)}")

        try:
            from src.config.settings import DATA_ROOT
            from src.core.vector_db.vector_db_client import VectorDBClient
            from langchain.docstore.document import Document

            # æ„å»ºç´¢å¼•è·¯å¾„
            index_dir = Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index"
            index_dir.mkdir(parents=True, exist_ok=True)
            index_path = str(index_dir)

            # åˆ›å»º VectorDBClientï¼Œç›´æ¥ä½¿ç”¨ self.agent.embedding_model
            vector_db_client = VectorDBClient(index_path, embedding_model=self.agent.embedding_model)

            # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
            vector_db_docs = []

            # æå–å…ƒæ•°æ®
            summary = metadata.get("summary", "") if metadata else ""

            # ä¸ºæ¯ä¸ªåˆ†å—åˆ›å»ºDocumentå¯¹è±¡
            for i, chunk_item in enumerate(chunks):
                chunk_data = chunk_item.get("data", "")
                chunk_page = chunk_item.get("page", f"chunk_{i+1}")

                if not chunk_data or not chunk_data.strip():
                    continue

                # åˆ›å»ºå†…å®¹æ–‡æ¡£
                doc = Document(
                    page_content=chunk_data,
                    metadata={
                        "type": "content",
                        "chunk_id": i,
                        "page": chunk_page,
                        "doc_name": doc_name,
                    }
                )
                vector_db_docs.append(doc)

            # æ·»åŠ æ–‡æ¡£ç»“æ„ä¿¡æ¯
            structure_doc = Document(
                page_content="Document Structure",
                metadata={
                    "type": "structure",
                    "doc_name": doc_name,
                    "total_chunks": len(chunks),
                    "summary": summary,
                }
            )
            vector_db_docs.append(structure_doc)

            # æ„å»ºå‘é‡æ•°æ®åº“
            logger.info(f"å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“ï¼Œå…± {len(vector_db_docs)} ä¸ªæ–‡æ¡£...")
            vector_db_client.build_vector_db(vector_db_docs)

            logger.info(f"âœ… [Tool:build_index] ç´¢å¼•æ„å»ºå®Œæˆ: {index_path}")
            return index_path

        except Exception as e:
            logger.error(f"âŒ [Tool:build_index] ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise

    async def extract_pdf_data_impl(self, pdf_file_path: str) -> Dict[str, Any]:
        """
        å°† PDF è½¬ä¸ºå›¾ç‰‡å¹¶ç”¨ LLM æå–æ¯é¡µå†…å®¹ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            pdf_file_path: PDF æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰

        Returns:
            æå–ç»“æœå­—å…¸:
            {
                "pdf_data_list": List[Dict],  # æ¯é¡µæå–çš„å†…å®¹
                "image_paths": List[str],      # å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
                "json_path": str,              # JSONæ•°æ®æ–‡ä»¶è·¯å¾„
                "image_folder": str            # å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
            }

        Raises:
            ValueError: è¾“å…¥å‚æ•°æ— æ•ˆ
            FileNotFoundError: PDFæ–‡ä»¶ä¸å­˜åœ¨
            Exception: å¤„ç†è¿‡ç¨‹ä¸­çš„å…¶ä»–é”™è¯¯
        """
        from src.utils.helpers import pdf_to_images, read_images_in_directory
        from src.config.prompts.indexing_prompts import IndexingRole, INDEXING_PROMPTS

        logger.info(f"ğŸ“„ [Tool:extract_pdf] ========== å¼€å§‹æå–PDFå†…å®¹ ==========")
        logger.info(f"ğŸ“„ [Tool:extract_pdf] è¾“å…¥æ–‡ä»¶å: {pdf_file_path}")

        # è¾“å…¥éªŒè¯
        if not pdf_file_path or not isinstance(pdf_file_path, str):
            raise ValueError("PDFæ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©ºä¸”å¿…é¡»æ˜¯å­—ç¬¦ä¸²")

        # æ„å»ºè·¯å¾„
        output_folder_path = os.path.join(self.agent.pdf_image_path, pdf_file_path)
        pdf_path = os.path.join(self.agent.pdf_path, f"{pdf_file_path}.pdf")
        # JSONæ–‡ä»¶æ”¾åœ¨æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­
        doc_json_folder = os.path.join(self.agent.json_data_path, pdf_file_path)
        output_json_path = os.path.join(doc_json_folder, "data.json")

        logger.info(f"ğŸ“„ [Tool:extract_pdf] å®Œæ•´è·¯å¾„:")
        logger.info(f"ğŸ“„ [Tool:extract_pdf]   - PDF: {pdf_path}")
        logger.info(f"ğŸ“„ [Tool:extract_pdf]   - å›¾ç‰‡æ–‡ä»¶å¤¹: {output_folder_path}")
        logger.info(f"ğŸ“„ [Tool:extract_pdf]   - JSONè¾“å‡º: {output_json_path}")

        # éªŒè¯PDFæ–‡ä»¶å­˜åœ¨
        if not os.path.exists(pdf_path):
            logger.error(f"ğŸ“„ [Tool:extract_pdf] âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

        logger.info(f"ğŸ“„ [Tool:extract_pdf] âœ… PDFæ–‡ä»¶å­˜åœ¨")

        try:
            image_paths = []

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰JSONæ•°æ®
            if os.path.exists(output_json_path):
                logger.info(f"å‘ç°å·²å­˜åœ¨çš„JSONæ•°æ®: {output_json_path}")
                try:
                    with open(output_json_path, 'r', encoding='utf-8') as f:
                        image_content_list = json.load(f)

                    # è·å–å·²å­˜åœ¨çš„å›¾ç‰‡è·¯å¾„
                    if os.path.exists(output_folder_path):
                        image_paths = read_images_in_directory(output_folder_path)
                        # æ’åºå›¾ç‰‡è·¯å¾„
                        def safe_page_sort(path):
                            try:
                                match = re.search(r'page_(\d+)\.png', path)
                                return int(match.group(1)) if match else float('inf')
                            except:
                                return float('inf')
                        image_paths = sorted(image_paths, key=safe_page_sort)

                    logger.info(f"âœ… [Tool:extract_pdf] ä»ç¼“å­˜åŠ è½½: {len(image_content_list)} é¡µ")

                    return {
                        "pdf_data_list": image_content_list,
                        "image_paths": image_paths,
                        "json_path": output_json_path,
                        "image_folder": output_folder_path
                    }
                except Exception as e:
                    logger.warning(f"è¯»å–ç¼“å­˜JSONå¤±è´¥ï¼Œå°†é‡æ–°æå–: {e}")

            # è½¬æ¢PDFä¸ºå›¾ç‰‡
            logger.info(f"ğŸ“„ [Tool:extract_pdf] å¼€å§‹è½¬æ¢PDFä¸ºå›¾ç‰‡...")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - DPI: {self.agent.pdf_dpi}")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - è´¨é‡é¢„è®¾: {self.agent.pdf_quality}")
            conversion_stats = pdf_to_images(
                pdf_path, output_folder_path,
                dpi=self.agent.pdf_dpi, quality=self.agent.pdf_quality
            )
            logger.info(f"ğŸ“„ [Tool:extract_pdf] âœ… PDFè½¬å›¾ç‰‡å®Œæˆ: æˆåŠŸ {conversion_stats['successful_pages']} é¡µ")

            # è·å–å›¾ç‰‡è·¯å¾„å¹¶æ’åº
            image_paths = read_images_in_directory(output_folder_path)
            if not image_paths:
                logger.error("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„å›¾ç‰‡æ–‡ä»¶")
                return {
                    "pdf_data_list": [],
                    "image_paths": [],
                    "json_path": output_json_path,
                    "image_folder": output_folder_path
                }

            # å®‰å…¨çš„é¡µç æ’åº
            def safe_page_sort(path):
                try:
                    match = re.search(r'page_(\d+)\.png', path)
                    return int(match.group(1)) if match else float('inf')
                except:
                    return float('inf')

            sorted_image_paths = sorted(image_paths, key=safe_page_sort)
            logger.info(f"æ‰¾åˆ° {len(sorted_image_paths)} ä¸ªå›¾ç‰‡æ–‡ä»¶å¾…å¤„ç†")

            # ä½¿ç”¨å¹¶è¡Œå¤„ç†æå–å›¾ç‰‡å†…å®¹
            extract_prompt = INDEXING_PROMPTS.get(
                IndexingRole.IMAGE_EXTRACT, "è¯·æå–å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹"
            )
            logger.info(f"ğŸ“„ [Tool:extract_pdf] å¼€å§‹å¹¶è¡Œæå–å›¾ç‰‡å†…å®¹...")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - å›¾ç‰‡æ•°é‡: {len(sorted_image_paths)}")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - æœ€å¤§å¹¶å‘: 5")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - ä½¿ç”¨LLM: {self.agent.llm.provider}")

            # ç›´æ¥ä½¿ç”¨å¼‚æ­¥æ–¹æ³•ï¼ˆå› ä¸ºå½“å‰å·²ç»åœ¨asyncä¸Šä¸‹æ–‡ä¸­ï¼‰
            from src.core.processing.parallel_processor import PageExtractor
            extractor = PageExtractor(self.agent.llm, extract_prompt, max_concurrent=5)
            image_content_list = await extractor.extract_pages_parallel(sorted_image_paths)

            logger.info(f"ğŸ“„ [Tool:extract_pdf] âœ… å›¾ç‰‡å†…å®¹æå–å®Œæˆ")

            # ä¿å­˜æå–ç»“æœåˆ°JSONæ–‡ä»¶
            if image_content_list:
                try:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)

                    with open(output_json_path, 'w', encoding='utf-8') as file:
                        json.dump(image_content_list, file, ensure_ascii=False, indent=2)

                    logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_json_path}")
                    logger.info(f"âœ… [Tool:extract_pdf] æå–ç»Ÿè®¡: æˆåŠŸ{len(image_content_list)}é¡µ")
                except Exception as e:
                    logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
                    raise
            else:
                logger.error("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•é¡µé¢å†…å®¹")

            return {
                "pdf_data_list": image_content_list,
                "image_paths": sorted_image_paths,
                "json_path": output_json_path,
                "image_folder": output_folder_path
            }

        except Exception as e:
            logger.error(f"âŒ [Tool:extract_pdf] PDFæ•°æ®æå–å¤±è´¥: {e}")
            raise

    def split_pdf_raw_data(self, pdf_raw_data: List[Any]) -> List[List[Any]]:
        """
        å°† PDF åŸå§‹æ•°æ®æŒ‰ç…§ chunk_count è¿›è¡Œåˆ‡åˆ†

        Args:
            pdf_raw_data: PDFåŸå§‹æ•°æ®åˆ—è¡¨

        Returns:
            åˆ‡åˆ†åçš„æ•°æ®å—åˆ—è¡¨
        """
        if not isinstance(pdf_raw_data, list):
            logger.error("pdf_raw_data ä¸æ˜¯ listï¼Œæ— æ³•åˆ‡åˆ†")
            return []

        chunks = [
            pdf_raw_data[i:i + self.agent.chunk_count]
            for i in range(0, len(pdf_raw_data), self.agent.chunk_count)
        ]
        logger.info(f"å·²å°† pdf_raw_data åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—ï¼Œæ¯å—æœ€å¤š {self.agent.chunk_count} æ¡")
        return chunks

    async def extract_toc_from_pages_impl(
        self,
        pdf_data_list: List[Dict[str, str]],
        max_pages: int = 10
    ) -> tuple[Optional[Dict[str, List]], bool]:
        """
        ä»PDFå‰å‡ é¡µå¿«é€Ÿæå–ç›®å½•ç»“æ„ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            pdf_data_list: PDFæ¯é¡µæ•°æ®åˆ—è¡¨
            max_pages: æœ€å¤šæ£€æŸ¥çš„é¡µæ•°

        Returns:
            (agenda_dict, has_toc): ç›®å½•å­—å…¸å’Œæ˜¯å¦æ‰¾åˆ°ç›®å½•çš„æ ‡å¿—
        """
        from src.utils.helpers import extract_data_from_LLM_res
        from src.config.prompts.indexing_prompts import IndexingRole

        logger.info(f"ğŸ“– [Tool:extract_toc] å°è¯•ä»å‰ {max_pages} é¡µæå–ç›®å½•")

        try:
            # åˆå¹¶å‰å‡ é¡µçš„å†…å®¹
            toc_pages = pdf_data_list[:max_pages]
            combined_content = "\n\n".join([
                f"[Page {item.get('page', i+1)}]\n{item.get('data', '')}"
                for i, item in enumerate(toc_pages)
            ])

            # æ„å»ºæå–ç›®å½•çš„ prompt
            input_prompt = f"è¿™é‡Œæ˜¯æ–‡ç« çš„å‰ {len(toc_pages)} é¡µå†…å®¹ï¼Œè¯·æŸ¥æ‰¾å¹¶æå–ç›®å½•ç»“æ„: {combined_content}"

            # è°ƒç”¨ LLM æå–ç›®å½•
            response = self.agent.llm.call_llm_chain(
                IndexingRole.CHAPTER_EXTRACT,
                input_prompt,
                "toc_extract"
            )

            if not response:
                logger.warning("LLMè¿”å›ç©ºå“åº”ï¼Œæœªæ‰¾åˆ°ç›®å½•")
                return None, False

            # è§£æ LLM è¿”å›çš„ç»“æœ
            result = extract_data_from_LLM_res(response)

            if not result or not isinstance(result, list) or len(result) == 0:
                logger.info("æœªåœ¨å‰å‡ é¡µæ£€æµ‹åˆ°ç›®å½•ç»“æ„")
                return None, False

            # è½¬æ¢ä¸º agenda_dict æ ¼å¼: {title: [pages]}
            agenda_dict = {}
            for item in result:
                if isinstance(item, dict) and "title" in item and "pages" in item:
                    title = item["title"]
                    pages = item["pages"]
                    if isinstance(pages, list):
                        agenda_dict[title] = pages
                    else:
                        agenda_dict[title] = [pages]

            if agenda_dict:
                logger.info(f"âœ… [Tool:extract_toc] æˆåŠŸæå–ç›®å½•: {len(agenda_dict)} ä¸ªç« èŠ‚")
                return agenda_dict, True
            else:
                logger.info("è§£æç»“æœä¸ºç©ºï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆç›®å½•")
                return None, False

        except Exception as e:
            logger.error(f"âŒ [Tool:extract_toc] æå–ç›®å½•å¤±è´¥: {e}")
            return None, False

    async def analyze_full_structure_impl(
        self,
        pdf_data_list: List[Dict[str, str]]
    ) -> Dict[str, List]:
        """
        åˆ†ææ•´ä¸ªPDFæ–‡æ¡£çš„ç»“æ„ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        å½“PDFæ²¡æœ‰æ˜ç¡®ç›®å½•æ—¶ï¼Œéå†å…¨æ–‡åˆ†æç« èŠ‚ç»“æ„

        Args:
            pdf_data_list: PDFæ¯é¡µæ•°æ®åˆ—è¡¨

        Returns:
            agenda_dict: ç›®å½•å­—å…¸ {title: [pages]}
        """
        from src.utils.helpers import extract_data_from_LLM_res, group_data_by_sections_with_titles
        from src.config.prompts.indexing_prompts import IndexingRole

        logger.info(f"ğŸ” [Tool:analyze_structure] å¼€å§‹åˆ†æå…¨æ–‡ç»“æ„: {len(pdf_data_list)} é¡µ")

        try:
            # å°†PDFæ•°æ®åˆ†å—å¤„ç†ï¼ˆé¿å…å•æ¬¡å¤„ç†è¿‡é•¿ï¼‰
            chunks = self.split_pdf_raw_data(pdf_data_list)

            all_agenda_list = []

            # å¹¶è¡Œå¤„ç†æ¯ä¸ªåˆ†å—
            for i, chunk in enumerate(chunks):
                logger.info(f"å¤„ç†åˆ†å— {i+1}/{len(chunks)}")

                # åˆå¹¶åˆ†å—å†…å®¹
                chunk_content = "\n\n".join([
                    f"[Page {item.get('page', idx+1)}]\n{item.get('data', '')}"
                    for idx, item in enumerate(chunk)
                ])

                # æ„å»º prompt
                input_prompt = f"è¿™é‡Œæ˜¯æ–‡ç« çš„éƒ¨åˆ†å†…å®¹: {chunk_content}"

                # è°ƒç”¨ LLM æå–ç« èŠ‚
                response = self.agent.llm.call_llm_chain(
                    IndexingRole.CHAPTER_EXTRACT,
                    input_prompt,
                    f"structure_extract_chunk"
                )

                if response:
                    result = extract_data_from_LLM_res(response)
                    if isinstance(result, list):
                        all_agenda_list.extend(result)

            # è½¬æ¢ä¸º agenda_dict
            _, agenda_list = group_data_by_sections_with_titles(all_agenda_list, pdf_data_list)

            # å°†åˆ—è¡¨æ ¼å¼è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            agenda_dict = {
                item['title']: item['pages']
                for item in agenda_list
            }

            logger.info(f"âœ… [Tool:analyze_structure] ç»“æ„åˆ†æå®Œæˆ: {len(agenda_dict)} ä¸ªç« èŠ‚")

            return agenda_dict

        except Exception as e:
            logger.error(f"âŒ [Tool:analyze_structure] ç»“æ„åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æ„ï¼ˆæ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªç« èŠ‚ï¼‰
            return {"å…¨æ–‡": list(range(1, len(pdf_data_list) + 1))}

    # ==================== æ‰¹é‡å¤„ç†å’Œç®¡ç†æ–¹æ³• ====================

    async def process_documents_batch(
        self,
        doc_list: List[Dict[str, Any]],
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†æ–‡æ¡£åˆ—è¡¨

        Args:
            doc_list: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼ï¼š
                {
                    "doc_name": str,
                    "doc_path": str,
                    "doc_type": "pdf" | "url"
                }
            max_concurrent: æœ€å¤§å¹¶å‘å¤„ç†æ•°

        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        import asyncio

        logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å¤„ç†æ–‡æ¡£: å…± {len(doc_list)} ä¸ªæ–‡æ¡£")

        results = []

        # åˆ†æ‰¹å¤„ç†é¿å…è¿‡è½½
        for i in range(0, len(doc_list), max_concurrent):
            batch = doc_list[i:i + max_concurrent]
            logger.info(f"å¤„ç†ç¬¬ {i // max_concurrent + 1} æ‰¹: {len(batch)} ä¸ªæ–‡æ¡£")

            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
            tasks = []
            for doc_info in batch:
                # æ„å»ºåˆå§‹çŠ¶æ€
                state = {
                    "doc_name": doc_info["doc_name"],
                    "doc_path": doc_info["doc_path"],
                    "doc_type": doc_info["doc_type"],
                    "status": "pending"
                }
                # åˆ›å»ºå¤„ç†ä»»åŠ¡
                task = self.agent.graph.ainvoke(state)
                tasks.append(task)

            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†ç»“æœ
            for j, result in enumerate(batch_results):
                doc_name = batch[j]["doc_name"]
                if isinstance(result, Exception):
                    logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {doc_name}, é”™è¯¯: {result}")
                    results.append({
                        "doc_name": doc_name,
                        "status": "error",
                        "error": str(result)
                    })
                else:
                    logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {doc_name}, çŠ¶æ€: {result.get('status')}")
                    results.append(result)

        logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {sum(1 for r in results if r.get('status') == 'completed')} ä¸ª, å¤±è´¥ {sum(1 for r in results if r.get('status') == 'error')} ä¸ª")

        return results

    async def rebuild_from_structure(
        self,
        doc_name: str,
        doc_path: str
    ) -> Dict[str, Any]:
        """
        åŸºäºå·²æœ‰çš„ structure.json é‡å»ºæ–‡æ¡£æ•°æ®

        ä¿æŒä¸å˜çš„æ–‡ä»¶ï¼š
        - structure.json: æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æ„
        - data.json: PDF åŸå§‹æ•°æ®
        - pdf_image/: PDF å›¾ç‰‡æ–‡ä»¶

        é‡æ–°ç”Ÿæˆçš„å†…å®¹ï¼š
        - chunks.json: åŸºäºæ–°ç»“æ„é‡å»ºç« èŠ‚æ•°æ®
        - ç« èŠ‚æ‘˜è¦: é‡æ–°ç”Ÿæˆæ‰€æœ‰ç« èŠ‚çš„æ‘˜è¦å’Œé‡æ„å†…å®¹
        - å‘é‡æ•°æ®åº“: å®Œå…¨é‡å»º FAISS ç´¢å¼•
        - ç®€è¦æ‘˜è¦: é‡æ–°ç”Ÿæˆæ•´ä½“æ–‡æ¡£æ‘˜è¦

        Args:
            doc_name: æ–‡æ¡£åç§°
            doc_path: æ–‡æ¡£è·¯å¾„

        Returns:
            é‡å»ºç»“æœå­—å…¸
        """
        logger.info(f"ğŸ”„ [Rebuild] ========== å¼€å§‹ä» structure å…¨é¢é‡å»º ==========")
        logger.info(f"ğŸ”„ [Rebuild] æ–‡æ¡£: {doc_name}")
        logger.info(f"ğŸ”„ [Rebuild] é‡å»ºå†…å®¹: chunks + summaries + vectordb + brief_summary")

        try:
            # 1. åŠ è½½å·²æœ‰æ•°æ®
            doc_json_folder = os.path.join(self.agent.json_data_path, doc_name)
            structure_path = os.path.join(doc_json_folder, "structure.json")
            data_path = os.path.join(doc_json_folder, "data.json")

            if not os.path.exists(structure_path):
                raise FileNotFoundError(f"ç»“æ„æ–‡ä»¶ä¸å­˜åœ¨: {structure_path}")

            if not os.path.exists(data_path):
                raise FileNotFoundError(f"æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {data_path}")

            # åŠ è½½ structure
            logger.info(f"ğŸ“¥ [Rebuild] åŠ è½½ structure.json...")
            with open(structure_path, 'r', encoding='utf-8') as f:
                structure_data = json.load(f)

            if "agenda_dict" in structure_data:
                agenda_dict = structure_data["agenda_dict"]
                has_toc = structure_data.get("has_toc", False)
            else:
                agenda_dict = structure_data
                has_toc = True

            logger.info(f"   âœ… åŠ è½½å®Œæˆ: {len(agenda_dict)} ä¸ªç« èŠ‚")

            # åŠ è½½ PDF æ•°æ®
            logger.info(f"ğŸ“¥ [Rebuild] åŠ è½½ data.json...")
            with open(data_path, 'r', encoding='utf-8') as f:
                pdf_data_list = json.load(f)

            json_data_dict = {
                str(item.get("page", i+1)): item.get("data", "")
                for i, item in enumerate(pdf_data_list)
            }

            raw_data = "\n\n".join([
                f"[Page {item.get('page', i+1)}]\n{item.get('data', '')}"
                for i, item in enumerate(pdf_data_list)
            ])

            logger.info(f"   âœ… åŠ è½½å®Œæˆ: {len(pdf_data_list)} é¡µ")

            # 2. æ„å»ºåˆå§‹çŠ¶æ€
            state = {
                "doc_name": doc_name,
                "doc_path": doc_path,
                "doc_type": "pdf",
                "pdf_data_list": pdf_data_list,
                "json_data_dict": json_data_dict,
                "raw_data": raw_data,
                "agenda_dict": agenda_dict,
                "has_toc": has_toc,
                "status": "loaded",
                "is_complete": False,
                "generated_files": {
                    "images": [],
                    "json_data": data_path,
                    "vector_db": "",
                    "summaries": []
                },
                "stage_status": {}
            }

            # 3-7. é‡å»ºå„é˜¶æ®µï¼ˆé€šè¿‡è°ƒç”¨ nodes æ¨¡å—ï¼‰
            # è¿™äº›è°ƒç”¨å°†åœ¨ nodes.py ä¸­å®ç°
            logger.info(f"ğŸ”„ [Rebuild] æ­¥éª¤1: é‡å»ºç« èŠ‚æ•°æ®...")
            # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦è®¿é—® nodesï¼Œä½† nodes è¿˜æœªåˆ›å»º
            # ä¸´æ—¶æ–¹æ¡ˆï¼šåœ¨ agent.py ä¸­å®ç° rebuildï¼Œæˆ–è€…å°† rebuild ç§»åˆ° agent.py

            logger.info(f"âœ… [Rebuild] é‡å»ºå®Œæˆï¼")

            return {
                "success": True,
                "doc_name": doc_name,
                "total_chapters": len(agenda_dict),
                "status": "completed",
            }

        except Exception as e:
            logger.error(f"âŒ [Rebuild] é‡å»ºå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "success": False,
                "error": str(e)
            }
