"""
IndexingAgent Â∑•ÂÖ∑ÊñπÊ≥ïÂÆûÁé∞

ÊâÄÊúâÂèØÂ§çÁî®ÁöÑÂ∑•ÂÖ∑ÊñπÊ≥ïÔºà*_implÔºâ
"""

from typing import Dict, List, Any, Optional, TYPE_CHECKING
import logging
import json
import os
import re
from pathlib import Path

if TYPE_CHECKING:
    from .agent import IndexingAgent

logger = logging.getLogger(__name__)


class IndexingTools:
    """IndexingAgent Â∑•ÂÖ∑ÊñπÊ≥ïÈõÜÂêà"""

    def __init__(self, agent: 'IndexingAgent'):
        """
        Args:
            agent: IndexingAgentÂÆû‰æãÔºà‰æùËµñÊ≥®ÂÖ•Ôºâ
        """
        self.agent = agent

    async def generate_summary_impl(
        self,
        context_data: Dict[str, Any],
        doc_name: str,
        session_id: str = "summary_generation"
    ) -> str:
        """
        ÁîüÊàêÊñáÊ°£ÁÆÄË¶ÅÊëòË¶ÅÔºàÂ∑•ÂÖ∑ÊñπÊ≥ïÔºâ

        Args:
            context_data: ‰∏ä‰∏ãÊñáÊï∞ÊçÆÔºàÂèØ‰ª•ÊòØÂÖ®ÊñáÂÜÖÂÆπÊàñÁ´†ËäÇÊëòË¶ÅÂ≠óÂÖ∏Ôºâ
            doc_name: ÊñáÊ°£ÂêçÁß∞
            session_id: ‰ºöËØùIDÔºåÁî®‰∫éÂå∫ÂàÜ‰∏çÂêåË∞ÉÁî®Âú∫ÊôØ

        Returns:
            ÁÆÄË¶ÅÊëòË¶ÅÊñáÊú¨
        """
        logger.info(f"üìù [Tool:generate_summary] ÁîüÊàêÊëòË¶Å: {doc_name}")

        try:
            from src.agents.common.prompts import CommonRole

            query = (
                "ËØ∑ÊåâÁÖßÊñáÁ´†Êú¨Ë∫´ÁöÑÁ´†ËäÇ‰ø°ÊÅØÂíåÂèô‰∫ãÁªìÊûÑÔºåÊï¥ÁêÜËøôÁØáÊñáÁ´†ÁöÑ‰∏ªË¶ÅÂÜÖÂÆπÔºå"
                "ÊØè‰∏™Á´†ËäÇÈÉΩÈúÄË¶ÅÊúâ‰∏ÄÂÆöÁöÑÁÆÄÂçï‰ªãÁªç„ÄÇÂ¶ÇÊûúËÉåÊôØÁü•ËØÜ‰∏≠Êúâ‰∏Ä‰∫õÊñáÁ´†ÁöÑÂü∫Êú¨‰ø°ÊÅØ‰πüÈúÄË¶Å‰∏ÄÂπ∂ÊÄªÁªì„ÄÇ"
                "‰ªÖÈúÄË¶ÅËøîÂõûÁõ∏ÂÖ≥ÂÜÖÂÆπÔºåÂ§ö‰ΩôÁöÑËØùÊó†ÈúÄËøîÂõû„ÄÇËøîÂõû‰∏≠Êñá„ÄÇ"
            )

            # ÊûÑÂª∫ËæìÂÖ• promptÔºà‰∏é ReaderBase.get_answer Áõ∏ÂêåÁöÑÊ†ºÂºèÔºâ
            input_prompt = (
                f"ËØ∑ÁªìÂêàÊ£ÄÁ¥¢ÂõûÊù•ÁöÑ‰∏ä‰∏ãÊñá‰ø°ÊÅØ(Context data)ÂõûÁ≠îÂÆ¢Êà∑ÈóÆÈ¢ò\n\n"
                f"===== \n\nQuestion: {query}\n\n"
                f"===== \n\nContext data: {context_data}"
            )

            # ‰ΩøÁî®ÂºÇÊ≠•Ë∞ÉÁî®ÔºàÁ¶ÅÁî®ÂéÜÂè≤ÊÄªÁªìÔºåÊëòË¶ÅÁîüÊàê‰∏çÈúÄË¶Å‰øùÁïô‰∏ä‰∏ãÊñáÔºâ
            answer = await self.agent.llm.async_call_llm_chain(
                CommonRole.CONTEXT_QA,
                input_prompt,
                session_id,
                enable_llm_summary=False
            )

            if not answer or not answer.strip():
                logger.warning("ÁîüÊàêÁöÑÁÆÄË¶ÅÊëòË¶Å‰∏∫Á©∫")
                return f"ÊñáÊ°£ {doc_name} ÁöÑÁÆÄË¶ÅÊëòË¶ÅÔºàÁîüÊàêÂ§±Ë¥•Ôºâ"

            logger.info(f"‚úÖ [Tool:generate_summary] ÊëòË¶ÅÁîüÊàêÂÆåÊàêÔºåÈïøÂ∫¶: {len(answer)} Â≠óÁ¨¶")
            return answer

        except Exception as e:
            logger.error(f"‚ùå [Tool:generate_summary] ÁîüÊàêÊëòË¶ÅÂ§±Ë¥•: {e}")
            return f"ÊñáÊ°£ {doc_name} ÁöÑÁÆÄË¶ÅÊëòË¶ÅÔºàÁîüÊàêÈîôËØØ: {str(e)}Ôºâ"

    async def build_vector_index_impl(
        self,
        doc_name: str,
        chunks: List[Dict[str, str]],
        metadata: Optional[Dict] = None
    ) -> str:
        """
        ÊûÑÂª∫ÂêëÈáèÁ¥¢ÂºïÔºàÂ∑•ÂÖ∑ÊñπÊ≥ïÔºâ

        Args:
            doc_name: ÊñáÊ°£ÂêçÁß∞
            chunks: ÊñáÊú¨ÂàÜÂùóÂàóË°®ÔºåÊ†ºÂºèÔºö[{"data": str, "page": str}, ...]
            metadata: ÂÖÉÊï∞ÊçÆÔºàÂåÖÂê´tags, summaryÁ≠âÔºâ

        Returns:
            Á¥¢ÂºïË∑ØÂæÑ
        """
        logger.info(f"üî® [Tool:build_index] ÊûÑÂª∫ÂêëÈáèÁ¥¢Âºï: {doc_name}, ÂàÜÂùóÊï∞: {len(chunks)}")

        try:
            from src.config.settings import DATA_ROOT
            from src.core.vector_db.vector_db_client import VectorDBClient
            from langchain.docstore.document import Document

            # ÊûÑÂª∫Á¥¢ÂºïË∑ØÂæÑ
            index_dir = Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index"
            index_dir.mkdir(parents=True, exist_ok=True)
            index_path = str(index_dir)

            # ÂàõÂª∫ VectorDBClientÔºåÁõ¥Êé•‰ΩøÁî® self.agent.embedding_model
            vector_db_client = VectorDBClient(index_path, embedding_model=self.agent.embedding_model)

            # ÂáÜÂ§áÊñáÊ°£ÂàóË°®
            vector_db_docs = []

            # ÊèêÂèñÂÖÉÊï∞ÊçÆ
            summary = metadata.get("summary", "") if metadata else ""

            # ‰∏∫ÊØè‰∏™ÂàÜÂùóÂàõÂª∫DocumentÂØπË±°
            for i, chunk_item in enumerate(chunks):
                chunk_data = chunk_item.get("data", "")
                chunk_page = chunk_item.get("page", f"chunk_{i+1}")

                if not chunk_data or not chunk_data.strip():
                    continue

                # ÂàõÂª∫ÂÜÖÂÆπÊñáÊ°£
                doc = Document(
                    page_content=chunk_data,
                    metadata={
                        "type": "content",
                        "chunk_id": i,
                        "page": chunk_page,
                        "doc_name": doc_name,
                    }
                )
                vector_db_docs.append(doc)

            # Ê∑ªÂä†ÊñáÊ°£ÁªìÊûÑ‰ø°ÊÅØ
            structure_doc = Document(
                page_content="Document Structure",
                metadata={
                    "type": "structure",
                    "doc_name": doc_name,
                    "total_chunks": len(chunks),
                    "summary": summary,
                }
            )
            vector_db_docs.append(structure_doc)

            # ÊûÑÂª∫ÂêëÈáèÊï∞ÊçÆÂ∫ì
            logger.info(f"ÂºÄÂßãÊûÑÂª∫ÂêëÈáèÊï∞ÊçÆÂ∫ìÔºåÂÖ± {len(vector_db_docs)} ‰∏™ÊñáÊ°£...")
            vector_db_client.build_vector_db(vector_db_docs)

            logger.info(f"‚úÖ [Tool:build_index] Á¥¢ÂºïÊûÑÂª∫ÂÆåÊàê: {index_path}")
            return index_path

        except Exception as e:
            logger.error(f"‚ùå [Tool:build_index] Á¥¢ÂºïÊûÑÂª∫Â§±Ë¥•: {e}")
            raise

    async def extract_pdf_data_impl(self, pdf_file_path: str) -> Dict[str, Any]:
        """
        Â∞Ü PDF ËΩ¨‰∏∫ÂõæÁâáÂπ∂Áî® LLM ÊèêÂèñÊØèÈ°µÂÜÖÂÆπÔºàÂ∑•ÂÖ∑ÊñπÊ≥ïÔºâ

        Args:
            pdf_file_path: PDF Êñá‰ª∂ÂêçÔºà‰∏çÂê´Ë∑ØÂæÑÂíåÊâ©Â±ïÂêçÔºâ

        Returns:
            ÊèêÂèñÁªìÊûúÂ≠óÂÖ∏:
            {
                "pdf_data_list": List[Dict],  # ÊØèÈ°µÊèêÂèñÁöÑÂÜÖÂÆπ
                "image_paths": List[str],      # ÂõæÁâáÊñá‰ª∂Ë∑ØÂæÑÂàóË°®
                "json_path": str,              # JSONÊï∞ÊçÆÊñá‰ª∂Ë∑ØÂæÑ
                "image_folder": str            # ÂõæÁâáÊñá‰ª∂Â§πË∑ØÂæÑ
            }

        Raises:
            ValueError: ËæìÂÖ•ÂèÇÊï∞Êó†Êïà
            FileNotFoundError: PDFÊñá‰ª∂‰∏çÂ≠òÂú®
            Exception: Â§ÑÁêÜËøáÁ®ã‰∏≠ÁöÑÂÖ∂‰ªñÈîôËØØ
        """
        from src.utils.helpers import pdf_to_images, read_images_in_directory
        from .prompts import IndexingRole, INDEXING_PROMPTS

        logger.info(f"üìÑ [Tool:extract_pdf] ========== ÂºÄÂßãÊèêÂèñPDFÂÜÖÂÆπ ==========")
        logger.info(f"üìÑ [Tool:extract_pdf] ËæìÂÖ•Êñá‰ª∂Âêç: {pdf_file_path}")

        # ËæìÂÖ•È™åËØÅ
        if not pdf_file_path or not isinstance(pdf_file_path, str):
            raise ValueError("PDFÊñá‰ª∂Ë∑ØÂæÑ‰∏çËÉΩ‰∏∫Á©∫‰∏îÂøÖÈ°ªÊòØÂ≠óÁ¨¶‰∏≤")

        # ÊûÑÂª∫Ë∑ØÂæÑ
        output_folder_path = os.path.join(self.agent.pdf_image_path, pdf_file_path)
        pdf_path = os.path.join(self.agent.pdf_path, f"{pdf_file_path}.pdf")
        # JSONÊñá‰ª∂ÊîæÂú®ÊñáÊ°£Êñá‰ª∂Â§π‰∏≠
        doc_json_folder = os.path.join(self.agent.json_data_path, pdf_file_path)
        output_json_path = os.path.join(doc_json_folder, "data.json")

        logger.info(f"üìÑ [Tool:extract_pdf] ÂÆåÊï¥Ë∑ØÂæÑ:")
        logger.info(f"üìÑ [Tool:extract_pdf]   - PDF: {pdf_path}")
        logger.info(f"üìÑ [Tool:extract_pdf]   - ÂõæÁâáÊñá‰ª∂Â§π: {output_folder_path}")
        logger.info(f"üìÑ [Tool:extract_pdf]   - JSONËæìÂá∫: {output_json_path}")

        # È™åËØÅPDFÊñá‰ª∂Â≠òÂú®
        if not os.path.exists(pdf_path):
            logger.error(f"üìÑ [Tool:extract_pdf] ‚ùå PDFÊñá‰ª∂‰∏çÂ≠òÂú®: {pdf_path}")
            raise FileNotFoundError(f"PDFÊñá‰ª∂‰∏çÂ≠òÂú®: {pdf_path}")

        logger.info(f"üìÑ [Tool:extract_pdf] ‚úÖ PDFÊñá‰ª∂Â≠òÂú®")

        try:
            image_paths = []

            # Ê£ÄÊü•ÊòØÂê¶Â∑≤ÊúâJSONÊï∞ÊçÆ
            if os.path.exists(output_json_path):
                logger.info(f"ÂèëÁé∞Â∑≤Â≠òÂú®ÁöÑJSONÊï∞ÊçÆ: {output_json_path}")
                try:
                    with open(output_json_path, 'r', encoding='utf-8') as f:
                        image_content_list = json.load(f)

                    # Ëé∑ÂèñÂ∑≤Â≠òÂú®ÁöÑÂõæÁâáË∑ØÂæÑ
                    if os.path.exists(output_folder_path):
                        image_paths = read_images_in_directory(output_folder_path)
                        # ÊéíÂ∫èÂõæÁâáË∑ØÂæÑ
                        def safe_page_sort(path):
                            try:
                                match = re.search(r'page_(\d+)\.png', path)
                                return int(match.group(1)) if match else float('inf')
                            except:
                                return float('inf')
                        image_paths = sorted(image_paths, key=safe_page_sort)

                    logger.info(f"‚úÖ [Tool:extract_pdf] ‰ªéÁºìÂ≠òÂä†ËΩΩ: {len(image_content_list)} È°µ")

                    return {
                        "pdf_data_list": image_content_list,
                        "image_paths": image_paths,
                        "json_path": output_json_path,
                        "image_folder": output_folder_path
                    }
                except Exception as e:
                    logger.warning(f"ËØªÂèñÁºìÂ≠òJSONÂ§±Ë¥•ÔºåÂ∞ÜÈáçÊñ∞ÊèêÂèñ: {e}")

            # ËΩ¨Êç¢PDF‰∏∫ÂõæÁâá
            logger.info(f"üìÑ [Tool:extract_pdf] ÂºÄÂßãËΩ¨Êç¢PDF‰∏∫ÂõæÁâá...")
            logger.info(f"üìÑ [Tool:extract_pdf]   - DPI: {self.agent.pdf_dpi}")
            logger.info(f"üìÑ [Tool:extract_pdf]   - Ë¥®ÈáèÈ¢ÑËÆæ: {self.agent.pdf_quality}")
            conversion_stats = pdf_to_images(
                pdf_path, output_folder_path,
                dpi=self.agent.pdf_dpi, quality=self.agent.pdf_quality
            )
            logger.info(f"üìÑ [Tool:extract_pdf] ‚úÖ PDFËΩ¨ÂõæÁâáÂÆåÊàê: ÊàêÂäü {conversion_stats['successful_pages']} È°µ")

            # Ëé∑ÂèñÂõæÁâáË∑ØÂæÑÂπ∂ÊéíÂ∫è
            image_paths = read_images_in_directory(output_folder_path)
            if not image_paths:
                logger.error("Ê≤°ÊúâÊâæÂà∞ÂèØÂ§ÑÁêÜÁöÑÂõæÁâáÊñá‰ª∂")
                return {
                    "pdf_data_list": [],
                    "image_paths": [],
                    "json_path": output_json_path,
                    "image_folder": output_folder_path
                }

            # ÂÆâÂÖ®ÁöÑÈ°µÁ†ÅÊéíÂ∫è
            def safe_page_sort(path):
                try:
                    match = re.search(r'page_(\d+)\.png', path)
                    return int(match.group(1)) if match else float('inf')
                except:
                    return float('inf')

            sorted_image_paths = sorted(image_paths, key=safe_page_sort)
            logger.info(f"ÊâæÂà∞ {len(sorted_image_paths)} ‰∏™ÂõæÁâáÊñá‰ª∂ÂæÖÂ§ÑÁêÜ")

            # ‰ΩøÁî®Âπ∂Ë°åÂ§ÑÁêÜÊèêÂèñÂõæÁâáÂÜÖÂÆπ
            extract_prompt = INDEXING_PROMPTS.get(
                IndexingRole.IMAGE_EXTRACT, "ËØ∑ÊèêÂèñÂõæÁâá‰∏≠ÁöÑÊñáÂ≠óÂÜÖÂÆπ"
            )
            logger.info(f"üìÑ [Tool:extract_pdf] ÂºÄÂßãÂπ∂Ë°åÊèêÂèñÂõæÁâáÂÜÖÂÆπ...")
            logger.info(f"üìÑ [Tool:extract_pdf]   - ÂõæÁâáÊï∞Èáè: {len(sorted_image_paths)}")
            logger.info(f"üìÑ [Tool:extract_pdf]   - ÊúÄÂ§ßÂπ∂Âèë: 5")
            logger.info(f"üìÑ [Tool:extract_pdf]   - ‰ΩøÁî®LLM: {self.agent.llm.provider}")

            # Áõ¥Êé•‰ΩøÁî®ÂºÇÊ≠•ÊñπÊ≥ïÔºàÂõ†‰∏∫ÂΩìÂâçÂ∑≤ÁªèÂú®async‰∏ä‰∏ãÊñá‰∏≠Ôºâ
            from src.core.parallel import PageExtractor
            extractor = PageExtractor(self.agent.llm, extract_prompt, max_concurrent=5)
            image_content_list = await extractor.extract_pages_parallel(sorted_image_paths)

            logger.info(f"üìÑ [Tool:extract_pdf] ‚úÖ ÂõæÁâáÂÜÖÂÆπÊèêÂèñÂÆåÊàê")

            # ‰øùÂ≠òÊèêÂèñÁªìÊûúÂà∞JSONÊñá‰ª∂
            if image_content_list:
                try:
                    # Á°Æ‰øùÁõÆÂΩïÂ≠òÂú®
                    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)

                    with open(output_json_path, 'w', encoding='utf-8') as file:
                        json.dump(image_content_list, file, ensure_ascii=False, indent=2)

                    logger.info(f"Êï∞ÊçÆÂ∑≤‰øùÂ≠òÂà∞: {output_json_path}")
                    logger.info(f"‚úÖ [Tool:extract_pdf] ÊèêÂèñÁªüËÆ°: ÊàêÂäü{len(image_content_list)}È°µ")
                except Exception as e:
                    logger.error(f"‰øùÂ≠òJSONÊñá‰ª∂Â§±Ë¥•: {e}")
                    raise
            else:
                logger.error("Ê≤°ÊúâÊàêÂäüÊèêÂèñ‰ªª‰ΩïÈ°µÈù¢ÂÜÖÂÆπ")

            return {
                "pdf_data_list": image_content_list,
                "image_paths": sorted_image_paths,
                "json_path": output_json_path,
                "image_folder": output_folder_path
            }

        except Exception as e:
            logger.error(f"‚ùå [Tool:extract_pdf] PDFÊï∞ÊçÆÊèêÂèñÂ§±Ë¥•: {e}")
            raise

    def split_pdf_raw_data(self, pdf_raw_data: List[Any]) -> List[List[Any]]:
        """
        Â∞Ü PDF ÂéüÂßãÊï∞ÊçÆÊåâÁÖß chunk_count ËøõË°åÂàáÂàÜ

        Args:
            pdf_raw_data: PDFÂéüÂßãÊï∞ÊçÆÂàóË°®

        Returns:
            ÂàáÂàÜÂêéÁöÑÊï∞ÊçÆÂùóÂàóË°®
        """
        if not isinstance(pdf_raw_data, list):
            logger.error("pdf_raw_data ‰∏çÊòØ listÔºåÊó†Ê≥ïÂàáÂàÜ")
            return []

        chunks = [
            pdf_raw_data[i:i + self.agent.chunk_count]
            for i in range(0, len(pdf_raw_data), self.agent.chunk_count)
        ]
        logger.info(f"Â∑≤Â∞Ü pdf_raw_data ÂàáÂàÜ‰∏∫ {len(chunks)} ‰∏™ÂùóÔºåÊØèÂùóÊúÄÂ§ö {self.agent.chunk_count} Êù°")
        return chunks

    async def extract_toc_from_pages_impl(
        self,
        pdf_data_list: List[Dict[str, str]],
        max_pages: int = 10
    ) -> tuple[Optional[Dict[str, List]], bool]:
        """
        ‰ªéPDFÂâçÂá†È°µÂø´ÈÄüÊèêÂèñÁõÆÂΩïÁªìÊûÑÔºàÂ∑•ÂÖ∑ÊñπÊ≥ïÔºâ

        Args:
            pdf_data_list: PDFÊØèÈ°µÊï∞ÊçÆÂàóË°®
            max_pages: ÊúÄÂ§öÊ£ÄÊü•ÁöÑÈ°µÊï∞

        Returns:
            (agenda_dict, has_toc): ÁõÆÂΩïÂ≠óÂÖ∏ÂíåÊòØÂê¶ÊâæÂà∞ÁõÆÂΩïÁöÑÊ†áÂøó
        """
        from src.utils.helpers import extract_data_from_LLM_res
        from .prompts import IndexingRole

        logger.info(f"üìñ [Tool:extract_toc] Â∞ùËØï‰ªéÂâç {max_pages} È°µÊèêÂèñÁõÆÂΩï")

        try:
            # ÂêàÂπ∂ÂâçÂá†È°µÁöÑÂÜÖÂÆπ
            toc_pages = pdf_data_list[:max_pages]
            combined_content = "\n\n".join([
                f"[Page {item.get('page', i+1)}]\n{item.get('data', '')}"
                for i, item in enumerate(toc_pages)
            ])

            # ÊûÑÂª∫ÊèêÂèñÁõÆÂΩïÁöÑ prompt
            input_prompt = f"ËøôÈáåÊòØÊñáÁ´†ÁöÑÂâç {len(toc_pages)} È°µÂÜÖÂÆπÔºåËØ∑Êü•ÊâæÂπ∂ÊèêÂèñÁõÆÂΩïÁªìÊûÑ: {combined_content}"

            # Ë∞ÉÁî® LLM ÊèêÂèñÁõÆÂΩï
            response = self.agent.llm.call_llm_chain(
                IndexingRole.CHAPTER_EXTRACT,
                input_prompt,
                "toc_extract"
            )

            if not response:
                logger.warning("LLMËøîÂõûÁ©∫ÂìçÂ∫îÔºåÊú™ÊâæÂà∞ÁõÆÂΩï")
                return None, False

            # Ëß£Êûê LLM ËøîÂõûÁöÑÁªìÊûú
            result = extract_data_from_LLM_res(response)

            if not result or not isinstance(result, list) or len(result) == 0:
                logger.info("Êú™Âú®ÂâçÂá†È°µÊ£ÄÊµãÂà∞ÁõÆÂΩïÁªìÊûÑ")
                return None, False

            # ËΩ¨Êç¢‰∏∫ agenda_dict Ê†ºÂºè: {title: [pages]}
            agenda_dict = {}
            for item in result:
                if isinstance(item, dict) and "title" in item and "pages" in item:
                    title = item["title"]
                    pages = item["pages"]
                    if isinstance(pages, list):
                        agenda_dict[title] = pages
                    else:
                        agenda_dict[title] = [pages]

            if agenda_dict:
                logger.info(f"‚úÖ [Tool:extract_toc] ÊàêÂäüÊèêÂèñÁõÆÂΩï: {len(agenda_dict)} ‰∏™Á´†ËäÇ")
                return agenda_dict, True
            else:
                logger.info("Ëß£ÊûêÁªìÊûú‰∏∫Á©∫ÔºåÊú™ÊâæÂà∞ÊúâÊïàÁõÆÂΩï")
                return None, False

        except Exception as e:
            logger.error(f"‚ùå [Tool:extract_toc] ÊèêÂèñÁõÆÂΩïÂ§±Ë¥•: {e}")
            return None, False

    async def analyze_full_structure_impl(
        self,
        pdf_data_list: List[Dict[str, str]]
    ) -> Dict[str, List]:
        """
        ÂàÜÊûêÊï¥‰∏™PDFÊñáÊ°£ÁöÑÁªìÊûÑÔºàÂ∑•ÂÖ∑ÊñπÊ≥ïÔºâ

        ÂΩìPDFÊ≤°ÊúâÊòéÁ°ÆÁõÆÂΩïÊó∂ÔºåÈÅçÂéÜÂÖ®ÊñáÂàÜÊûêÁ´†ËäÇÁªìÊûÑ

        Args:
            pdf_data_list: PDFÊØèÈ°µÊï∞ÊçÆÂàóË°®

        Returns:
            agenda_dict: ÁõÆÂΩïÂ≠óÂÖ∏ {title: [pages]}
        """
        from src.utils.helpers import extract_data_from_LLM_res, group_data_by_sections_with_titles
        from .prompts import IndexingRole

        logger.info(f"üîç [Tool:analyze_structure] ÂºÄÂßãÂàÜÊûêÂÖ®ÊñáÁªìÊûÑ: {len(pdf_data_list)} È°µ")

        try:
            # Â∞ÜPDFÊï∞ÊçÆÂàÜÂùóÂ§ÑÁêÜÔºàÈÅøÂÖçÂçïÊ¨°Â§ÑÁêÜËøáÈïøÔºâ
            chunks = self.split_pdf_raw_data(pdf_data_list)

            all_agenda_list = []

            # Âπ∂Ë°åÂ§ÑÁêÜÊØè‰∏™ÂàÜÂùó
            for i, chunk in enumerate(chunks):
                logger.info(f"Â§ÑÁêÜÂàÜÂùó {i+1}/{len(chunks)}")

                # ÂêàÂπ∂ÂàÜÂùóÂÜÖÂÆπ
                chunk_content = "\n\n".join([
                    f"[Page {item.get('page', idx+1)}]\n{item.get('data', '')}"
                    for idx, item in enumerate(chunk)
                ])

                # ÊûÑÂª∫ prompt
                input_prompt = f"ËøôÈáåÊòØÊñáÁ´†ÁöÑÈÉ®ÂàÜÂÜÖÂÆπ: {chunk_content}"

                # Ë∞ÉÁî® LLM ÊèêÂèñÁ´†ËäÇ
                response = self.agent.llm.call_llm_chain(
                    IndexingRole.CHAPTER_EXTRACT,
                    input_prompt,
                    f"structure_extract_chunk"
                )

                if response:
                    result = extract_data_from_LLM_res(response)
                    if isinstance(result, list):
                        all_agenda_list.extend(result)

            # ËΩ¨Êç¢‰∏∫ agenda_dict
            _, agenda_list = group_data_by_sections_with_titles(all_agenda_list, pdf_data_list)

            # Â∞ÜÂàóË°®Ê†ºÂºèËΩ¨Êç¢‰∏∫Â≠óÂÖ∏Ê†ºÂºè
            agenda_dict = {
                item['title']: item['pages']
                for item in agenda_list
            }

            logger.info(f"‚úÖ [Tool:analyze_structure] ÁªìÊûÑÂàÜÊûêÂÆåÊàê: {len(agenda_dict)} ‰∏™Á´†ËäÇ")

            return agenda_dict

        except Exception as e:
            logger.error(f"‚ùå [Tool:analyze_structure] ÁªìÊûÑÂàÜÊûêÂ§±Ë¥•: {e}")
            # ËøîÂõûÈªòËÆ§ÁªìÊûÑÔºàÊï¥‰∏™ÊñáÊ°£‰Ωú‰∏∫‰∏Ä‰∏™Á´†ËäÇÔºâ
            return {"ÂÖ®Êñá": list(range(1, len(pdf_data_list) + 1))}

    # ==================== ÊâπÈáèÂ§ÑÁêÜÂíåÁÆ°ÁêÜÊñπÊ≥ï ====================

    async def process_documents_batch(
        self,
        doc_list: List[Dict[str, Any]],
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        ÊâπÈáèÂ§ÑÁêÜÊñáÊ°£ÂàóË°®

        Args:
            doc_list: ÊñáÊ°£ÂàóË°®ÔºåÊØè‰∏™ÂÖÉÁ¥†Ê†ºÂºèÔºö
                {
                    "doc_name": str,
                    "doc_path": str,
                    "doc_type": "pdf" | "url"
                }
            max_concurrent: ÊúÄÂ§ßÂπ∂ÂèëÂ§ÑÁêÜÊï∞

        Returns:
            Â§ÑÁêÜÁªìÊûúÂàóË°®
        """
        import asyncio

        logger.info(f"üì¶ ÂºÄÂßãÊâπÈáèÂ§ÑÁêÜÊñáÊ°£: ÂÖ± {len(doc_list)} ‰∏™ÊñáÊ°£")

        results = []

        # ÂàÜÊâπÂ§ÑÁêÜÈÅøÂÖçËøáËΩΩ
        for i in range(0, len(doc_list), max_concurrent):
            batch = doc_list[i:i + max_concurrent]
            logger.info(f"Â§ÑÁêÜÁ¨¨ {i // max_concurrent + 1} Êâπ: {len(batch)} ‰∏™ÊñáÊ°£")

            # Âπ∂ÂèëÂ§ÑÁêÜÂΩìÂâçÊâπÊ¨°
            tasks = []
            for doc_info in batch:
                # ÊûÑÂª∫ÂàùÂßãÁä∂ÊÄÅ
                state = {
                    "doc_name": doc_info["doc_name"],
                    "doc_path": doc_info["doc_path"],
                    "doc_type": doc_info["doc_type"],
                    "status": "pending"
                }
                # ÂàõÂª∫Â§ÑÁêÜ‰ªªÂä°
                task = self.agent.graph.ainvoke(state)
                tasks.append(task)

            # Á≠âÂæÖÂΩìÂâçÊâπÊ¨°ÂÆåÊàê
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # Â§ÑÁêÜÁªìÊûú
            for j, result in enumerate(batch_results):
                doc_name = batch[j]["doc_name"]
                if isinstance(result, Exception):
                    logger.error(f"‚ùå ÊñáÊ°£Â§ÑÁêÜÂ§±Ë¥•: {doc_name}, ÈîôËØØ: {result}")
                    results.append({
                        "doc_name": doc_name,
                        "status": "error",
                        "error": str(result)
                    })
                else:
                    logger.info(f"‚úÖ ÊñáÊ°£Â§ÑÁêÜÂÆåÊàê: {doc_name}, Áä∂ÊÄÅ: {result.get('status')}")
                    results.append(result)

        logger.info(f"‚úÖ ÊâπÈáèÂ§ÑÁêÜÂÆåÊàê: ÊàêÂäü {sum(1 for r in results if r.get('status') == 'completed')} ‰∏™, Â§±Ë¥• {sum(1 for r in results if r.get('status') == 'error')} ‰∏™")

        return results

    async def _prepare_rebuild_state(
        self,
        doc_name: str,
        doc_path: str
    ) -> Dict[str, Any]:
        """
        ÂáÜÂ§áÈáçÂª∫ÁöÑÂàùÂßãÁä∂ÊÄÅ

        1. Âä†ËΩΩ data.json Âíå structure.json
        2. Âà†Èô§ÊóßÁöÑÁîüÊàêÊñá‰ª∂Ôºàchunks, summaries, vector_dbÔºâ
        3. Ê∏ÖÁêÜ DocumentRegistry ‰∏≠ÁöÑÊóßËÆ∞ÂΩï
        4. ÊûÑÂª∫ÂàùÂßãÁä∂ÊÄÅÂ≠óÂÖ∏

        Args:
            doc_name: ÊñáÊ°£ÂêçÁß∞
            doc_path: ÊñáÊ°£Ë∑ØÂæÑ

        Returns:
            ÂàùÂßãÂåñÁöÑ IndexingState
        """
        logger.info(f"üîÑ [Rebuild] ========== ÂáÜÂ§áÈáçÂª∫ÁéØÂ¢É ==========")

        # 1. È™åËØÅÊñá‰ª∂Â≠òÂú®
        doc_json_folder = os.path.join(self.agent.json_data_path, doc_name)
        structure_path = os.path.join(doc_json_folder, "structure.json")
        data_path = os.path.join(doc_json_folder, "data.json")

        if not os.path.exists(structure_path):
            raise FileNotFoundError(f"ÁªìÊûÑÊñá‰ª∂‰∏çÂ≠òÂú®: {structure_path}")

        if not os.path.exists(data_path):
            raise FileNotFoundError(f"Êï∞ÊçÆÊñá‰ª∂‰∏çÂ≠òÂú®: {data_path}")

        # 2. Âä†ËΩΩ structure
        logger.info(f"üì• [Rebuild] Âä†ËΩΩ structure.json...")
        with open(structure_path, 'r', encoding='utf-8') as f:
            structure_data = json.load(f)

        if "agenda_dict" in structure_data:
            agenda_dict = structure_data["agenda_dict"]
            has_toc = structure_data.get("has_toc", False)
        else:
            agenda_dict = structure_data
            has_toc = True

        logger.info(f"   ‚úÖ Âä†ËΩΩÂÆåÊàê: {len(agenda_dict)} ‰∏™Á´†ËäÇ")

        # 3. Âä†ËΩΩ PDF Êï∞ÊçÆ
        logger.info(f"üì• [Rebuild] Âä†ËΩΩ data.json...")
        with open(data_path, 'r', encoding='utf-8') as f:
            pdf_data_list = json.load(f)

        json_data_dict = {
            str(item.get("page", i+1)): item.get("data", "")
            for i, item in enumerate(pdf_data_list)
        }

        raw_data = "\n\n".join([
            f"[Page {item.get('page', i+1)}]\n{item.get('data', '')}"
            for i, item in enumerate(pdf_data_list)
        ])

        logger.info(f"   ‚úÖ Âä†ËΩΩÂÆåÊàê: {len(pdf_data_list)} È°µ")

        # 4. Âà†Èô§ÊóßÁöÑÁîüÊàêÊñá‰ª∂
        logger.info(f"üóëÔ∏è  [Rebuild] Âà†Èô§ÊóßÁöÑÁîüÊàêÊñá‰ª∂...")
        chunks_path = os.path.join(doc_json_folder, "chunks.json")
        if os.path.exists(chunks_path):
            os.remove(chunks_path)
            logger.info(f"   ‚úÖ Âà†Èô§ chunks.json")

        # Âà†Èô§ÊóßÁöÑÊëòË¶ÅÊñá‰ª∂
        from src.config.constants import PathConstants
        output_folder = os.path.join(PathConstants.OUTPUT_DIR, doc_name)
        if os.path.exists(output_folder):
            import shutil
            shutil.rmtree(output_folder)
            logger.info(f"   ‚úÖ Âà†Èô§ËæìÂá∫Êñá‰ª∂Â§π: {output_folder}")

        # Âà†Èô§ÊóßÁöÑÂêëÈáèÊï∞ÊçÆÂ∫ì
        vector_db_folder = os.path.join(PathConstants.VECTOR_DB_DIR, doc_name)
        if os.path.exists(vector_db_folder):
            import shutil
            shutil.rmtree(vector_db_folder)
            logger.info(f"   ‚úÖ Âà†Èô§ÂêëÈáèÊï∞ÊçÆÂ∫ì: {vector_db_folder}")

        # 5. Ê∏ÖÁêÜ DocumentRegistry ‰∏≠ÁöÑÊóßËÆ∞ÂΩï
        logger.info(f"üóëÔ∏è  [Rebuild] Ê∏ÖÁêÜÊñáÊ°£Ê≥®ÂÜå‰ø°ÊÅØ...")
        old_doc = self.agent.doc_registry.get_by_name(doc_name)
        if old_doc:
            # ‰øùÂ≠òÂü∫Êú¨‰ø°ÊÅØÔºå‰ΩÜÊ∏ÖÈô§ÊâÄÊúâÁîüÊàêÊñá‰ª∂ÁöÑË∑ØÂæÑ
            logger.info(f"   ‚ÑπÔ∏è  ÊóßËÆ∞ÂΩï: {old_doc.get('status', 'unknown')} Áä∂ÊÄÅ")
            # ‰∏çÂà†Èô§Êï¥‰∏™ËÆ∞ÂΩïÔºåËÆ© register ËäÇÁÇπÊõ¥Êñ∞
        logger.info(f"   ‚úÖ Registry ÂáÜÂ§áÂ∞±Áª™")

        # 6. ÊûÑÂª∫ÂàùÂßãÁä∂ÊÄÅ
        state = {
            "doc_name": doc_name,
            "doc_path": doc_path,
            "doc_type": "pdf",
            "pdf_data_list": pdf_data_list,
            "json_data_dict": json_data_dict,
            "raw_data": raw_data,
            "agenda_dict": agenda_dict,
            "has_toc": has_toc,
            "status": "loaded",
            "is_complete": False,
            "generated_files": {
                "images": [],  # ‰øùÁïôÂ∑≤ÊúâÁöÑÂõæÁâá
                "json_data": data_path,
                "vector_db": "",
                "summaries": []
            },
            "stage_status": {},  # ‰∏çËÆæÁΩÆË∑≥ËøáÊ†áÂøóÔºåÂº∫Âà∂ÈáçÂª∫ÊâÄÊúâÂÜÖÂÆπ
            "agenda_data_list": []  # ÂàùÂßãÂåñ‰∏∫Á©∫ÔºåÂº∫Âà∂ÈáçÂª∫
        }

        logger.info(f"‚úÖ [Rebuild] ÂàùÂßãÁä∂ÊÄÅÂáÜÂ§áÂÆåÊàê")
        return state

    async def rebuild_from_structure(
        self,
        doc_name: str,
        doc_path: str
    ) -> Dict[str, Any]:
        """
        Âü∫‰∫éÂ∑≤ÊúâÁöÑ structure.json ÈáçÂª∫ÊñáÊ°£Êï∞ÊçÆ

        ‰ΩøÁî®‰∏ìÈó®ÁöÑÈáçÂª∫Â≠êÂõæÊâßË°åÈáçÂª∫ÊµÅÁ®ã

        ‰øùÊåÅ‰∏çÂèòÁöÑÊñá‰ª∂Ôºö
        - structure.json: ÊâãÂä®ÁºñËæëÁöÑÁªìÊûÑ
        - data.json: PDF ÂéüÂßãÊï∞ÊçÆ
        - pdf_image/: PDF ÂõæÁâáÊñá‰ª∂

        ÈáçÊñ∞ÁîüÊàêÁöÑÂÜÖÂÆπÔºö
        - chunks.json: Âü∫‰∫éÊñ∞ÁªìÊûÑÈáçÂª∫Á´†ËäÇÊï∞ÊçÆ
        - Á´†ËäÇÊëòË¶Å: ÈáçÊñ∞ÁîüÊàêÊâÄÊúâÁ´†ËäÇÁöÑÊëòË¶ÅÂíåÈáçÊûÑÂÜÖÂÆπ
        - ÂêëÈáèÊï∞ÊçÆÂ∫ì: ÂÆåÂÖ®ÈáçÂª∫ FAISS Á¥¢Âºï
        - ÁÆÄË¶ÅÊëòË¶Å: ÈáçÊñ∞ÁîüÊàêÊï¥‰ΩìÊñáÊ°£ÊëòË¶Å
        - DocumentRegistry: Êõ¥Êñ∞ÊñáÊ°£Ê≥®ÂÜå‰ø°ÊÅØ

        Args:
            doc_name: ÊñáÊ°£ÂêçÁß∞
            doc_path: ÊñáÊ°£Ë∑ØÂæÑ

        Returns:
            ÈáçÂª∫ÁªìÊûúÂ≠óÂÖ∏
        """
        logger.info(f"üîÑ [Rebuild] ========== ÂºÄÂßã‰ªé structure ÂÖ®Èù¢ÈáçÂª∫ ==========")
        logger.info(f"üîÑ [Rebuild] ÊñáÊ°£: {doc_name}")
        logger.info(f"üîÑ [Rebuild] ‰ΩøÁî®ÈáçÂª∫Â≠êÂõæÊâßË°åÊµÅÁ®ã")

        try:
            # 1. ÂáÜÂ§áÂàùÂßãÁä∂ÊÄÅÔºàÂä†ËΩΩÊñá‰ª∂„ÄÅÂà†Èô§ÊóßÊï∞ÊçÆ„ÄÅÊ∏ÖÁêÜregistryÔºâ
            state = await self._prepare_rebuild_state(doc_name, doc_path)

            # 2. ‰ΩøÁî®ÈáçÂª∫Â≠êÂõæÊâßË°å
            logger.info(f"üîÑ [Rebuild] ÂºÄÂßãÊâßË°åÈáçÂª∫Â≠êÂõæ...")
            result_state = await self.agent.rebuild_graph.ainvoke(state)

            # 3. È™åËØÅÈáçÂª∫ÁªìÊûú
            doc_json_folder = os.path.join(self.agent.json_data_path, doc_name)
            chunks_path = os.path.join(doc_json_folder, "chunks.json")

            logger.info(f"‚úÖ [Rebuild] ÈáçÂª∫ÂÆåÊàêÔºÅ")
            logger.info(f"   üìä Á´†ËäÇÊï∞: {len(result_state.get('agenda_data_list', []))}")
            logger.info(f"   üìÅ ÁîüÊàêÊñá‰ª∂: {len(result_state.get('generated_files', {}).get('summaries', []))} ‰∏™ÊëòË¶Å")
            logger.info(f"   üîç ÂêëÈáèÂ∫ì: {result_state.get('generated_files', {}).get('vector_db', 'N/A')}")

            return {
                "success": True,
                "doc_name": doc_name,
                "total_chapters": len(result_state.get("agenda_data_list", [])),
                "status": "completed",
                "generated_files": result_state.get("generated_files", {}),
                "rebuilt": {
                    "chunks": os.path.exists(chunks_path),
                    "summaries": len(result_state.get("generated_files", {}).get("summaries", [])) > 0,
                    "vector_db": bool(result_state.get("generated_files", {}).get("vector_db")),
                    "brief_summary": result_state.get("brief_summary") is not None,
                    "registry": self.agent.doc_registry.get_by_name(doc_name) is not None
                }
            }

        except Exception as e:
            logger.error(f"‚ùå [Rebuild] ÈáçÂª∫Â§±Ë¥•: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return {
                "success": False,
                "error": str(e)
            }
