"""
Indexing Agent - æ–‡æ¡£ç´¢å¼•æ„å»ºAgent

è´Ÿè´£æ–‡æ¡£çš„è§£æã€æ‘˜è¦ç”Ÿæˆã€æ ‡ç­¾åˆ†ç±»ã€å‘é‡ç´¢å¼•æ„å»ºå’Œæ–‡æ¡£æ³¨å†Œ
"""

from langgraph.graph import StateGraph, END
from typing import Dict, List
import logging
import json
import re

from ..base import AgentBase
from .state import IndexingState
from .doc_registry import DocumentRegistry

logger = logging.getLogger(__name__)


class IndexingAgent(AgentBase):
    """
    ç´¢å¼•æ„å»ºAgent

    å·¥ä½œæµç¨‹ï¼š
    1. parse - è§£ææ–‡æ¡£å†…å®¹
    2. chunk - æ–‡æœ¬åˆ†å—
    3. summarize - ç”Ÿæˆæ‘˜è¦
    4. tag - è‡ªåŠ¨æ ‡ç­¾åˆ†ç±»
    5. build_index - æ„å»ºå‘é‡ç´¢å¼•
    6. register - æ³¨å†Œåˆ°æ–‡æ¡£åº“

    å·¥å…·æ–¹æ³•ï¼ˆç›´æ¥åœ¨ç±»ä¸­å®ç°ï¼‰ï¼š
    - extract_basic_info_impl - æå–åŸºæœ¬ä¿¡æ¯
    - generate_summary_impl - ç”Ÿæˆæ‘˜è¦
    - auto_tag_impl - è‡ªåŠ¨æ ‡ç­¾
    - build_vector_index_impl - æ„å»ºå‘é‡ç´¢å¼•
    """

    def __init__(self):
        super().__init__(name="IndexingAgent")

        self.doc_registry = DocumentRegistry()
        self.graph = self.build_graph()

    def build_graph(self) -> StateGraph:
        """æ„å»ºLangGraph workflow"""
        workflow = StateGraph(IndexingState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("parse", self.parse_document)
        workflow.add_node("chunk", self.chunk_text)
        workflow.add_node("summarize", self.generate_summary)
        workflow.add_node("tag", self.auto_tag)
        workflow.add_node("build_index", self.build_index)
        workflow.add_node("register", self.register_document)

        # æ·»åŠ è¾¹
        workflow.add_edge("parse", "chunk")
        workflow.add_edge("chunk", "summarize")
        workflow.add_edge("summarize", "tag")
        workflow.add_edge("tag", "build_index")
        workflow.add_edge("build_index", "register")
        workflow.add_edge("register", END)

        # è®¾ç½®å…¥å£å’Œå‡ºå£
        workflow.set_entry_point("parse")

        return workflow.compile()

    # ==================== å·¥å…·æ–¹æ³•å®ç° ====================

    async def generate_summary_impl(self, content: str, doc_name: str) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£ç®€è¦æ‘˜è¦ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            content: æ–‡æ¡£å†…å®¹
            doc_name: æ–‡æ¡£åç§°

        Returns:
            ç®€è¦æ‘˜è¦æ–‡æœ¬
        """
        logger.info(f"ğŸ“ [Tool:generate_summary] ç”Ÿæˆæ‘˜è¦: {doc_name}")

        try:
            from src.config.prompts.reader_prompts import ReaderRole

            query = (
                "è¯·æŒ‰ç…§æ–‡ç« æœ¬èº«çš„ç« èŠ‚ä¿¡æ¯å’Œå™äº‹ç»“æ„ï¼Œæ•´ç†è¿™ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹ï¼Œ"
                "æ¯ä¸ªç« èŠ‚éƒ½éœ€è¦æœ‰ä¸€å®šçš„ç®€å•ä»‹ç»ã€‚å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸­æœ‰ä¸€äº›æ–‡ç« çš„åŸºæœ¬ä¿¡æ¯ä¹Ÿéœ€è¦ä¸€å¹¶æ€»ç»“ã€‚"
                "ä»…éœ€è¦è¿”å›ç›¸å…³å†…å®¹ï¼Œå¤šä½™çš„è¯æ— éœ€è¿”å›ã€‚è¿”å›ä¸­æ–‡ã€‚"
            )

            context = {"å…¨æ–‡å†…å®¹": content}

            # ä½¿ç”¨Agentçš„LLMå®ä¾‹
            answer = self.llm.get_answer(
                retrieval_data_dict=context,
                query=query,
                answer_role=ReaderRole.CONTEXT_QA
            )

            if not answer or not answer.strip():
                logger.error("ç”Ÿæˆçš„ç®€è¦æ‘˜è¦ä¸ºç©º")
                return f"æ–‡æ¡£ {doc_name} çš„ç®€è¦æ‘˜è¦ï¼ˆç”Ÿæˆå¤±è´¥ï¼‰"

            logger.info(f"âœ… [Tool:generate_summary] æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
            return answer

        except Exception as e:
            logger.error(f"âŒ [Tool:generate_summary] ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return f"æ–‡æ¡£ {doc_name} çš„ç®€è¦æ‘˜è¦ï¼ˆç”Ÿæˆé”™è¯¯: {str(e)}ï¼‰"

    async def auto_tag_impl(self, doc_name: str, brief_summary: str, max_tags: int = 5) -> List[str]:
        """
        è‡ªåŠ¨ä¸ºæ–‡æ¡£ç”Ÿæˆåˆ†ç±»æ ‡ç­¾ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            doc_name: æ–‡æ¡£åç§°
            brief_summary: æ–‡æ¡£ç®€è¦æ‘˜è¦
            max_tags: æœ€å¤§æ ‡ç­¾æ•°é‡

        Returns:
            æ ‡ç­¾åˆ—è¡¨
        """
        logger.info(f"ğŸ·ï¸ [Tool:auto_tag] è‡ªåŠ¨ç”Ÿæˆæ ‡ç­¾: {doc_name}")

        if not brief_summary:
            logger.warning("æ‘˜è¦ä¸ºç©ºï¼Œè¿”å›é»˜è®¤æ ‡ç­¾")
            return ["æœªåˆ†ç±»"]

        try:
            prompt = f"""
è¯·ä¸ºä»¥ä¸‹æ–‡æ¡£ç”Ÿæˆ3-{max_tags}ä¸ªåˆ†ç±»æ ‡ç­¾ã€‚

æ–‡æ¡£åç§°ï¼š{doc_name}
æ–‡æ¡£æ‘˜è¦ï¼š{brief_summary}

è¦æ±‚ï¼š
1. æ ‡ç­¾åº”è¯¥åæ˜ æ–‡æ¡£çš„ä¸»é¢˜ã€é¢†åŸŸã€ç±»å‹
2. ä½¿ç”¨ç®€çŸ­çš„è¯æˆ–çŸ­è¯­ï¼ˆ2-5ä¸ªå­—ï¼‰
3. è¿”å›JSONæ ¼å¼ï¼š{{"tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", ...]}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

            # ä½¿ç”¨Agentçš„LLMå®ä¾‹
            response = await self.llm.async_get_response(prompt)

            # è§£æLLMè¿”å›çš„JSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    result = json.loads(json_match.group())
                    tags = result.get("tags", [])

                    if len(tags) > max_tags:
                        tags = tags[:max_tags]

                    logger.info(f"âœ… [Tool:auto_tag] ç”Ÿæˆæ ‡ç­¾: {tags}")
                    return tags

                except json.JSONDecodeError as e:
                    logger.warning(f"âš ï¸ [Tool:auto_tag] JSONè§£æå¤±è´¥: {e}")

            # å¤±è´¥æ—¶å°è¯•ä»æ–‡æ¡£åæå–æ ‡ç­¾
            logger.warning("LLMè¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼Œå°è¯•ä»æ–‡æ¡£åæå–æ ‡ç­¾")
            fallback_tags = self._extract_tags_from_filename(doc_name)
            return fallback_tags if fallback_tags else ["æœªåˆ†ç±»"]

        except Exception as e:
            logger.error(f"âŒ [Tool:auto_tag] ç”Ÿæˆæ ‡ç­¾å¤±è´¥: {e}")
            return ["æœªåˆ†ç±»"]

    async def build_vector_index_impl(
        self,
        doc_name: str,
        chunks: List[str],
        metadata: Dict = None
    ) -> str:
        """
        æ„å»ºå‘é‡ç´¢å¼•ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            doc_name: æ–‡æ¡£åç§°
            chunks: æ–‡æœ¬åˆ†å—åˆ—è¡¨
            metadata: å…ƒæ•°æ®

        Returns:
            ç´¢å¼•è·¯å¾„
        """
        logger.info(f"ğŸ”¨ [Tool:build_index] æ„å»ºå‘é‡ç´¢å¼•: {doc_name}, åˆ†å—æ•°: {len(chunks)}")

        try:
            from pathlib import Path
            from src.config.settings import DATA_ROOT

            # ä½¿ç”¨Agentçš„embeddingæ¨¡å‹
            embedding_model = self.embedding_model

            # æ„å»ºç´¢å¼•è·¯å¾„
            index_dir = Path(DATA_ROOT) / "vector_db" / doc_name
            index_dir.mkdir(parents=True, exist_ok=True)

            # TODO: å®é™…çš„å‘é‡ç´¢å¼•æ„å»ºé€»è¾‘
            # ä½¿ç”¨ VectorDBClient æ„å»ºç´¢å¼•

            index_path = str(index_dir)

            logger.info(f"âœ… [Tool:build_index] ç´¢å¼•æ„å»ºå®Œæˆ: {index_path}")
            return index_path

        except Exception as e:
            logger.error(f"âŒ [Tool:build_index] ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise

    def _extract_tags_from_filename(self, filename: str) -> List[str]:
        """
        ä»æ–‡ä»¶åä¸­æå–å¯èƒ½çš„æ ‡ç­¾

        Args:
            filename: æ–‡ä»¶å

        Returns:
            æ ‡ç­¾åˆ—è¡¨
        """
        name_without_ext = filename.rsplit('.', 1)[0]

        keywords_map = {
            'ml': 'æœºå™¨å­¦ä¹ ',
            'ai': 'äººå·¥æ™ºèƒ½',
            'deep': 'æ·±åº¦å­¦ä¹ ',
            'paper': 'è®ºæ–‡',
            'report': 'æŠ¥å‘Š',
            'tutorial': 'æ•™ç¨‹',
            'guide': 'æŒ‡å—',
            'doc': 'æ–‡æ¡£',
            'manual': 'æ‰‹å†Œ',
        }

        tags = []
        name_lower = name_without_ext.lower()

        for keyword, tag in keywords_map.items():
            if keyword in name_lower:
                tags.append(tag)

        return tags

    # ==================== WorkflowèŠ‚ç‚¹æ–¹æ³• ====================

    async def parse_document(self, state: IndexingState) -> Dict:
        """
        æ­¥éª¤1ï¼šè§£ææ–‡æ¡£å†…å®¹

        æ ¹æ®doc_typeé€‰æ‹©åˆé€‚çš„Parser
        """
        logger.info(f"ğŸ“„ [Parse] è§£ææ–‡æ¡£: {state['doc_name']}")

        try:
            doc_type = state["doc_type"]
            doc_path = state["doc_path"]

            if doc_type == "pdf":
                # TODO: ä½¿ç”¨PDFReaderæå–å†…å®¹
                # ä¸´æ—¶å®ç°ï¼šè¯»å–æ–‡ä»¶
                from pathlib import Path
                if Path(doc_path).exists():
                    raw_data = f"PDF content from {doc_path}"
                else:
                    raw_data = "Sample PDF content for testing"

            elif doc_type == "url":
                # TODO: ä½¿ç”¨WebReaderæå–å†…å®¹
                raw_data = f"Web content from {doc_path}"

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£ç±»å‹: {doc_type}")

            logger.info(f"âœ… [Parse] è§£æå®Œæˆï¼Œå†…å®¹é•¿åº¦: {len(raw_data)}")

            return {
                "raw_data": raw_data,
                "status": "parsed"
            }

        except Exception as e:
            logger.error(f"âŒ [Parse] è§£æå¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }

    async def chunk_text(self, state: IndexingState) -> Dict:
        """
        æ­¥éª¤2ï¼šæ–‡æœ¬åˆ†å—
        """
        logger.info(f"âœ‚ï¸ [Chunk] æ–‡æœ¬åˆ†å—: {state['doc_name']}")

        try:
            from src.processing.text.splitter import StrictOverlapSplitter

            # åˆ›å»ºåˆ†å—å™¨
            splitter = StrictOverlapSplitter(
                token_threshold=1000,
                overlap=1
            )

            # æ‰§è¡Œåˆ†å—
            raw_data = state.get("raw_data", "")
            chunks = splitter.split_text(raw_data)

            logger.info(f"âœ… [Chunk] åˆ†å—å®Œæˆï¼Œå…± {len(chunks)} ä¸ªåˆ†å—")

            return {
                "chunks": chunks,
                "status": "chunked"
            }

        except Exception as e:
            logger.error(f"âŒ [Chunk] åˆ†å—å¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }

    async def generate_summary(self, state: IndexingState) -> Dict:
        """
        æ­¥éª¤3ï¼šç”Ÿæˆæ‘˜è¦
        """
        logger.info(f"ğŸ“ [Summarize] ç”Ÿæˆæ‘˜è¦: {state['doc_name']}")

        try:
            raw_data = state.get("raw_data", "")
            doc_name = state["doc_name"]

            # è°ƒç”¨å·¥å…·æ–¹æ³•ï¼ˆç›´æ¥è°ƒç”¨ï¼Œä¸é€šè¿‡execute_toolï¼‰
            brief_summary = await self.generate_summary_impl(raw_data, doc_name)

            logger.info(f"âœ… [Summarize] æ‘˜è¦ç”Ÿæˆå®Œæˆ")

            return {
                "brief_summary": brief_summary,
                "status": "summarized"
            }

        except Exception as e:
            logger.error(f"âŒ [Summarize] æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }

    async def auto_tag(self, state: IndexingState) -> Dict:
        """
        æ­¥éª¤4ï¼šè‡ªåŠ¨æ ‡ç­¾åˆ†ç±»
        """
        logger.info(f"ğŸ·ï¸ [Tag] è‡ªåŠ¨æ ‡ç­¾: {state['doc_name']}")

        try:
            doc_name = state["doc_name"]
            brief_summary = state.get("brief_summary", "")

            # è°ƒç”¨å·¥å…·æ–¹æ³•ï¼ˆç›´æ¥è°ƒç”¨ï¼‰
            auto_tags = await self.auto_tag_impl(doc_name, brief_summary)

            # åˆå¹¶æ‰‹åŠ¨æ ‡ç­¾
            manual_tags = state.get("manual_tags", []) or []
            final_tags = list(set(auto_tags + manual_tags))

            logger.info(f"âœ… [Tag] æ ‡ç­¾ç”Ÿæˆå®Œæˆ: {final_tags}")

            return {
                "auto_tags": auto_tags,
                "tags": final_tags,
                "status": "tagged"
            }

        except Exception as e:
            logger.error(f"âŒ [Tag] æ ‡ç­¾ç”Ÿæˆå¤±è´¥: {e}")
            # å¤±è´¥æ—¶ä½¿ç”¨manual_tags
            return {
                "auto_tags": [],
                "tags": state.get("manual_tags", []) or [],
                "status": "tagged"
            }

    async def build_index(self, state: IndexingState) -> Dict:
        """
        æ­¥éª¤5ï¼šæ„å»ºå‘é‡ç´¢å¼•
        """
        logger.info(f"ğŸ”¨ [BuildIndex] æ„å»ºç´¢å¼•: {state['doc_name']}")

        try:
            doc_name = state["doc_name"]
            chunks = state.get("chunks", [])
            tags = state.get("tags", [])
            brief_summary = state.get("brief_summary", "")

            # è°ƒç”¨å·¥å…·æ–¹æ³•ï¼ˆç›´æ¥è°ƒç”¨ï¼‰
            index_path = await self.build_vector_index_impl(
                doc_name,
                chunks,
                metadata={
                    "tags": tags,
                    "summary": brief_summary
                }
            )

            logger.info(f"âœ… [BuildIndex] ç´¢å¼•æ„å»ºå®Œæˆ: {index_path}")

            return {
                "index_path": index_path,
                "status": "indexed"
            }

        except Exception as e:
            logger.error(f"âŒ [BuildIndex] ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }

    async def register_document(self, state: IndexingState) -> Dict:
        """
        æ­¥éª¤6ï¼šæ³¨å†Œåˆ°æ–‡æ¡£åº“
        """
        logger.info(f"ğŸ“‹ [Register] æ³¨å†Œæ–‡æ¡£: {state['doc_name']}")

        try:
            # æ³¨å†Œæ–‡æ¡£
            doc_id = self.doc_registry.register(
                doc_name=state["doc_name"],
                doc_path=state["doc_path"],
                doc_type=state["doc_type"],
                index_path=state.get("index_path", ""),
                tags=state.get("tags", []),
                brief_summary=state.get("brief_summary", ""),
                metadata={
                    "auto_tags": state.get("auto_tags", []),
                    "manual_tags": state.get("manual_tags", [])
                }
            )

            logger.info(f"âœ… [Register] æ–‡æ¡£æ³¨å†Œå®Œæˆ: {doc_id}")

            return {
                "doc_id": doc_id,
                "status": "completed"
            }

        except Exception as e:
            logger.error(f"âŒ [Register] æ–‡æ¡£æ³¨å†Œå¤±è´¥: {e}")
            return {
                "status": "error",
                "error": str(e)
            }
