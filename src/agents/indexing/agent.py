"""
Indexing Agent - æ–‡æ¡£ç´¢å¼•æ„å»ºAgent

è´Ÿè´£æ–‡æ¡£çš„è§£æã€æ‘˜è¦ç”Ÿæˆã€æ ‡ç­¾åˆ†ç±»ã€å‘é‡ç´¢å¼•æ„å»ºå’Œæ–‡æ¡£æ³¨å†Œ
"""

from langgraph.graph import StateGraph, END
from typing import Dict, List, Any, Optional
import logging
import json
import re
import os
from pathlib import Path

from ..base import AgentBase
from .state import IndexingState
from .doc_registry import DocumentRegistry
from src.config.settings import (
    PDF_IMAGE_PATH,
    PDF_PATH,
    JSON_DATA_PATH,
    PDF_IMAGE_CONFIG,
)
from src.config.prompts.reader_prompts import ReaderRole, READER_PROMPTS
from src.config.constants import ReaderConstants
from src.utils.helpers import (
    pdf_to_images,
    read_images_in_directory,
    makedir,
    get_pdf_name,
)

logger = logging.getLogger(__name__)


class IndexingAgent(AgentBase):
    """
    ç´¢å¼•æ„å»ºAgent

    å·¥ä½œæµç¨‹ï¼š
    1. parse - è§£ææ–‡æ¡£å†…å®¹
    2. chunk - æ–‡æœ¬åˆ†å—
    3. summarize - ç”Ÿæˆæ‘˜è¦
    4. build_index - æ„å»ºå‘é‡ç´¢å¼•
    5. register - æ³¨å†Œåˆ°æ–‡æ¡£åº“

    - extract_basic_info_impl - æå–åŸºæœ¬ä¿¡æ¯
    - generate_summary_impl - ç”Ÿæˆæ‘˜è¦
    - build_vector_index_impl - æ„å»ºå‘é‡ç´¢å¼•
    """

    def __init__(self, provider: str = "openai", pdf_preset: str = "high"):
        """
        åˆå§‹åŒ– IndexingAgent

        Args:
            provider: LLMæä¾›å•† ('azure', 'openai', 'ollama')
            pdf_preset: PDFè½¬å›¾ç‰‡è´¨é‡é¢„è®¾ ('fast', 'balanced', 'high', 'ultra')
        """
        # åˆå§‹åŒ–åŸºç±»ï¼ˆä¼šåˆå§‹åŒ– self.llm å’Œ self.embedding_modelï¼‰
        super().__init__(name="IndexingAgent", provider=provider)

        self.doc_registry = DocumentRegistry()

        # PDF å¤„ç†ç›¸å…³é…ç½®
        self.pdf_image_path = PDF_IMAGE_PATH
        self.pdf_path = PDF_PATH
        self.json_data_path = JSON_DATA_PATH
        self.chunk_count = ReaderConstants.DEFAULT_CHUNK_COUNT

        # é…ç½® PDF è½¬å›¾ç‰‡å‚æ•°
        try:
            if pdf_preset in PDF_IMAGE_CONFIG.get("presets", {}):
                preset_config = PDF_IMAGE_CONFIG["presets"][pdf_preset]
                self.pdf_dpi = preset_config.get("dpi", PDF_IMAGE_CONFIG.get("dpi", 300))
                self.pdf_quality = pdf_preset
                logger.info(f"ä½¿ç”¨PDFè½¬å›¾ç‰‡é¢„è®¾'{pdf_preset}': DPI={self.pdf_dpi}, è´¨é‡çº§åˆ«={self.pdf_quality}")
            else:
                self.pdf_dpi = PDF_IMAGE_CONFIG.get("dpi", 300)
                self.pdf_quality = PDF_IMAGE_CONFIG.get("quality", "high")
                logger.info(f"ä½¿ç”¨é»˜è®¤PDFè½¬å›¾ç‰‡é…ç½®: DPI={self.pdf_dpi}, è´¨é‡={self.pdf_quality}")
        except Exception as e:
            logger.warning(f"PDFå›¾ç‰‡é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            self.pdf_dpi = 300
            self.pdf_quality = "high"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for path in [self.pdf_image_path, self.pdf_path, self.json_data_path]:
            makedir(path)

        self.graph = self.build_graph()

    # ==================== æ£€æŸ¥ç‚¹è¾…åŠ©æ–¹æ³• ====================

    def _check_stage_files_exist(self, stage_name: str, output_files: List[str]) -> bool:
        """
        æ£€æŸ¥é˜¶æ®µè¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨

        Args:
            stage_name: é˜¶æ®µåç§°
            output_files: è¾“å‡ºæ–‡ä»¶è·¯å¾„åˆ—è¡¨

        Returns:
            æ‰€æœ‰æ–‡ä»¶éƒ½å­˜åœ¨è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if not output_files:
            return False

        for file_path in output_files:
            path = Path(file_path)
            # æ£€æŸ¥æ–‡ä»¶æˆ–ç›®å½•æ˜¯å¦å­˜åœ¨
            if not path.exists():
                logger.info(f"â­ï¸  [{stage_name}] æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ: {file_path}")
                return False

            # å¦‚æœæ˜¯ç›®å½•ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç©º
            if path.is_dir():
                if not any(path.iterdir()):
                    logger.info(f"â­ï¸  [{stage_name}] ç›®å½•ä¸ºç©ºï¼Œéœ€è¦æ‰§è¡Œ: {file_path}")
                    return False

        logger.info(f"âœ… [{stage_name}] æ‰€æœ‰è¾“å‡ºæ–‡ä»¶å·²å­˜åœ¨")
        return True

    def _should_skip_stage(self, doc_name: str, stage_name: str) -> tuple[bool, Optional[List[str]]]:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æŸä¸ªé˜¶æ®µ

        Args:
            doc_name: æ–‡æ¡£åç§°
            stage_name: é˜¶æ®µåç§°

        Returns:
            (should_skip, output_files): æ˜¯å¦è·³è¿‡ å’Œ è¾“å‡ºæ–‡ä»¶åˆ—è¡¨
        """
        # æ£€æŸ¥æ³¨å†Œè¡¨ä¸­çš„é˜¶æ®µçŠ¶æ€
        stage_info = self.doc_registry.get_stage_status(doc_name, stage_name)

        if not stage_info or stage_info.get("status") != "completed":
            logger.info(f"ğŸ”„ [{stage_name}] é˜¶æ®µæœªå®Œæˆï¼Œéœ€è¦æ‰§è¡Œ")
            return False, None

        # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        output_files = stage_info.get("output_files", [])
        if self._check_stage_files_exist(stage_name, output_files):
            logger.info(f"â­ï¸  [{stage_name}] é˜¶æ®µå·²å®Œæˆä¸”æ–‡ä»¶å­˜åœ¨ï¼Œè·³è¿‡æ‰§è¡Œ")
            return True, output_files
        else:
            logger.info(f"ğŸ”„ [{stage_name}] é˜¶æ®µçŠ¶æ€ä¸ºå®Œæˆä½†æ–‡ä»¶ä¸å­˜åœ¨ï¼Œé‡æ–°æ‰§è¡Œ")
            return False, None

    # ==================== Graphæ„å»º ====================

    def build_graph(self) -> StateGraph:
        """
        æ„å»ºLangGraph workflow

        å·¥ä½œæµç¨‹ï¼š
        1. check_cache - æ£€æŸ¥æ‰€æœ‰é˜¶æ®µçš„æ–‡ä»¶ï¼Œè®¾ç½®æ¯ä¸ªé˜¶æ®µçš„è·³è¿‡æ ‡å¿—
        2. parse - è§£ææ–‡æ¡£ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        3. extract_structure - æå–ç›®å½•ç»“æ„ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        4. chunk - æ„å»ºç« èŠ‚æ•°æ®åˆ—è¡¨ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        5. process_chapters - å¹¶è¡Œå¤„ç†ç« èŠ‚ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        6. build_index - æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        7. generate_brief_summary - ç”Ÿæˆç®€è¦æ‘˜è¦ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        8. register - æ³¨å†Œæ–‡æ¡£
        """
        workflow = StateGraph(IndexingState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("check_cache", self.check_cache)  # åˆå§‹æ£€æŸ¥èŠ‚ç‚¹
        workflow.add_node("parse", self.parse_document)
        workflow.add_node("extract_structure", self.extract_structure)
        workflow.add_node("chunk", self.chunk_text)
        workflow.add_node("process_chapters", self.process_chapters)
        workflow.add_node("build_index", self.build_index)
        workflow.add_node("generate_brief_summary", self.generate_brief_summary)
        workflow.add_node("register", self.register_document)

        # æ·»åŠ è¾¹ - çº¿æ€§æµç¨‹ï¼Œæ¯ä¸ªèŠ‚ç‚¹å†…éƒ¨æ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡
        workflow.add_edge("check_cache", "parse")
        workflow.add_edge("parse", "extract_structure")
        workflow.add_edge("extract_structure", "chunk")
        workflow.add_edge("chunk", "process_chapters")
        workflow.add_edge("process_chapters", "build_index")
        workflow.add_edge("build_index", "generate_brief_summary")
        workflow.add_edge("generate_brief_summary", "register")
        workflow.add_edge("register", END)

        # è®¾ç½®å…¥å£
        workflow.set_entry_point("check_cache")

        return workflow.compile()

    async def generate_summary_impl(
        self,
        context_data: Dict[str, Any],
        doc_name: str,
        session_id: str = "summary_generation"
    ) -> str:
        """
        ç”Ÿæˆæ–‡æ¡£ç®€è¦æ‘˜è¦ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            context_data: ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå¯ä»¥æ˜¯å…¨æ–‡å†…å®¹æˆ–ç« èŠ‚æ‘˜è¦å­—å…¸ï¼‰
            doc_name: æ–‡æ¡£åç§°
            session_id: ä¼šè¯IDï¼Œç”¨äºåŒºåˆ†ä¸åŒè°ƒç”¨åœºæ™¯

        Returns:
            ç®€è¦æ‘˜è¦æ–‡æœ¬
        """
        logger.info(f"ğŸ“ [Tool:generate_summary] ç”Ÿæˆæ‘˜è¦: {doc_name}")

        try:
            from src.config.prompts.reader_prompts import ReaderRole

            query = (
                "è¯·æŒ‰ç…§æ–‡ç« æœ¬èº«çš„ç« èŠ‚ä¿¡æ¯å’Œå™äº‹ç»“æ„ï¼Œæ•´ç†è¿™ç¯‡æ–‡ç« çš„ä¸»è¦å†…å®¹ï¼Œ"
                "æ¯ä¸ªç« èŠ‚éƒ½éœ€è¦æœ‰ä¸€å®šçš„ç®€å•ä»‹ç»ã€‚å¦‚æœèƒŒæ™¯çŸ¥è¯†ä¸­æœ‰ä¸€äº›æ–‡ç« çš„åŸºæœ¬ä¿¡æ¯ä¹Ÿéœ€è¦ä¸€å¹¶æ€»ç»“ã€‚"
                "ä»…éœ€è¦è¿”å›ç›¸å…³å†…å®¹ï¼Œå¤šä½™çš„è¯æ— éœ€è¿”å›ã€‚è¿”å›ä¸­æ–‡ã€‚"
            )

            # æ„å»ºè¾“å…¥ promptï¼ˆä¸ ReaderBase.get_answer ç›¸åŒçš„æ ¼å¼ï¼‰
            input_prompt = (
                f"è¯·ç»“åˆæ£€ç´¢å›æ¥çš„ä¸Šä¸‹æ–‡ä¿¡æ¯(Context data)å›ç­”å®¢æˆ·é—®é¢˜\n\n"
                f"===== \n\nQuestion: {query}\n\n"
                f"===== \n\nContext data: {context_data}"
            )

            # ä½¿ç”¨å¼‚æ­¥è°ƒç”¨ï¼ˆç¦ç”¨å†å²æ€»ç»“ï¼Œæ‘˜è¦ç”Ÿæˆä¸éœ€è¦ä¿ç•™ä¸Šä¸‹æ–‡ï¼‰
            answer = await self.llm.async_call_llm_chain(
                ReaderRole.CONTEXT_QA,
                input_prompt,
                session_id,
                enable_llm_summary=False
            )

            if not answer or not answer.strip():
                logger.warning("ç”Ÿæˆçš„ç®€è¦æ‘˜è¦ä¸ºç©º")
                return f"æ–‡æ¡£ {doc_name} çš„ç®€è¦æ‘˜è¦ï¼ˆç”Ÿæˆå¤±è´¥ï¼‰"

            logger.info(f"âœ… [Tool:generate_summary] æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")
            return answer

        except Exception as e:
            logger.error(f"âŒ [Tool:generate_summary] ç”Ÿæˆæ‘˜è¦å¤±è´¥: {e}")
            return f"æ–‡æ¡£ {doc_name} çš„ç®€è¦æ‘˜è¦ï¼ˆç”Ÿæˆé”™è¯¯: {str(e)}ï¼‰"


    async def build_vector_index_impl(
        self,
        doc_name: str,
        chunks: List[Dict[str, str]],
        metadata: Optional[Dict] = None
    ) -> str:
        """
        æ„å»ºå‘é‡ç´¢å¼•ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            doc_name: æ–‡æ¡£åç§°
            chunks: æ–‡æœ¬åˆ†å—åˆ—è¡¨ï¼Œæ ¼å¼ï¼š[{"data": str, "page": str}, ...]
            metadata: å…ƒæ•°æ®ï¼ˆåŒ…å«tags, summaryç­‰ï¼‰

        Returns:
            ç´¢å¼•è·¯å¾„
        """
        logger.info(f"ğŸ”¨ [Tool:build_index] æ„å»ºå‘é‡ç´¢å¼•: {doc_name}, åˆ†å—æ•°: {len(chunks)}")

        try:
            from pathlib import Path
            from src.config.settings import DATA_ROOT
            from src.core.vector_db.vector_db_client import VectorDBClient
            from langchain.docstore.document import Document

            # æ„å»ºç´¢å¼•è·¯å¾„
            index_dir = Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index"
            index_dir.mkdir(parents=True, exist_ok=True)
            index_path = str(index_dir)

            # åˆ›å»º VectorDBClientï¼Œç›´æ¥ä½¿ç”¨ self.embedding_model
            # self.embedding_model æ¥è‡ª AgentBaseï¼Œå®ƒä» self.llm.embedding_model è·å–
            vector_db_client = VectorDBClient(index_path, embedding_model=self.embedding_model)

            # å‡†å¤‡æ–‡æ¡£åˆ—è¡¨
            vector_db_docs = []

            # æå–å…ƒæ•°æ®
            summary = metadata.get("summary", "") if metadata else ""

            # ä¸ºæ¯ä¸ªåˆ†å—åˆ›å»ºDocumentå¯¹è±¡
            for i, chunk_item in enumerate(chunks):
                chunk_data = chunk_item.get("data", "")
                chunk_page = chunk_item.get("page", f"chunk_{i+1}")

                if not chunk_data or not chunk_data.strip():
                    continue

                # åˆ›å»ºå†…å®¹æ–‡æ¡£
                doc = Document(
                    page_content=chunk_data,
                    metadata={
                        "type": "content",
                        "chunk_id": i,
                        "page": chunk_page,
                        "doc_name": doc_name,
                    }
                )
                vector_db_docs.append(doc)

            # æ·»åŠ æ–‡æ¡£ç»“æ„ä¿¡æ¯
            structure_doc = Document(
                page_content="Document Structure",
                metadata={
                    "type": "structure",
                    "doc_name": doc_name,
                    "total_chunks": len(chunks),
                    "summary": summary,
                }
            )
            vector_db_docs.append(structure_doc)

            # æ„å»ºå‘é‡æ•°æ®åº“
            logger.info(f"å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“ï¼Œå…± {len(vector_db_docs)} ä¸ªæ–‡æ¡£...")
            vector_db_client.build_vector_db(vector_db_docs)

            logger.info(f"âœ… [Tool:build_index] ç´¢å¼•æ„å»ºå®Œæˆ: {index_path}")
            return index_path

        except Exception as e:
            logger.error(f"âŒ [Tool:build_index] ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            raise


    async def extract_pdf_data_impl(self, pdf_file_path: str) -> Dict[str, Any]:
        """
        å°† PDF è½¬ä¸ºå›¾ç‰‡å¹¶ç”¨ LLM æå–æ¯é¡µå†…å®¹ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            pdf_file_path: PDF æ–‡ä»¶åï¼ˆä¸å«è·¯å¾„å’Œæ‰©å±•åï¼‰

        Returns:
            æå–ç»“æœå­—å…¸:
            {
                "pdf_data_list": List[Dict],  # æ¯é¡µæå–çš„å†…å®¹
                "image_paths": List[str],      # å›¾ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨
                "json_path": str,              # JSONæ•°æ®æ–‡ä»¶è·¯å¾„
                "image_folder": str            # å›¾ç‰‡æ–‡ä»¶å¤¹è·¯å¾„
            }

        Raises:
            ValueError: è¾“å…¥å‚æ•°æ— æ•ˆ
            FileNotFoundError: PDFæ–‡ä»¶ä¸å­˜åœ¨
            Exception: å¤„ç†è¿‡ç¨‹ä¸­çš„å…¶ä»–é”™è¯¯
        """
        logger.info(f"ğŸ“„ [Tool:extract_pdf] ========== å¼€å§‹æå–PDFå†…å®¹ ==========")
        logger.info(f"ğŸ“„ [Tool:extract_pdf] è¾“å…¥æ–‡ä»¶å: {pdf_file_path}")

        # è¾“å…¥éªŒè¯
        if not pdf_file_path or not isinstance(pdf_file_path, str):
            raise ValueError("PDFæ–‡ä»¶è·¯å¾„ä¸èƒ½ä¸ºç©ºä¸”å¿…é¡»æ˜¯å­—ç¬¦ä¸²")

        # æ„å»ºè·¯å¾„
        output_folder_path = os.path.join(self.pdf_image_path, pdf_file_path)
        pdf_path = os.path.join(self.pdf_path, f"{pdf_file_path}.pdf")
        # JSONæ–‡ä»¶æ”¾åœ¨æ–‡æ¡£æ–‡ä»¶å¤¹ä¸­
        doc_json_folder = os.path.join(self.json_data_path, pdf_file_path)
        output_json_path = os.path.join(doc_json_folder, "data.json")

        logger.info(f"ğŸ“„ [Tool:extract_pdf] å®Œæ•´è·¯å¾„:")
        logger.info(f"ğŸ“„ [Tool:extract_pdf]   - PDF: {pdf_path}")
        logger.info(f"ğŸ“„ [Tool:extract_pdf]   - å›¾ç‰‡æ–‡ä»¶å¤¹: {output_folder_path}")
        logger.info(f"ğŸ“„ [Tool:extract_pdf]   - JSONè¾“å‡º: {output_json_path}")

        # éªŒè¯PDFæ–‡ä»¶å­˜åœ¨
        if not os.path.exists(pdf_path):
            logger.error(f"ğŸ“„ [Tool:extract_pdf] âŒ PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")
            raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {pdf_path}")

        logger.info(f"ğŸ“„ [Tool:extract_pdf] âœ… PDFæ–‡ä»¶å­˜åœ¨")

        try:
            image_paths = []

            # æ£€æŸ¥æ˜¯å¦å·²æœ‰JSONæ•°æ®
            if os.path.exists(output_json_path):
                logger.info(f"å‘ç°å·²å­˜åœ¨çš„JSONæ•°æ®: {output_json_path}")
                try:
                    with open(output_json_path, 'r', encoding='utf-8') as f:
                        image_content_list = json.load(f)

                    # è·å–å·²å­˜åœ¨çš„å›¾ç‰‡è·¯å¾„
                    if os.path.exists(output_folder_path):
                        image_paths = read_images_in_directory(output_folder_path)
                        # æ’åºå›¾ç‰‡è·¯å¾„
                        def safe_page_sort(path):
                            try:
                                match = re.search(r'page_(\d+)\.png', path)
                                return int(match.group(1)) if match else float('inf')
                            except:
                                return float('inf')
                        image_paths = sorted(image_paths, key=safe_page_sort)

                    logger.info(f"âœ… [Tool:extract_pdf] ä»ç¼“å­˜åŠ è½½: {len(image_content_list)} é¡µ")

                    return {
                        "pdf_data_list": image_content_list,
                        "image_paths": image_paths,
                        "json_path": output_json_path,
                        "image_folder": output_folder_path
                    }
                except Exception as e:
                    logger.warning(f"è¯»å–ç¼“å­˜JSONå¤±è´¥ï¼Œå°†é‡æ–°æå–: {e}")

            # è½¬æ¢PDFä¸ºå›¾ç‰‡
            logger.info(f"ğŸ“„ [Tool:extract_pdf] å¼€å§‹è½¬æ¢PDFä¸ºå›¾ç‰‡...")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - DPI: {self.pdf_dpi}")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - è´¨é‡é¢„è®¾: {self.pdf_quality}")
            conversion_stats = pdf_to_images(
                pdf_path, output_folder_path,
                dpi=self.pdf_dpi, quality=self.pdf_quality
            )
            logger.info(f"ğŸ“„ [Tool:extract_pdf] âœ… PDFè½¬å›¾ç‰‡å®Œæˆ: æˆåŠŸ {conversion_stats['successful_pages']} é¡µ")

            # è·å–å›¾ç‰‡è·¯å¾„å¹¶æ’åº
            image_paths = read_images_in_directory(output_folder_path)
            if not image_paths:
                logger.error("æ²¡æœ‰æ‰¾åˆ°å¯å¤„ç†çš„å›¾ç‰‡æ–‡ä»¶")
                return {
                    "pdf_data_list": [],
                    "image_paths": [],
                    "json_path": output_json_path,
                    "image_folder": output_folder_path
                }

            # å®‰å…¨çš„é¡µç æ’åº
            def safe_page_sort(path):
                try:
                    match = re.search(r'page_(\d+)\.png', path)
                    return int(match.group(1)) if match else float('inf')
                except:
                    return float('inf')

            sorted_image_paths = sorted(image_paths, key=safe_page_sort)
            logger.info(f"æ‰¾åˆ° {len(sorted_image_paths)} ä¸ªå›¾ç‰‡æ–‡ä»¶å¾…å¤„ç†")

            # ä½¿ç”¨å¹¶è¡Œå¤„ç†æå–å›¾ç‰‡å†…å®¹
            extract_prompt = READER_PROMPTS.get(
                ReaderRole.IMAGE_EXTRACT, "è¯·æå–å›¾ç‰‡ä¸­çš„æ–‡å­—å†…å®¹"
            )
            logger.info(f"ğŸ“„ [Tool:extract_pdf] å¼€å§‹å¹¶è¡Œæå–å›¾ç‰‡å†…å®¹...")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - å›¾ç‰‡æ•°é‡: {len(sorted_image_paths)}")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - æœ€å¤§å¹¶å‘: 5")
            logger.info(f"ğŸ“„ [Tool:extract_pdf]   - ä½¿ç”¨LLM: {self.llm.provider}")

            # ç›´æ¥ä½¿ç”¨å¼‚æ­¥æ–¹æ³•ï¼ˆå› ä¸ºå½“å‰å·²ç»åœ¨asyncä¸Šä¸‹æ–‡ä¸­ï¼‰
            from src.core.processing.parallel_processor import PageExtractor
            extractor = PageExtractor(self.llm, extract_prompt, max_concurrent=5)
            image_content_list = await extractor.extract_pages_parallel(sorted_image_paths)

            logger.info(f"ğŸ“„ [Tool:extract_pdf] âœ… å›¾ç‰‡å†…å®¹æå–å®Œæˆ")

            # ä¿å­˜æå–ç»“æœåˆ°JSONæ–‡ä»¶
            if image_content_list:
                try:
                    # ç¡®ä¿ç›®å½•å­˜åœ¨
                    os.makedirs(os.path.dirname(output_json_path), exist_ok=True)

                    with open(output_json_path, 'w', encoding='utf-8') as file:
                        json.dump(image_content_list, file, ensure_ascii=False, indent=2)

                    logger.info(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_json_path}")
                    logger.info(f"âœ… [Tool:extract_pdf] æå–ç»Ÿè®¡: æˆåŠŸ{len(image_content_list)}é¡µ")
                except Exception as e:
                    logger.error(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")
                    raise
            else:
                logger.error("æ²¡æœ‰æˆåŠŸæå–ä»»ä½•é¡µé¢å†…å®¹")

            return {
                "pdf_data_list": image_content_list,
                "image_paths": sorted_image_paths,
                "json_path": output_json_path,
                "image_folder": output_folder_path
            }

        except Exception as e:
            logger.error(f"âŒ [Tool:extract_pdf] PDFæ•°æ®æå–å¤±è´¥: {e}")
            raise

    def split_pdf_raw_data(self, pdf_raw_data: List[Any]) -> List[List[Any]]:
        """
        å°† PDF åŸå§‹æ•°æ®æŒ‰ç…§ chunk_count è¿›è¡Œåˆ‡åˆ†

        Args:
            pdf_raw_data: PDFåŸå§‹æ•°æ®åˆ—è¡¨

        Returns:
            åˆ‡åˆ†åçš„æ•°æ®å—åˆ—è¡¨
        """
        if not isinstance(pdf_raw_data, list):
            logger.error("pdf_raw_data ä¸æ˜¯ listï¼Œæ— æ³•åˆ‡åˆ†")
            return []

        chunks = [
            pdf_raw_data[i:i + self.chunk_count]
            for i in range(0, len(pdf_raw_data), self.chunk_count)
        ]
        logger.info(f"å·²å°† pdf_raw_data åˆ‡åˆ†ä¸º {len(chunks)} ä¸ªå—ï¼Œæ¯å—æœ€å¤š {self.chunk_count} æ¡")
        return chunks

    async def extract_toc_from_pages_impl(
        self,
        pdf_data_list: List[Dict[str, str]],
        max_pages: int = 10
    ) -> tuple[Optional[Dict[str, List]], bool]:
        """
        ä»PDFå‰å‡ é¡µå¿«é€Ÿæå–ç›®å½•ç»“æ„ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            pdf_data_list: PDFæ¯é¡µæ•°æ®åˆ—è¡¨
            max_pages: æœ€å¤šæ£€æŸ¥çš„é¡µæ•°

        Returns:
            (agenda_dict, has_toc): ç›®å½•å­—å…¸å’Œæ˜¯å¦æ‰¾åˆ°ç›®å½•çš„æ ‡å¿—
        """
        logger.info(f"ğŸ“– [Tool:extract_toc] å°è¯•ä»å‰ {max_pages} é¡µæå–ç›®å½•")

        try:
            from src.config.prompts.reader_prompts import ReaderRole
            from src.utils.helpers import extract_data_from_LLM_res

            # åˆå¹¶å‰å‡ é¡µçš„å†…å®¹
            toc_pages = pdf_data_list[:max_pages]
            combined_content = "\n\n".join([
                f"[Page {item.get('page', i+1)}]\n{item.get('data', '')}"
                for i, item in enumerate(toc_pages)
            ])

            # æ„å»ºæå–ç›®å½•çš„ prompt
            input_prompt = f"è¿™é‡Œæ˜¯æ–‡ç« çš„å‰ {len(toc_pages)} é¡µå†…å®¹ï¼Œè¯·æŸ¥æ‰¾å¹¶æå–ç›®å½•ç»“æ„: {combined_content}"

            # è°ƒç”¨ LLM æå–ç›®å½•
            response = self.llm.call_llm_chain(
                ReaderRole.CHAPTER_EXTRACT,
                input_prompt,
                "toc_extract"
            )

            if not response:
                logger.warning("LLMè¿”å›ç©ºå“åº”ï¼Œæœªæ‰¾åˆ°ç›®å½•")
                return None, False

            # è§£æ LLM è¿”å›çš„ç»“æœ
            result = extract_data_from_LLM_res(response)

            if not result or not isinstance(result, list) or len(result) == 0:
                logger.info("æœªåœ¨å‰å‡ é¡µæ£€æµ‹åˆ°ç›®å½•ç»“æ„")
                return None, False

            # è½¬æ¢ä¸º agenda_dict æ ¼å¼: {title: [pages]}
            agenda_dict = {}
            for item in result:
                if isinstance(item, dict) and "title" in item and "pages" in item:
                    title = item["title"]
                    pages = item["pages"]
                    if isinstance(pages, list):
                        agenda_dict[title] = pages
                    else:
                        agenda_dict[title] = [pages]

            if agenda_dict:
                logger.info(f"âœ… [Tool:extract_toc] æˆåŠŸæå–ç›®å½•: {len(agenda_dict)} ä¸ªç« èŠ‚")
                return agenda_dict, True
            else:
                logger.info("è§£æç»“æœä¸ºç©ºï¼Œæœªæ‰¾åˆ°æœ‰æ•ˆç›®å½•")
                return None, False

        except Exception as e:
            logger.error(f"âŒ [Tool:extract_toc] æå–ç›®å½•å¤±è´¥: {e}")
            return None, False

    async def analyze_full_structure_impl(
        self,
        pdf_data_list: List[Dict[str, str]]
    ) -> Dict[str, List]:
        """
        åˆ†ææ•´ä¸ªPDFæ–‡æ¡£çš„ç»“æ„ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        å½“PDFæ²¡æœ‰æ˜ç¡®ç›®å½•æ—¶ï¼Œéå†å…¨æ–‡åˆ†æç« èŠ‚ç»“æ„

        Args:
            pdf_data_list: PDFæ¯é¡µæ•°æ®åˆ—è¡¨

        Returns:
            agenda_dict: ç›®å½•å­—å…¸ {title: [pages]}
        """
        logger.info(f"ğŸ” [Tool:analyze_structure] å¼€å§‹åˆ†æå…¨æ–‡ç»“æ„: {len(pdf_data_list)} é¡µ")

        try:
            from src.config.prompts.reader_prompts import ReaderRole
            from src.utils.helpers import extract_data_from_LLM_res

            # å°†PDFæ•°æ®åˆ†å—å¤„ç†ï¼ˆé¿å…å•æ¬¡å¤„ç†è¿‡é•¿ï¼‰
            chunks = self.split_pdf_raw_data(pdf_data_list)

            all_agenda_list = []

            # å¹¶è¡Œå¤„ç†æ¯ä¸ªåˆ†å—
            for i, chunk in enumerate(chunks):
                logger.info(f"å¤„ç†åˆ†å— {i+1}/{len(chunks)}")

                # åˆå¹¶åˆ†å—å†…å®¹
                chunk_content = "\n\n".join([
                    f"[Page {item.get('page', idx+1)}]\n{item.get('data', '')}"
                    for idx, item in enumerate(chunk)
                ])

                # æ„å»º prompt
                input_prompt = f"è¿™é‡Œæ˜¯æ–‡ç« çš„éƒ¨åˆ†å†…å®¹: {chunk_content}"

                # è°ƒç”¨ LLM æå–ç« èŠ‚
                response = self.llm.call_llm_chain(
                    ReaderRole.CHAPTER_EXTRACT,
                    input_prompt,
                    f"structure_extract_chunk"
                )

                if response:
                    result = extract_data_from_LLM_res(response)
                    if isinstance(result, list):
                        all_agenda_list.extend(result)

            # è½¬æ¢ä¸º agenda_dict
            from src.utils.helpers import group_data_by_sections_with_titles

            # ç›´æ¥ä½¿ç”¨ pdf_data_listï¼ˆå·²ç»æ˜¯æ­£ç¡®æ ¼å¼ï¼šList[Dict[str, Any]]ï¼‰
            # pdf_data_list æ ¼å¼: [{"page": "1", "data": "..."}, ...]
            _, agenda_list = group_data_by_sections_with_titles(all_agenda_list, pdf_data_list)

            # å°†åˆ—è¡¨æ ¼å¼è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
            # agenda_list: [{'title': 'ç« èŠ‚1', 'pages': [1,2,3]}, ...]
            # agenda_dict: {'ç« èŠ‚1': [1,2,3], ...}
            agenda_dict = {
                item['title']: item['pages']
                for item in agenda_list
            }

            logger.info(f"âœ… [Tool:analyze_structure] ç»“æ„åˆ†æå®Œæˆ: {len(agenda_dict)} ä¸ªç« èŠ‚")

            return agenda_dict

        except Exception as e:
            logger.error(f"âŒ [Tool:analyze_structure] ç»“æ„åˆ†æå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æ„ï¼ˆæ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªç« èŠ‚ï¼‰
            return {"å…¨æ–‡": list(range(1, len(pdf_data_list) + 1))}

    # ==================== WorkflowèŠ‚ç‚¹æ–¹æ³• ====================

    async def check_cache(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤0ï¼šæ£€æŸ¥æ‰€æœ‰é˜¶æ®µçš„ç¼“å­˜æ–‡ä»¶

        æ£€æŸ¥æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œè®¾ç½®è·³è¿‡æ ‡å¿—ï¼Œå¹¶å°è¯•åŠ è½½å·²æœ‰æ•°æ®
        """
        logger.info(f"ğŸ” [CheckCache] ========== æ­¥éª¤0: æ£€æŸ¥æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ ==========")
        logger.info(f"ğŸ” [CheckCache] æ–‡æ¡£åç§°: {state['doc_name']}")

        doc_name = state["doc_name"]
        doc_type = state.get("doc_type")

        # åˆå§‹åŒ–é˜¶æ®µçŠ¶æ€å­—å…¸
        stage_status = {
            "parse": {"skip": False, "files": []},
            "extract_structure": {"skip": False, "files": []},
            "chunk_text": {"skip": False, "files": []},
            "process_chapters": {"skip": False, "files": []},
            "build_index": {"skip": False, "files": []},
            "generate_summary": {"skip": False, "files": []},
        }

        # å®šä¹‰æ‰€æœ‰æ–‡ä»¶è·¯å¾„
        from src.config.settings import DATA_ROOT
        # JSON æ–‡ä»¶ç»Ÿä¸€æ”¾åœ¨ä»¥æ–‡æ¡£åå‘½åçš„æ–‡ä»¶å¤¹ä¸­
        doc_json_folder = os.path.join(self.json_data_path, doc_name)
        json_path = os.path.join(doc_json_folder, "data.json")
        structure_json_path = os.path.join(doc_json_folder, "structure.json")
        chunk_json_path = os.path.join(doc_json_folder, "chunks.json")
        image_folder = os.path.join(self.pdf_image_path, doc_name)
        # æ³¨æ„ï¼šä¸å†ä½¿ç”¨å•ç‹¬çš„ chapters.jsonï¼Œæ•°æ®å­˜å‚¨åœ¨ vector db ä¸­
        vector_db_path = Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index"
        summary_txt_path = os.path.join(DATA_ROOT, "output", f"{doc_name}_brief_summary.md")

        # æ£€æŸ¥æ¯ä¸ªé˜¶æ®µçš„æ–‡ä»¶
        logger.info(f"ğŸ” [CheckCache] å¼€å§‹æ£€æŸ¥å„é˜¶æ®µæ–‡ä»¶...")

        # 1. æ£€æŸ¥ parse é˜¶æ®µ
        if Path(json_path).exists():
            logger.info(f"âœ… [CheckCache] parse: JSONæ–‡ä»¶å­˜åœ¨")
            stage_status["parse"]["skip"] = True
            stage_status["parse"]["files"] = [image_folder, json_path] if Path(image_folder).exists() else [json_path]

            # å°è¯•åŠ è½½ PDF æ•°æ®
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    pdf_data_list = json.load(f)
                state["pdf_data_list"] = pdf_data_list
                state["json_data_dict"] = {str(item.get("page", i+1)): item.get("data", "") for i, item in enumerate(pdf_data_list)}
                state["raw_data"] = "\n\n".join([f"[Page {item.get('page', i+1)}]\n{item.get('data', '')}" for i, item in enumerate(pdf_data_list)])
                logger.info(f"   ğŸ“¥ å·²åŠ è½½ PDF æ•°æ®: {len(pdf_data_list)} é¡µ")
            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] PDF æ•°æ®åŠ è½½å¤±è´¥: {e}")
                stage_status["parse"]["skip"] = False
        else:
            logger.info(f"âŒ [CheckCache] parse: JSONæ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")

        # 2. æ£€æŸ¥ extract_structure é˜¶æ®µ
        if Path(structure_json_path).exists():
            logger.info(f"âœ… [CheckCache] extract_structure: ç»“æ„æ–‡ä»¶å­˜åœ¨")
            stage_status["extract_structure"]["skip"] = True
            stage_status["extract_structure"]["files"] = [structure_json_path]
            logger.debug(f"ğŸ” [CheckCache] è®¾ç½® stage_status['extract_structure']['skip'] = {stage_status['extract_structure']['skip']}")

            try:
                with open(structure_json_path, 'r', encoding='utf-8') as f:
                    structure_data = json.load(f)

                # å…¼å®¹ä¸¤ç§æ ¼å¼ï¼š
                # æ–°æ ¼å¼: {"agenda_dict": {...}, "has_toc": true}
                # æ—§æ ¼å¼: ç›´æ¥æ˜¯ agenda_dict å­—å…¸
                if isinstance(structure_data, dict):
                    if "agenda_dict" in structure_data:
                        # æ–°æ ¼å¼
                        state["agenda_dict"] = structure_data.get("agenda_dict", {})
                        state["has_toc"] = structure_data.get("has_toc", False)
                    else:
                        # æ—§æ ¼å¼ï¼šæ•´ä¸ªæ–‡ä»¶å°±æ˜¯ agenda_dict
                        state["agenda_dict"] = structure_data
                        state["has_toc"] = True  # æœ‰ç»“æ„æ–‡ä»¶å°±è®¤ä¸ºæœ‰ç›®å½•

                    logger.info(f"   ğŸ“¥ å·²åŠ è½½ç»“æ„: {len(state['agenda_dict'])} ä¸ªç« èŠ‚, has_toc={state.get('has_toc')}")
                else:
                    logger.warning(f"âš ï¸  [CheckCache] ç»“æ„æ•°æ®æ ¼å¼é”™è¯¯ï¼ˆéå­—å…¸ç±»å‹ï¼‰")
                    stage_status["extract_structure"]["skip"] = False
            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] ç»“æ„æ•°æ®åŠ è½½å¤±è´¥: {e}")
                stage_status["extract_structure"]["skip"] = False
                logger.debug(f"ğŸ” [CheckCache] åŠ è½½å¤±è´¥ï¼Œé‡ç½® stage_status['extract_structure']['skip'] = {stage_status['extract_structure']['skip']}")
        else:
            logger.info(f"âŒ [CheckCache] extract_structure: ç»“æ„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")

        # 3. æ£€æŸ¥ chunk_text é˜¶æ®µ
        if Path(chunk_json_path).exists():
            logger.info(f"âœ… [CheckCache] chunk_text: ç« èŠ‚æ•°æ®æ–‡ä»¶å­˜åœ¨")
            stage_status["chunk_text"]["skip"] = True
            stage_status["chunk_text"]["files"] = [chunk_json_path]

            try:
                with open(chunk_json_path, 'r', encoding='utf-8') as f:
                    agenda_data_list = json.load(f)
                state["agenda_data_list"] = agenda_data_list
                logger.info(f"   ğŸ“¥ å·²åŠ è½½ç« èŠ‚æ•°æ®: {len(agenda_data_list)} ä¸ªç« èŠ‚")
            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] ç« èŠ‚æ•°æ®åŠ è½½å¤±è´¥: {e}")
                stage_status["chunk_text"]["skip"] = False
        else:
            logger.info(f"âŒ [CheckCache] chunk_text: ç« èŠ‚æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")

        # 4 & 5. æ£€æŸ¥ build_index é˜¶æ®µï¼ˆprocess_chapters ä¸ build_index ç»‘å®šï¼‰
        # å¦‚æœ vector db å­˜åœ¨ï¼Œåˆ™åŒæ—¶è·³è¿‡ process_chapters å’Œ build_index
        if vector_db_path.exists() and any(vector_db_path.iterdir()):
            logger.info(f"âœ… [CheckCache] build_index: Vector DBå­˜åœ¨")
            stage_status["build_index"]["skip"] = True
            stage_status["process_chapters"]["skip"] = True  # ç»‘å®šè·³è¿‡
            stage_status["build_index"]["files"] = [str(vector_db_path)]
            stage_status["process_chapters"]["files"] = [str(vector_db_path)]
            state["index_path"] = str(vector_db_path)

            # ä» Vector DB åŠ è½½ chapter_summaries æ•°æ®
            try:
                from src.core.vector_db.vector_db_client import VectorDBClient

                logger.info(f"   ğŸ“¥ æ­£åœ¨ä» Vector DB åŠ è½½ç« èŠ‚æ‘˜è¦æ•°æ®...")

                # ä½¿ç”¨ VectorDBClient åŠ è½½ vector db
                vector_db_client = VectorDBClient(str(vector_db_path), embedding_model=self.embedding_model)
                # VectorDBClient åœ¨åˆå§‹åŒ–æ—¶ä¼šè‡ªåŠ¨åŠ è½½å·²å­˜åœ¨çš„ vector dbï¼ˆè§ __init__ çš„ auto-load é€»è¾‘ï¼‰

                # ä» docstore ä¸­æå–æ‰€æœ‰æ–‡æ¡£
                chapter_summaries = {}
                chapter_refactors = {}
                raw_data_dict = {}

                # éå† docstore ä¸­çš„æ‰€æœ‰æ–‡æ¡£
                if vector_db_client.vector_db and vector_db_client.vector_db.docstore:
                    for doc_id, doc in vector_db_client.vector_db.docstore._dict.items():
                        metadata = doc.metadata
                        doc_type = metadata.get("type")

                        # åªå¤„ç† type="context" çš„æ–‡æ¡£ï¼ˆåŒ…å«æ‘˜è¦ä¿¡æ¯ï¼‰
                        if doc_type == "context":
                            title = metadata.get("title", "")
                            if title:
                                # page_content å°±æ˜¯ summary
                                chapter_summaries[title] = doc.page_content
                                # metadata ä¸­çš„å…¶ä»–ä¿¡æ¯
                                chapter_refactors[title] = metadata.get("refactor", "")
                                raw_data_dict[title] = metadata.get("raw_data", {})

                    state["chapter_summaries"] = chapter_summaries
                    state["chapter_refactors"] = chapter_refactors
                    state["raw_data_dict"] = raw_data_dict

                    logger.info(f"   ğŸ“¥ å·²ä» Vector DB åŠ è½½: {len(chapter_summaries)} ä¸ªç« èŠ‚æ‘˜è¦")
                    logger.info(f"   â­ï¸  process_chapters å’Œ build_index éƒ½å°†è·³è¿‡")
                else:
                    logger.warning(f"âš ï¸  [CheckCache] Vector DB åŠ è½½åä¸ºç©º")
                    stage_status["build_index"]["skip"] = False
                    stage_status["process_chapters"]["skip"] = False

            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] ä» Vector DB åŠ è½½æ•°æ®å¤±è´¥: {e}")
                import traceback
                logger.debug(traceback.format_exc())
                # åŠ è½½å¤±è´¥ï¼Œä¸è·³è¿‡è¿™ä¸¤ä¸ªé˜¶æ®µ
                stage_status["build_index"]["skip"] = False
                stage_status["process_chapters"]["skip"] = False
                logger.info(f"   âŒ éœ€è¦é‡æ–°æ‰§è¡Œ process_chapters å’Œ build_index")
        else:
            logger.info(f"âŒ [CheckCache] build_index: Vector DBä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")
            logger.info(f"âŒ [CheckCache] process_chapters: éœ€è¦æ‰§è¡Œ")

        # 6. æ£€æŸ¥ generate_summary é˜¶æ®µ
        if Path(summary_txt_path).exists():
            logger.info(f"âœ… [CheckCache] generate_summary: æ‘˜è¦æ–‡ä»¶å­˜åœ¨")
            stage_status["generate_summary"]["skip"] = True
            stage_status["generate_summary"]["files"] = [summary_txt_path]

            try:
                with open(summary_txt_path, 'r', encoding='utf-8') as f:
                    brief_summary = f.read()
                state["brief_summary"] = brief_summary
                logger.info(f"   ğŸ“¥ å·²åŠ è½½æ‘˜è¦: {len(brief_summary)} å­—ç¬¦")
            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] æ‘˜è¦åŠ è½½å¤±è´¥: {e}")
                stage_status["generate_summary"]["skip"] = False
        else:
            logger.info(f"âŒ [CheckCache] generate_summary: æ‘˜è¦æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")

        # ä¿å­˜é˜¶æ®µçŠ¶æ€åˆ° state
        state["stage_status"] = stage_status
        logger.debug(f"ğŸ” [CheckCache] ä¿å­˜åˆ° state çš„ stage_status: {stage_status}")

        # ç»Ÿè®¡ä¿¡æ¯
        skip_count = sum(1 for s in stage_status.values() if s["skip"])
        total_count = len(stage_status)
        logger.info(f"\nğŸ” [CheckCache] æ£€æŸ¥å®Œæˆ: {skip_count}/{total_count} ä¸ªé˜¶æ®µå¯è·³è¿‡")

        # è¯¦ç»†è¾“å‡ºæ¯ä¸ªé˜¶æ®µçš„çŠ¶æ€
        for stage_name, status_info in stage_status.items():
            skip_status = "âœ… è·³è¿‡" if status_info["skip"] else "âŒ æ‰§è¡Œ"
            logger.debug(f"   {stage_name}: {skip_status}")

        # æ›´æ–° registry çŠ¶æ€ï¼ˆåŒæ­¥å·²æœ‰æ–‡ä»¶ï¼‰
        for stage_name, status_info in stage_status.items():
            if status_info["skip"]:
                self.doc_registry.update_stage_status(
                    doc_name=doc_name,
                    stage_name=stage_name,
                    status="completed",
                    output_files=status_info["files"]
                )

        return state

    async def extract_structure(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤2ï¼šæå–æ–‡æ¡£ç›®å½•ç»“æ„

        ç­–ç•¥ï¼š
        1. å…ˆå°è¯•ä»å‰ 5-10 é¡µæå–ç›®å½•ï¼ˆå¿«é€Ÿï¼‰
        2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåˆ†æå…¨æ–‡ç»“æ„ï¼ˆæ…¢ä½†å…¨é¢ï¼‰
        """
        logger.info(f"ğŸ“š [ExtractStructure] ========== æ­¥éª¤2: æå–æ–‡æ¡£ç»“æ„ ==========")
        logger.info(f"ğŸ“š [ExtractStructure] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        logger.debug(f"ğŸ” [ExtractStructure] stage_status = {stage_status}")
        extract_status = stage_status.get("extract_structure", {})
        logger.debug(f"ğŸ” [ExtractStructure] extract_structure status = {extract_status}")
        should_skip = extract_status.get("skip", False)
        logger.debug(f"ğŸ” [ExtractStructure] should_skip = {should_skip}")
        if should_skip:
            logger.info(f"â­ï¸  [ExtractStructure] å·²æœ‰ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡ç»“æ„æå–")
            logger.info(f"ğŸ“š [ExtractStructure] å·²æœ‰ {len(state.get('agenda_dict', {}))} ä¸ªç« èŠ‚")
            return state

        logger.info(f"ğŸ“š [ExtractStructure] å¼€å§‹æå–æ–‡æ¡£ç»“æ„...")

        doc_name = state["doc_name"]
        doc_type = state.get("doc_type")

        # ä»…PDFç±»å‹éœ€è¦æå–ç»“æ„
        if doc_type != "pdf":
            logger.info("éPDFæ–‡æ¡£ï¼Œè·³è¿‡ç»“æ„æå–")
            state["has_toc"] = False
            state["agenda_dict"] = {}
            return state

        # å®šä¹‰ç»“æ„æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨æ–‡æ¡£æ–‡ä»¶å¤¹ï¼‰
        doc_json_folder = os.path.join(self.json_data_path, doc_name)
        structure_json_path = os.path.join(doc_json_folder, "structure.json")

        try:
            pdf_data_list = state.get("pdf_data_list", [])
            if not pdf_data_list:
                logger.warning("PDFæ•°æ®ä¸ºç©ºï¼Œæ— æ³•æå–ç»“æ„")
                state["has_toc"] = False
                state["agenda_dict"] = {}
                return state

            # ç­–ç•¥1ï¼šå°è¯•ä»å‰å‡ é¡µå¿«é€Ÿæå–ç›®å½•
            logger.info("ğŸš€ [ExtractStructure] ç­–ç•¥1: å°è¯•ä»å‰10é¡µæå–ç›®å½•")
            agenda_dict, has_toc = await self.extract_toc_from_pages_impl(
                pdf_data_list,
                max_pages=10
            )

            if has_toc and agenda_dict:
                # æˆåŠŸæ‰¾åˆ°ç›®å½•
                logger.info(f"âœ… [ExtractStructure] æ£€æµ‹åˆ°ç›®å½•ç»“æ„: {len(agenda_dict)} ä¸ªç« èŠ‚")
                state["agenda_dict"] = agenda_dict
                state["has_toc"] = True
            else:
                # ç­–ç•¥2ï¼šæ²¡æ‰¾åˆ°ç›®å½•ï¼Œåˆ†æå…¨æ–‡ç»“æ„
                logger.info("ğŸ” [ExtractStructure] ç­–ç•¥2: åˆ†æå…¨æ–‡ç»“æ„")
                agenda_dict = await self.analyze_full_structure_impl(pdf_data_list)

                state["agenda_dict"] = agenda_dict
                state["has_toc"] = False
                logger.info(f"âœ… [ExtractStructure] å…¨æ–‡åˆ†æå®Œæˆ: {len(agenda_dict)} ä¸ªç« èŠ‚")

            # æ‰“å°ç›®å½•ä¿¡æ¯
            logger.info("ğŸ“‘ [ExtractStructure] æ–‡æ¡£ç›®å½•ç»“æ„:")
            for title, pages in list(state["agenda_dict"].items())[:5]:
                logger.info(f"  - {title}: ç¬¬ {pages[0]}-{pages[-1]} é¡µ")
            if len(state["agenda_dict"]) > 5:
                logger.info(f"  ... è¿˜æœ‰ {len(state['agenda_dict']) - 5} ä¸ªç« èŠ‚")

            # ä¿å­˜ç»“æ„æ•°æ®åˆ°æ–‡ä»¶
            structure_data = {
                "agenda_dict": state["agenda_dict"],
                "has_toc": state["has_toc"]
            }
            try:
                os.makedirs(os.path.dirname(structure_json_path), exist_ok=True)
                with open(structure_json_path, 'w', encoding='utf-8') as f:
                    json.dump(structure_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ [ExtractStructure] ç»“æ„æ•°æ®å·²ä¿å­˜: {structure_json_path}")
            except Exception as e:
                logger.warning(f"âš ï¸  [ExtractStructure] ä¿å­˜ç»“æ„æ•°æ®å¤±è´¥: {e}")

            # æ›´æ–°é˜¶æ®µçŠ¶æ€
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="extract_structure",
                status="completed",
                output_files=[structure_json_path]
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [ExtractStructure] ç»“æ„æå–å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è®¾ç½®é»˜è®¤å€¼
            state["has_toc"] = False
            state["agenda_dict"] = {}

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="extract_structure",
                status="failed",
                output_files=[]
            )

            return state

    async def parse_document(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤1ï¼šè§£ææ–‡æ¡£å†…å®¹

        æ ¹æ® check_cache è®¾ç½®çš„æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡
        """
        logger.info(f"ğŸ“„ [Parse] ========== æ­¥éª¤1: è§£ææ–‡æ¡£ ==========")
        logger.info(f"ğŸ“„ [Parse] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("parse", {}).get("skip"):
            logger.info(f"â­ï¸  [Parse] å·²æœ‰ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡è§£æ")
            state["status"] = "parsed"
            return state

        logger.info(f"ğŸ“„ [Parse] å¼€å§‹è§£ææ–‡æ¡£...")

        doc_name = state["doc_name"]
        doc_type = state.get("doc_type")

        try:
            doc_path = state["doc_path"]

            # åˆå§‹åŒ– generated_filesï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if "generated_files" not in state:
                state["generated_files"] = {
                    "images": [],
                    "json_data": "",
                    "vector_db": "",
                    "summaries": []
                }
                logger.debug(f"ğŸ“„ [Parse] åˆå§‹åŒ– generated_files")

            if doc_type == "pdf":
                # ä½¿ç”¨å®é™…çš„PDFæå–åŠŸèƒ½
                logger.info(f"ğŸ“„ [Parse] ä½¿ç”¨PDFæå–å™¨å¤„ç†: {doc_path}")

                # doc_name å·²ç»æ˜¯ä¸å«æ‰©å±•åçš„æ–‡ä»¶åï¼Œç›´æ¥ä½¿ç”¨
                pdf_file_name = doc_name
                logger.info(f"ğŸ“„ [Parse] PDFæ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰: {pdf_file_name}")

                # æå–PDFæ•°æ®ï¼ˆè¿”å›åŒ…å«æ‰€æœ‰ä¿¡æ¯çš„å­—å…¸ï¼Œé¿å…é‡å¤è¯»å–ï¼‰
                logger.info(f"ğŸ“„ [Parse] å¼€å§‹è°ƒç”¨ extract_pdf_data_impl...")
                extract_result = await self.extract_pdf_data_impl(pdf_file_name)
                logger.info(f"ğŸ“„ [Parse] PDFæ•°æ®æå–å®Œæˆ")

                pdf_data_list = extract_result["pdf_data_list"]
                if not pdf_data_list:
                    raise ValueError(f"PDFæå–å¤±è´¥ï¼Œæœªè·å–ä»»ä½•æ•°æ®: {doc_path}")

                # å°†æå–çš„æ•°æ®è½¬æ¢ä¸ºåŸå§‹æ–‡æœ¬
                raw_data = "\n\n".join([
                    f"[Page {item.get('page', i+1)}]\n{item.get('data', '')}"
                    for i, item in enumerate(pdf_data_list)
                ])

                # åˆ›å»º json_data_dictï¼ˆä»¥é¡µç ä¸ºkeyï¼‰
                json_data_dict = {
                    str(item.get("page", i+1)): item.get("data", "")
                    for i, item in enumerate(pdf_data_list)
                }

                # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
                state["raw_data"] = raw_data
                state["pdf_data_list"] = pdf_data_list  # ä¿å­˜åŸå§‹æ•°æ®ä¾›åç»­ä½¿ç”¨
                state["json_data_dict"] = json_data_dict  # é¡µç ä¸ºkeyçš„æ•°æ®å­—å…¸
                state["generated_files"]["images"] = extract_result["image_paths"]
                state["generated_files"]["json_data"] = extract_result["json_path"]
                state["status"] = "parsed"

                logger.info(f"âœ… [Parse] PDFè§£æå®Œæˆï¼Œæå– {len(pdf_data_list)} é¡µï¼Œæ€»é•¿åº¦: {len(raw_data)} å­—ç¬¦")
                logger.info(f"ğŸ“ [Parse] ç”Ÿæˆæ–‡ä»¶: å›¾ç‰‡{len(state['generated_files']['images'])}ä¸ª, JSON: {state['generated_files']['json_data']}")

                # æ›´æ–°é˜¶æ®µçŠ¶æ€
                image_folder = extract_result.get("image_folder", "")
                json_path = extract_result.get("json_path", "")
                output_files = []
                if image_folder:
                    output_files.append(image_folder)
                if json_path:
                    output_files.append(json_path)

                self.doc_registry.update_stage_status(
                    doc_name=doc_name,
                    stage_name="parse",
                    status="completed",
                    output_files=output_files
                )

            elif doc_type == "url":
                # TODO: ä½¿ç”¨WebReaderæå–å†…å®¹
                logger.warning("URLç±»å‹æ–‡æ¡£æš‚æœªå®ç°ï¼Œä½¿ç”¨å ä½ç¬¦")
                state["raw_data"] = f"Web content from {doc_path}"
                state["status"] = "parsed"

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£ç±»å‹: {doc_type}")

            return state

        except Exception as e:
            logger.error(f"âŒ [Parse] è§£æå¤±è´¥: {e}")
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="parse",
                status="failed",
                output_files=[]
            )

            return state

    async def chunk_text(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤3ï¼šæ„å»ºç« èŠ‚æ•°æ®åˆ—è¡¨

        ç›´æ¥åŸºäº extract_structure å¾—åˆ°çš„ agenda_dict æ„å»º agenda_data_list
        """
        logger.info(f"ğŸ“¦ [Chunk] ========== æ­¥éª¤3: æ„å»ºç« èŠ‚æ•°æ®åˆ—è¡¨ ==========")
        logger.info(f"ğŸ“¦ [Chunk] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("chunk_text", {}).get("skip"):
            logger.info(f"â­ï¸  [Chunk] å·²æœ‰ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡ç« èŠ‚æ•°æ®æ„å»º")
            logger.info(f"ğŸ“¦ [Chunk] å·²æœ‰ {len(state.get('agenda_data_list', []))} ä¸ªç« èŠ‚")
            return state

        logger.info(f"ğŸ“¦ [Chunk] å¼€å§‹æ„å»ºç« èŠ‚æ•°æ®...")

        doc_name = state["doc_name"]

        # å®šä¹‰ç« èŠ‚æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨æ–‡æ¡£æ–‡ä»¶å¤¹ï¼‰
        doc_json_folder = os.path.join(self.json_data_path, doc_name)
        chunk_json_path = os.path.join(doc_json_folder, "chunks.json")

        try:
            agenda_dict = state.get("agenda_dict", {})
            json_data_dict = state.get("json_data_dict", {})

            if not agenda_dict:
                logger.warning("agenda_dict ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç« èŠ‚æ•°æ®")
                state["agenda_data_list"] = []
                state["status"] = "chunked"
                return state

            if not json_data_dict:
                logger.warning("json_data_dict ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç« èŠ‚æ•°æ®")
                state["agenda_data_list"] = []
                state["status"] = "chunked"
                return state

            # ç›´æ¥åŸºäº agenda_dict æ„å»º agenda_data_list
            agenda_data_list = []

            for title, page_numbers in agenda_dict.items():
                # æ”¶é›†è¯¥ç« èŠ‚çš„æ‰€æœ‰é¡µé¢æ•°æ®
                chapter_data = {}

                for page_num in page_numbers:
                    page_key = str(page_num)
                    if page_key in json_data_dict:
                        chapter_data[page_key] = json_data_dict[page_key]
                    else:
                        logger.warning(f"é¡µç  {page_key} ä¸åœ¨ json_data_dict ä¸­")

                if chapter_data:
                    agenda_data_list.append({
                        "title": title,
                        "data": chapter_data,
                        "pages": page_numbers
                    })
                else:
                    logger.warning(f"ç« èŠ‚ '{title}' æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„æ•°æ®")

            logger.info(f"âœ… [Chunk] ç« èŠ‚æ•°æ®æ„å»ºå®Œæˆ: {len(agenda_data_list)} ä¸ªç« èŠ‚")

            # æ‰“å°ç« èŠ‚ä¿¡æ¯
            for item in agenda_data_list:
                title = item.get("title", "æœªçŸ¥")
                pages = item.get("pages", [])
                data_pages = len(item.get("data", {}))
                logger.info(f"  - {title}: {len(pages)} é¡µ (å®é™…æ•°æ®: {data_pages} é¡µ)")

            # ä¿å­˜ç« èŠ‚æ•°æ®åˆ°æ–‡ä»¶
            try:
                os.makedirs(os.path.dirname(chunk_json_path), exist_ok=True)
                with open(chunk_json_path, 'w', encoding='utf-8') as f:
                    json.dump(agenda_data_list, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ [Chunk] ç« èŠ‚æ•°æ®å·²ä¿å­˜: {chunk_json_path}")
            except Exception as e:
                logger.warning(f"âš ï¸  [Chunk] ä¿å­˜ç« èŠ‚æ•°æ®å¤±è´¥: {e}")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["agenda_data_list"] = agenda_data_list
            state["status"] = "chunked"

            # æ›´æ–°é˜¶æ®µçŠ¶æ€
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="chunk_text",
                status="completed",
                output_files=[chunk_json_path]
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [Chunk] ç« èŠ‚æ•°æ®æ„å»ºå¤±è´¥: {e}")
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="chunk_text",
                status="failed",
                output_files=[]
            )

            return state

    async def process_chapters(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤4ï¼šå¤„ç†ç« èŠ‚ï¼ˆå¹¶è¡Œç”Ÿæˆæ‘˜è¦å’Œé‡æ„å†…å®¹ï¼‰

        å¯¹æ¯ä¸ªç« èŠ‚ï¼š
        1. ç”Ÿæˆæ‘˜è¦ï¼ˆsummaryï¼‰
        2. é‡æ„å†…å®¹ï¼ˆrefactorï¼‰
        """
        logger.info(f"ğŸ“ [ProcessChapters] ========== æ­¥éª¤4: å¤„ç†ç« èŠ‚ ==========")
        logger.info(f"ğŸ“ [ProcessChapters] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("process_chapters", {}).get("skip"):
            logger.info(f"â­ï¸  [ProcessChapters] å·²æœ‰ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡ç« èŠ‚å¤„ç†")
            logger.info(f"ğŸ“ [ProcessChapters] å·²æœ‰ {len(state.get('chapter_summaries', {}))} ä¸ªç« èŠ‚æ‘˜è¦")
            return state

        logger.info(f"ğŸ“ [ProcessChapters] å¼€å§‹å¤„ç†ç« èŠ‚...")

        doc_name = state["doc_name"]

        # æ³¨æ„ï¼šç« èŠ‚å¤„ç†ç»“æœä¸å†ä¿å­˜åˆ°å•ç‹¬æ–‡ä»¶ï¼Œè€Œæ˜¯ç›´æ¥åœ¨ build_index é˜¶æ®µå­˜å…¥ vector db

        try:
            agenda_data_list = state.get("agenda_data_list", [])
            logger.info(f"ğŸ“ [ProcessChapters] ç« èŠ‚æ•°é‡: {len(agenda_data_list)}")

            if not agenda_data_list:
                logger.warning("ğŸ“ [ProcessChapters] âš ï¸ agenda_data_list ä¸ºç©ºï¼Œè·³è¿‡ç« èŠ‚å¤„ç†")
                state["chapter_summaries"] = {}
                state["chapter_refactors"] = {}
                state["raw_data_dict"] = {}
                state["status"] = "summarized"
                return state

            # ä½¿ç”¨å¹¶è¡Œå¤„ç†å·¥å…·
            from src.core.processing.parallel_processor import ChapterProcessor
            from src.config.prompts.reader_prompts import ReaderRole

            logger.info(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(agenda_data_list)} ä¸ªç« èŠ‚...")

            # ç›´æ¥ä½¿ç”¨å¼‚æ­¥æ–¹æ³•ï¼ˆå› ä¸ºå½“å‰å·²ç»åœ¨asyncä¸Šä¸‹æ–‡ä¸­ï¼‰
            processor = ChapterProcessor(self.llm, max_concurrent=10)
            chapter_results = await processor.process_chapters_summary_and_refactor(
                agenda_data_list=agenda_data_list,
                summary_role=ReaderRole.CONTENT_SUMMARY,
                refactor_role=ReaderRole.CONTENT_MERGE
            )

            # å¤„ç†ç»“æœ
            chapter_summaries = {}
            chapter_refactors = {}
            raw_data_dict = {}

            for title, summary, refactor_content, _, data in chapter_results:
                chapter_summaries[title] = summary
                chapter_refactors[title] = refactor_content
                raw_data_dict[title] = data

                logger.info(f"âœ… ç« èŠ‚å¤„ç†å®Œæˆ: {title}")

            logger.info(f"âœ… [ProcessChapters] æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆ: {len(chapter_summaries)} ä¸ªç« èŠ‚")
            logger.info(f"ğŸ“Œ [ProcessChapters] æ•°æ®å°†åœ¨ build_index é˜¶æ®µå­˜å…¥ Vector DB")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["chapter_summaries"] = chapter_summaries
            state["chapter_refactors"] = chapter_refactors
            state["raw_data_dict"] = raw_data_dict
            state["status"] = "summarized"

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ï¼ˆæ•°æ®å­˜å‚¨åœ¨ vector db ä¸­ï¼Œæ— å•ç‹¬æ–‡ä»¶ï¼‰
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="process_chapters",
                status="completed",
                output_files=[]  # æ•°æ®åœ¨ vector db ä¸­
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [ProcessChapters] ç« èŠ‚å¤„ç†å¤±è´¥: {e}")
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="process_chapters",
                status="failed",
                output_files=[]
            )

            return state

    async def generate_brief_summary(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤6ï¼šç”Ÿæˆç®€è¦æ‘˜è¦ï¼ˆåŸºäºæ‰€æœ‰ç« èŠ‚æ‘˜è¦ï¼‰

        è¿™æ˜¯æœ€åä¸€æ­¥æ‘˜è¦ç”Ÿæˆï¼Œæ•´åˆæ‰€æœ‰ç« èŠ‚çš„æ‘˜è¦
        """
        logger.info(f"ğŸ“ [BriefSummary] ========== æ­¥éª¤6: ç”Ÿæˆç®€è¦æ‘˜è¦ ==========")
        logger.info(f"ğŸ“ [BriefSummary] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("generate_summary", {}).get("skip"):
            logger.info(f"â­ï¸  [BriefSummary] å·²æœ‰æ‘˜è¦æ–‡ä»¶ï¼Œè·³è¿‡ç”Ÿæˆ")
            logger.info(f"ğŸ“ [BriefSummary] æ‘˜è¦é•¿åº¦: {len(state.get('brief_summary', ''))} å­—ç¬¦")
            return state

        logger.info(f"ğŸ“ [BriefSummary] å¼€å§‹ç”Ÿæˆç®€è¦æ‘˜è¦...")

        doc_name = state["doc_name"]

        # å®šä¹‰æ‘˜è¦æ–‡ä»¶è·¯å¾„
        from src.config.settings import DATA_ROOT
        summary_txt_path = os.path.join(DATA_ROOT, "output", f"{doc_name}_brief_summary.md")

        try:
            chapter_summaries = state.get("chapter_summaries", {})
            logger.info(f"ğŸ“ [BriefSummary] ç« èŠ‚æ‘˜è¦æ•°é‡: {len(chapter_summaries)}")

            if not chapter_summaries:
                logger.warning("ç« èŠ‚æ‘˜è¦ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆç®€è¦æ‘˜è¦")
                state["brief_summary"] = ""
                return state

            # å¤ç”¨ generate_summary_implï¼Œä¼ å…¥ç« èŠ‚æ‘˜è¦
            answer = await self.generate_summary_impl(
                context_data=chapter_summaries,
                doc_name=doc_name,
                session_id="brief_summary"
            )

            logger.info(f"âœ… [BriefSummary] ç®€è¦æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")

            # ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶
            try:
                os.makedirs(os.path.dirname(summary_txt_path), exist_ok=True)
                with open(summary_txt_path, 'w', encoding='utf-8') as f:
                    f.write(answer)
                logger.info(f"ğŸ’¾ [BriefSummary] ç®€è¦æ‘˜è¦å·²ä¿å­˜: {summary_txt_path}")

                # æ›´æ–° generated_files
                if "generated_files" not in state:
                    state["generated_files"] = {"images": [], "json_data": "", "vector_db": "", "summaries": []}
                if "summaries" not in state["generated_files"]:
                    state["generated_files"]["summaries"] = []
                state["generated_files"]["summaries"].append(summary_txt_path)

            except Exception as e:
                logger.warning(f"âš ï¸  [BriefSummary] ä¿å­˜ç®€è¦æ‘˜è¦å¤±è´¥: {e}")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["brief_summary"] = answer

            # æ›´æ–°é˜¶æ®µçŠ¶æ€
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="generate_summary",
                status="completed",
                output_files=[summary_txt_path]
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [BriefSummary] ç®€è¦æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            state["brief_summary"] = f"æ–‡æ¡£ {state['doc_name']} çš„ç®€è¦æ‘˜è¦ï¼ˆç”Ÿæˆé”™è¯¯: {str(e)}ï¼‰"

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="generate_summary",
                status="failed",
                output_files=[]
            )

            return state

    async def build_index(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤5ï¼šæ„å»ºå‘é‡ç´¢å¼•

        åŸºäºç« èŠ‚æ‘˜è¦æ„å»º Document å¯¹è±¡ï¼š
        1. type="context": ç« èŠ‚æ‘˜è¦ä½œä¸ºæ£€ç´¢å†…å®¹
        2. type="title": ç« èŠ‚æ ‡é¢˜ä½œä¸ºæ£€ç´¢å†…å®¹
        3. type="structure": æ–‡æ¡£ç»“æ„ä¿¡æ¯
        """
        logger.info(f"ğŸ”¨ [BuildIndex] ========== æ­¥éª¤5: æ„å»ºå‘é‡ç´¢å¼• ==========")
        logger.info(f"ğŸ”¨ [BuildIndex] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("build_index", {}).get("skip"):
            logger.info(f"â­ï¸  [BuildIndex] å·²æœ‰ Vector DBï¼Œè·³è¿‡æ„å»º")
            logger.info(f"ğŸ”¨ [BuildIndex] Vector DB è·¯å¾„: {state.get('index_path')}")
            state["status"] = "indexed"
            return state

        logger.info(f"ğŸ”¨ [BuildIndex] å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")

        doc_name = state["doc_name"]

        # æ„å»ºç´¢å¼•è·¯å¾„
        from pathlib import Path
        from src.config.settings import DATA_ROOT
        index_path = str(Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index")

        try:
            from langchain.docstore.document import Document
            from src.core.vector_db.vector_db_client import VectorDBClient

            chapter_summaries = state.get("chapter_summaries", {})
            chapter_refactors = state.get("chapter_refactors", {})
            raw_data_dict = state.get("raw_data_dict", {})
            agenda_dict = state.get("agenda_dict", {})

            if not chapter_summaries:
                logger.warning("ç« èŠ‚æ‘˜è¦ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç´¢å¼•")
                state["status"] = "error"
                state["error"] = "ç« èŠ‚æ‘˜è¦ä¸ºç©º"
                return state

            # æ„å»º Document åˆ—è¡¨
            vector_db_docs = []

            logger.info("å¼€å§‹æ„å»º Document å¯¹è±¡...")

            # éå†æ¯ä¸ªç« èŠ‚
            for title, summary in chapter_summaries.items():
                refactor_content = chapter_refactors.get(title, "")
                raw_data = raw_data_dict.get(title, {})
                pages = agenda_dict.get(title, [])

                # Document 1: type="context" - æ‘˜è¦ä½œä¸ºæ£€ç´¢å†…å®¹
                vector_db_docs.append(
                    Document(
                        page_content=summary,
                        metadata={
                            "type": "context",
                            "title": title,
                            "pages": pages,
                            "raw_data": raw_data,
                            "refactor": refactor_content,
                        }
                    )
                )

                # Document 2: type="title" - æ ‡é¢˜ä½œä¸ºæ£€ç´¢å†…å®¹
                vector_db_docs.append(
                    Document(
                        page_content=title,
                        metadata={
                            "type": "title",
                            "pages": pages,
                            "summary": summary,
                            "raw_data": raw_data,
                            "refactor": refactor_content,
                        }
                    )
                )

            # Document 3: type="structure" - æ–‡æ¡£ç»“æ„ä¿¡æ¯
            structure_doc = Document(
                page_content="Document Structure",
                metadata={
                    "type": "structure",
                    "agenda_dict": agenda_dict,
                    "doc_name": doc_name,
                    "total_chapters": len(agenda_dict)
                }
            )
            vector_db_docs.append(structure_doc)

            logger.info(f"Document å¯¹è±¡æ„å»ºå®Œæˆ: {len(vector_db_docs)} ä¸ªæ–‡æ¡£")

            # æ„å»ºç´¢å¼•è·¯å¾„
            index_dir = Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index"
            index_dir.mkdir(parents=True, exist_ok=True)
            index_path = str(index_dir)

            # åˆ›å»º VectorDBClient å¹¶æ„å»ºå‘é‡æ•°æ®åº“
            vector_db_client = VectorDBClient(index_path, embedding_model=self.embedding_model)
            vector_db_client.build_vector_db(vector_db_docs)

            logger.info(f"âœ… [BuildIndex] å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ: {index_path}")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["index_path"] = index_path
            state["vector_db_docs"] = vector_db_docs
            if "generated_files" not in state:
                state["generated_files"] = {
                    "images": [],
                    "json_data": "",
                    "vector_db": "",
                    "summaries": []
                }
            state["generated_files"]["vector_db"] = index_path
            state["status"] = "indexed"

            # æ›´æ–°é˜¶æ®µçŠ¶æ€
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="build_index",
                status="completed",
                output_files=[index_path]
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [BuildIndex] ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="build_index",
                status="failed",
                output_files=[]
            )
            return state

    # ==================== æ‰¹é‡å¤„ç†å’Œç®¡ç†æ–¹æ³• ====================

    async def process_documents_batch(
        self,
        doc_list: List[Dict[str, Any]],
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†æ–‡æ¡£åˆ—è¡¨

        Args:
            doc_list: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼ï¼š
                {
                    "doc_name": str,
                    "doc_path": str,
                    "doc_type": "pdf" | "url"
                }
            max_concurrent: æœ€å¤§å¹¶å‘å¤„ç†æ•°

        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        import asyncio

        logger.info(f"ğŸ“¦ å¼€å§‹æ‰¹é‡å¤„ç†æ–‡æ¡£: å…± {len(doc_list)} ä¸ªæ–‡æ¡£")

        results = []

        # åˆ†æ‰¹å¤„ç†é¿å…è¿‡è½½
        for i in range(0, len(doc_list), max_concurrent):
            batch = doc_list[i:i + max_concurrent]
            logger.info(f"å¤„ç†ç¬¬ {i // max_concurrent + 1} æ‰¹: {len(batch)} ä¸ªæ–‡æ¡£")

            # å¹¶å‘å¤„ç†å½“å‰æ‰¹æ¬¡
            tasks = []
            for doc_info in batch:
                # æ„å»ºåˆå§‹çŠ¶æ€
                state = {
                    "doc_name": doc_info["doc_name"],
                    "doc_path": doc_info["doc_path"],
                    "doc_type": doc_info["doc_type"],
                    "status": "pending"
                }
                # åˆ›å»ºå¤„ç†ä»»åŠ¡
                task = self.graph.ainvoke(state)
                tasks.append(task)

            # ç­‰å¾…å½“å‰æ‰¹æ¬¡å®Œæˆ
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†ç»“æœ
            for j, result in enumerate(batch_results):
                doc_name = batch[j]["doc_name"]
                if isinstance(result, Exception):
                    logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {doc_name}, é”™è¯¯: {result}")
                    results.append({
                        "doc_name": doc_name,
                        "status": "error",
                        "error": str(result)
                    })
                else:
                    logger.info(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {doc_name}, çŠ¶æ€: {result.get('status')}")
                    results.append(result)

        logger.info(f"âœ… æ‰¹é‡å¤„ç†å®Œæˆ: æˆåŠŸ {sum(1 for r in results if r.get('status') == 'completed')} ä¸ª, å¤±è´¥ {sum(1 for r in results if r.get('status') == 'error')} ä¸ª")

        return results

    def delete_document(self, doc_id: str, delete_source: bool = False) -> Dict[str, Any]:
        """
        åˆ é™¤æ–‡æ¡£åŠå…¶æ‰€æœ‰å…³è”æ–‡ä»¶

        Args:
            doc_id: æ–‡æ¡£ID
            delete_source: æ˜¯å¦åˆ é™¤æºæ–‡ä»¶

        Returns:
            åˆ é™¤ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£: {doc_id}, åˆ é™¤æºæ–‡ä»¶: {delete_source}")

        result = self.doc_registry.delete_all_files(doc_id, delete_source=delete_source)

        if result["success"]:
            logger.info(f"âœ… æ–‡æ¡£åˆ é™¤æˆåŠŸ: åˆ é™¤ {len(result['deleted_files'])} ä¸ªæ–‡ä»¶")
        else:
            logger.error(f"âŒ æ–‡æ¡£åˆ é™¤éƒ¨åˆ†å¤±è´¥: æˆåŠŸ {len(result['deleted_files'])} ä¸ª, å¤±è´¥ {len(result['failed_files'])} ä¸ª")

        return result

    def list_documents(self, **filters) -> List[Dict]:
        """
        åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£

        Args:
            **filters: è¿‡æ»¤æ¡ä»¶ï¼ˆå¯é€‰ï¼‰
                - doc_type: æ–‡æ¡£ç±»å‹è¿‡æ»¤

        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        all_docs = self.doc_registry.list_all()

        # åº”ç”¨è¿‡æ»¤å™¨
        if "doc_type" in filters:
            all_docs = [d for d in all_docs if d.get("doc_type") == filters["doc_type"]]

        return all_docs

    def get_document_info(self, doc_id: str) -> Optional[Dict]:
        """
        è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯

        Args:
            doc_id: æ–‡æ¡£ID

        Returns:
            æ–‡æ¡£ä¿¡æ¯å­—å…¸
        """
        doc_info = self.doc_registry.get(doc_id)
        if doc_info:
            # æ·»åŠ æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
            file_stats = self.doc_registry.get_file_stats(doc_id)
            if file_stats:
                doc_info["file_stats"] = file_stats

        return doc_info


    def get_statistics(self) -> Dict:
        """
        è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return self.doc_registry.get_statistics()

    # ==================== WorkflowèŠ‚ç‚¹æ–¹æ³• ====================

    async def register_document(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤7ï¼šæ³¨å†Œåˆ°æ–‡æ¡£åº“
        """
        logger.info(f"ğŸ“‹ [Register] ========== æ­¥éª¤7: æ³¨å†Œæ–‡æ¡£ ==========")
        logger.info(f"ğŸ“‹ [Register] æ–‡æ¡£åç§°: {state['doc_name']}")

        doc_name = state["doc_name"]

        try:
            # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œæ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            # register æ–¹æ³•ä¼šå¤„ç†æ›´æ–°å·²å­˜åœ¨è®°å½•çš„é€»è¾‘
            # è·å–ç”Ÿæˆçš„æ–‡ä»¶ä¿¡æ¯
            generated_files = state.get("generated_files", {
                "images": [],
                "json_data": "",
                "vector_db": "",
                "summaries": []
            })

            # æ³¨å†Œæ–‡æ¡£
            doc_id = self.doc_registry.register(
                doc_name=state["doc_name"],
                doc_path=state["doc_path"],
                doc_type=state["doc_type"],
                index_path=state.get("index_path", ""),
                brief_summary=state.get("brief_summary", ""),
                metadata={},
                generated_files=generated_files
            )

            logger.info(f"âœ… [Register] æ–‡æ¡£æ³¨å†Œå®Œæˆ: {doc_id}")
            logger.info(f"ğŸ“ [Register] å…³è”æ–‡ä»¶ç»Ÿè®¡:")
            logger.info(f"  - å›¾ç‰‡: {len(generated_files.get('images', []))} ä¸ª")
            logger.info(f"  - JSON: {1 if generated_files.get('json_data') else 0} ä¸ª")
            logger.info(f"  - å‘é‡DB: {1 if generated_files.get('vector_db') else 0} ä¸ª")
            logger.info(f"  - æ‘˜è¦: {len(generated_files.get('summaries', []))} ä¸ª")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["doc_id"] = doc_id
            state["status"] = "completed"
            state["is_complete"] = True  # âœ… è®¾ç½®å®Œæˆæ ‡å¿—

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ (æ³¨å†Œé˜¶æ®µå®Œæˆå°±æ„å‘³ç€æ•´ä¸ªæµç¨‹å®Œæˆ)
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="register",
                status="completed",
                output_files=[]  # æ³¨å†Œæœ¬èº«ä¸ç”Ÿæˆæ–‡ä»¶
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [Register] æ–‡æ¡£æ³¨å†Œå¤±è´¥: {e}")
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="register",
                status="failed",
                output_files=[]
            )

            return state
