"""
Indexing Agent - æ–‡æ¡£ç´¢å¼•æ„å»ºAgent

è´Ÿè´£æ–‡æ¡£çš„è§£æã€æ‘˜è¦ç”Ÿæˆã€æ ‡ç­¾åˆ†ç±»ã€å‘é‡ç´¢å¼•æ„å»ºå’Œæ–‡æ¡£æ³¨å†Œ
"""

from langgraph.graph import StateGraph, END
from typing import Dict, List, Any, Optional
import logging

from ..base import AgentBase
from .state import IndexingState
from .doc_registry import DocumentRegistry
from .tools import IndexingTools
from .nodes import IndexingNodes
from .utils import IndexingUtils

logger = logging.getLogger(__name__)


class IndexingAgent(AgentBase):
    """
    ç´¢å¼•æ„å»ºAgent

    å·¥ä½œæµç¨‹ï¼š
    check_cache â†’ parse â†’ extract_structure â†’ chunk â†’
    process_chapters â†’ build_index â†’ generate_brief_summary â†’ register

    èŒè´£ï¼š
    - è§£æPDFæ–‡æ¡£å¹¶æå–æ–‡æœ¬
    - æå–æ–‡æ¡£ç›®å½•ç»“æ„
    - ç”Ÿæˆç« èŠ‚æ‘˜è¦å’Œé‡æ„å†…å®¹
    - æ„å»ºå‘é‡ç´¢å¼•
    - æ³¨å†Œæ–‡æ¡£åˆ°æ–‡æ¡£åº“
    """

    def __init__(self, provider: str = "openai", pdf_preset: str = "high"):
        """
        åˆå§‹åŒ– IndexingAgent

        Args:
            provider: LLMæä¾›å•† ('azure', 'openai', 'ollama')
            pdf_preset: PDFè½¬å›¾ç‰‡è´¨é‡é¢„è®¾ ('fast', 'balanced', 'high', 'ultra')
        """
        # åˆå§‹åŒ–åŸºç±»ï¼ˆä¼šåˆå§‹åŒ– self.llm å’Œ self.embedding_modelï¼‰
        super().__init__(name="IndexingAgent", provider=provider)

        self.doc_registry = DocumentRegistry()

        # åˆå§‹åŒ–åŠŸèƒ½æ¨¡å—ï¼ˆä½¿ç”¨ä¾èµ–æ³¨å…¥ï¼‰
        self.utils = IndexingUtils(self)
        self.tools = IndexingTools(self)
        self.nodes = IndexingNodes(self)

        # PDF å¤„ç†ç›¸å…³é…ç½®
        self._setup_pdf_config(pdf_preset)

        # æ„å»ºworkflow
        self.graph = self.build_graph()

    def _setup_pdf_config(self, pdf_preset: str):
        """
        é…ç½®PDFè½¬å›¾ç‰‡å‚æ•°

        Args:
            pdf_preset: è´¨é‡é¢„è®¾åç§°
        """
        from src.config.settings import (
            PDF_IMAGE_PATH,
            PDF_PATH,
            JSON_DATA_PATH,
            PDF_IMAGE_CONFIG,
        )
        from src.config.constants import ReaderConstants
        from src.utils.helpers import makedir

        self.pdf_image_path = PDF_IMAGE_PATH
        self.pdf_path = PDF_PATH
        self.json_data_path = JSON_DATA_PATH
        self.chunk_count = ReaderConstants.DEFAULT_CHUNK_COUNT

        # é…ç½® PDF è½¬å›¾ç‰‡å‚æ•°
        try:
            if pdf_preset in PDF_IMAGE_CONFIG.get("presets", {}):
                preset_config = PDF_IMAGE_CONFIG["presets"][pdf_preset]
                self.pdf_dpi = preset_config.get("dpi", PDF_IMAGE_CONFIG.get("dpi", 300))
                self.pdf_quality = pdf_preset
                logger.info(f"ä½¿ç”¨PDFè½¬å›¾ç‰‡é¢„è®¾'{pdf_preset}': DPI={self.pdf_dpi}, è´¨é‡çº§åˆ«={self.pdf_quality}")
            else:
                self.pdf_dpi = PDF_IMAGE_CONFIG.get("dpi", 300)
                self.pdf_quality = PDF_IMAGE_CONFIG.get("quality", "high")
                logger.info(f"ä½¿ç”¨é»˜è®¤PDFè½¬å›¾ç‰‡é…ç½®: DPI={self.pdf_dpi}, è´¨é‡={self.pdf_quality}")
        except Exception as e:
            logger.warning(f"PDFå›¾ç‰‡é…ç½®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            self.pdf_dpi = 300
            self.pdf_quality = "high"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        for path in [self.pdf_image_path, self.pdf_path, self.json_data_path]:
            makedir(path)

    # ==================== Graphæ„å»º ====================

    def build_graph(self) -> StateGraph:
        """
        æ„å»ºLangGraph workflow

        å·¥ä½œæµç¨‹ï¼š
        1. check_cache - æ£€æŸ¥æ‰€æœ‰é˜¶æ®µçš„æ–‡ä»¶ï¼Œè®¾ç½®æ¯ä¸ªé˜¶æ®µçš„è·³è¿‡æ ‡å¿—
        2. parse - è§£ææ–‡æ¡£ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        3. extract_structure - æå–ç›®å½•ç»“æ„ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        4. chunk - æ„å»ºç« èŠ‚æ•°æ®åˆ—è¡¨ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        5. process_chapters - å¹¶è¡Œå¤„ç†ç« èŠ‚ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        6. build_index - æ„å»ºå‘é‡æ•°æ®åº“ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        7. generate_brief_summary - ç”Ÿæˆç®€è¦æ‘˜è¦ï¼ˆæ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡ï¼‰
        8. register - æ³¨å†Œæ–‡æ¡£
        """
        workflow = StateGraph(IndexingState)

        # æ·»åŠ èŠ‚ç‚¹ï¼ˆå§”æ‰˜ç»™ nodes æ¨¡å—ï¼‰
        workflow.add_node("check_cache", self.nodes.check_cache)
        workflow.add_node("parse", self.nodes.parse_document)
        workflow.add_node("extract_structure", self.nodes.extract_structure)
        workflow.add_node("chunk", self.nodes.chunk_text)
        workflow.add_node("process_chapters", self.nodes.process_chapters)
        workflow.add_node("build_index", self.nodes.build_index)
        workflow.add_node("generate_brief_summary", self.nodes.generate_brief_summary)
        workflow.add_node("register", self.nodes.register_document)

        # æ·»åŠ è¾¹ - çº¿æ€§æµç¨‹ï¼Œæ¯ä¸ªèŠ‚ç‚¹å†…éƒ¨æ ¹æ®æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡
        workflow.add_edge("check_cache", "parse")
        workflow.add_edge("parse", "extract_structure")
        workflow.add_edge("extract_structure", "chunk")
        workflow.add_edge("chunk", "process_chapters")
        workflow.add_edge("process_chapters", "build_index")
        workflow.add_edge("build_index", "generate_brief_summary")
        workflow.add_edge("generate_brief_summary", "register")
        workflow.add_edge("register", END)

        # è®¾ç½®å…¥å£
        workflow.set_entry_point("check_cache")

        return workflow.compile()

    # ==================== å¯¹å¤–æ¥å£æ–¹æ³• ====================

    async def process_documents_batch(
        self,
        doc_list: List[Dict[str, Any]],
        max_concurrent: int = 3
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¤„ç†æ–‡æ¡£åˆ—è¡¨

        Args:
            doc_list: æ–‡æ¡£åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ ¼å¼ï¼š
                {
                    "doc_name": str,
                    "doc_path": str,
                    "doc_type": "pdf" | "url"
                }
            max_concurrent: æœ€å¤§å¹¶å‘å¤„ç†æ•°

        Returns:
            å¤„ç†ç»“æœåˆ—è¡¨
        """
        return await self.tools.process_documents_batch(doc_list, max_concurrent)

    def delete_document(self, doc_id: str, delete_source: bool = False) -> Dict[str, Any]:
        """
        åˆ é™¤æ–‡æ¡£åŠå…¶æ‰€æœ‰å…³è”æ–‡ä»¶

        Args:
            doc_id: æ–‡æ¡£ID
            delete_source: æ˜¯å¦åˆ é™¤æºæ–‡ä»¶

        Returns:
            åˆ é™¤ç»“æœå­—å…¸
        """
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ–‡æ¡£: {doc_id}, åˆ é™¤æºæ–‡ä»¶: {delete_source}")

        result = self.doc_registry.delete_all_files(doc_id, delete_source=delete_source)

        if result["success"]:
            logger.info(f"âœ… æ–‡æ¡£åˆ é™¤æˆåŠŸ: åˆ é™¤ {len(result['deleted_files'])} ä¸ªæ–‡ä»¶")
        else:
            logger.error(f"âŒ æ–‡æ¡£åˆ é™¤éƒ¨åˆ†å¤±è´¥: æˆåŠŸ {len(result['deleted_files'])} ä¸ª, å¤±è´¥ {len(result['failed_files'])} ä¸ª")

        return result

    def list_documents(self, **filters) -> List[Dict]:
        """
        åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£

        Args:
            **filters: è¿‡æ»¤æ¡ä»¶ï¼ˆå¯é€‰ï¼‰
                - doc_type: æ–‡æ¡£ç±»å‹è¿‡æ»¤

        Returns:
            æ–‡æ¡£åˆ—è¡¨
        """
        all_docs = self.doc_registry.list_all()

        # åº”ç”¨è¿‡æ»¤å™¨
        if "doc_type" in filters:
            all_docs = [d for d in all_docs if d.get("doc_type") == filters["doc_type"]]

        return all_docs

    def get_document_info(self, doc_id: str) -> Optional[Dict]:
        """
        è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯

        Args:
            doc_id: æ–‡æ¡£ID

        Returns:
            æ–‡æ¡£ä¿¡æ¯å­—å…¸
        """
        doc_info = self.doc_registry.get(doc_id)
        if doc_info:
            # æ·»åŠ æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
            file_stats = self.doc_registry.get_file_stats(doc_id)
            if file_stats:
                doc_info["file_stats"] = file_stats

        return doc_info

    def get_statistics(self) -> Dict:
        """
        è·å–æ–‡æ¡£ç»Ÿè®¡ä¿¡æ¯

        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        return self.doc_registry.get_statistics()

    async def rebuild_from_structure(
        self,
        doc_name: str,
        doc_path: str
    ) -> Dict[str, Any]:
        """
        åŸºäºå·²æœ‰çš„ structure.json é‡å»ºæ–‡æ¡£æ•°æ®

        ä¿æŒä¸å˜çš„æ–‡ä»¶ï¼š
        - structure.json: æ‰‹åŠ¨ç¼–è¾‘çš„ç»“æ„
        - data.json: PDF åŸå§‹æ•°æ®
        - pdf_image/: PDF å›¾ç‰‡æ–‡ä»¶

        é‡æ–°ç”Ÿæˆçš„å†…å®¹ï¼š
        - chunks.json: åŸºäºæ–°ç»“æ„é‡å»ºç« èŠ‚æ•°æ®
        - ç« èŠ‚æ‘˜è¦: é‡æ–°ç”Ÿæˆæ‰€æœ‰ç« èŠ‚çš„æ‘˜è¦å’Œé‡æ„å†…å®¹
        - å‘é‡æ•°æ®åº“: å®Œå…¨é‡å»º FAISS ç´¢å¼•
        - ç®€è¦æ‘˜è¦: é‡æ–°ç”Ÿæˆæ•´ä½“æ–‡æ¡£æ‘˜è¦

        Args:
            doc_name: æ–‡æ¡£åç§°
            doc_path: æ–‡æ¡£è·¯å¾„

        Returns:
            é‡å»ºç»“æœå­—å…¸
        """
        return await self.tools.rebuild_from_structure(doc_name, doc_path)
