"""
IndexingAgent WorkflowèŠ‚ç‚¹æ–¹æ³•

æ‰€æœ‰workflowèŠ‚ç‚¹çš„å®ç°
"""

from __future__ import annotations
from typing import Dict, TYPE_CHECKING
import logging
import os
import json
from pathlib import Path

from .state import IndexingState

if TYPE_CHECKING:
    from .agent import IndexingAgent

logger = logging.getLogger(__name__)


class IndexingNodes:
    """IndexingAgent WorkflowèŠ‚ç‚¹æ–¹æ³•é›†åˆ"""

    def __init__(self, agent: 'IndexingAgent'):
        """
        Args:
            agent: IndexingAgentå®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.agent = agent

    async def check_cache(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤0ï¼šæ£€æŸ¥æ‰€æœ‰é˜¶æ®µçš„ç¼“å­˜æ–‡ä»¶

        æ£€æŸ¥æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œè®¾ç½®è·³è¿‡æ ‡å¿—ï¼Œå¹¶å°è¯•åŠ è½½å·²æœ‰æ•°æ®
        """
        logger.info(f"ğŸ” [CheckCache] ========== æ­¥éª¤0: æ£€æŸ¥æ‰€æœ‰ç¼“å­˜æ–‡ä»¶ ==========")
        logger.info(f"ğŸ” [CheckCache] æ–‡æ¡£åç§°: {state['doc_name']}")

        doc_name = state["doc_name"]
        doc_type = state.get("doc_type")

        # åˆå§‹åŒ–é˜¶æ®µçŠ¶æ€å­—å…¸
        stage_status = {
            "parse": {"skip": False, "files": []},
            "extract_structure": {"skip": False, "files": []},
            "chunk_text": {"skip": False, "files": []},
            "process_chapters": {"skip": False, "files": []},
            "build_index": {"skip": False, "files": []},
            "generate_summary": {"skip": False, "files": []},
        }

        # å®šä¹‰æ‰€æœ‰æ–‡ä»¶è·¯å¾„
        from src.config.settings import DATA_ROOT
        doc_json_folder = os.path.join(self.agent.json_data_path, doc_name)
        json_path = os.path.join(doc_json_folder, "data.json")
        structure_json_path = os.path.join(doc_json_folder, "structure.json")
        chunk_json_path = os.path.join(doc_json_folder, "chunks.json")
        image_folder = os.path.join(self.agent.pdf_image_path, doc_name)
        vector_db_path = Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index"
        summary_txt_path = os.path.join(DATA_ROOT, "output", f"{doc_name}_brief_summary.md")

        # æ£€æŸ¥æ¯ä¸ªé˜¶æ®µçš„æ–‡ä»¶
        logger.info(f"ğŸ” [CheckCache] å¼€å§‹æ£€æŸ¥å„é˜¶æ®µæ–‡ä»¶...")

        # 1. æ£€æŸ¥ parse é˜¶æ®µ
        if Path(json_path).exists():
            logger.info(f"âœ… [CheckCache] parse: JSONæ–‡ä»¶å­˜åœ¨")
            stage_status["parse"]["skip"] = True
            stage_status["parse"]["files"] = [image_folder, json_path] if Path(image_folder).exists() else [json_path]

            # å°è¯•åŠ è½½ PDF æ•°æ®
            try:
                with open(json_path, 'r', encoding='utf-8') as f:
                    pdf_data_list = json.load(f)
                state["pdf_data_list"] = pdf_data_list
                state["json_data_dict"] = {str(item.get("page", i+1)): item.get("data", "") for i, item in enumerate(pdf_data_list)}
                state["raw_data"] = "\n\n".join([f"[Page {item.get('page', i+1)}]\n{item.get('data', '')}" for i, item in enumerate(pdf_data_list)])
                logger.info(f"   ğŸ“¥ å·²åŠ è½½ PDF æ•°æ®: {len(pdf_data_list)} é¡µ")
            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] PDF æ•°æ®åŠ è½½å¤±è´¥: {e}")
                stage_status["parse"]["skip"] = False
        else:
            logger.info(f"âŒ [CheckCache] parse: JSONæ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")

        # 2. æ£€æŸ¥ extract_structure é˜¶æ®µ
        if Path(structure_json_path).exists():
            logger.info(f"âœ… [CheckCache] extract_structure: ç»“æ„æ–‡ä»¶å­˜åœ¨")
            stage_status["extract_structure"]["skip"] = True
            stage_status["extract_structure"]["files"] = [structure_json_path]

            try:
                with open(structure_json_path, 'r', encoding='utf-8') as f:
                    structure_data = json.load(f)

                if isinstance(structure_data, dict):
                    if "agenda_dict" in structure_data:
                        state["agenda_dict"] = structure_data.get("agenda_dict", {})
                        state["has_toc"] = structure_data.get("has_toc", False)
                    else:
                        state["agenda_dict"] = structure_data
                        state["has_toc"] = True

                    logger.info(f"   ğŸ“¥ å·²åŠ è½½ç»“æ„: {len(state['agenda_dict'])} ä¸ªç« èŠ‚, has_toc={state.get('has_toc')}")
                else:
                    logger.warning(f"âš ï¸  [CheckCache] ç»“æ„æ•°æ®æ ¼å¼é”™è¯¯ï¼ˆéå­—å…¸ç±»å‹ï¼‰")
                    stage_status["extract_structure"]["skip"] = False
            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] ç»“æ„æ•°æ®åŠ è½½å¤±è´¥: {e}")
                stage_status["extract_structure"]["skip"] = False
        else:
            logger.info(f"âŒ [CheckCache] extract_structure: ç»“æ„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")

        # 3. æ£€æŸ¥ chunk_text é˜¶æ®µ
        if Path(chunk_json_path).exists():
            logger.info(f"âœ… [CheckCache] chunk_text: ç« èŠ‚æ•°æ®æ–‡ä»¶å­˜åœ¨")
            stage_status["chunk_text"]["skip"] = True
            stage_status["chunk_text"]["files"] = [chunk_json_path]

            try:
                with open(chunk_json_path, 'r', encoding='utf-8') as f:
                    agenda_data_list = json.load(f)
                state["agenda_data_list"] = agenda_data_list
                logger.info(f"   ğŸ“¥ å·²åŠ è½½ç« èŠ‚æ•°æ®: {len(agenda_data_list)} ä¸ªç« èŠ‚")
            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] ç« èŠ‚æ•°æ®åŠ è½½å¤±è´¥: {e}")
                stage_status["chunk_text"]["skip"] = False
        else:
            logger.info(f"âŒ [CheckCache] chunk_text: ç« èŠ‚æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")

        # 4 & 5. æ£€æŸ¥ build_index é˜¶æ®µï¼ˆprocess_chapters ä¸ build_index ç»‘å®šï¼‰
        if vector_db_path.exists() and any(vector_db_path.iterdir()):
            logger.info(f"âœ… [CheckCache] build_index: Vector DBå­˜åœ¨")
            stage_status["build_index"]["skip"] = True
            stage_status["process_chapters"]["skip"] = True
            stage_status["build_index"]["files"] = [str(vector_db_path)]
            stage_status["process_chapters"]["files"] = [str(vector_db_path)]
            state["index_path"] = str(vector_db_path)

            # ä» Vector DB åŠ è½½ chapter_summaries æ•°æ®
            try:
                from src.core.vector_db.vector_db_client import VectorDBClient

                logger.info(f"   ğŸ“¥ æ­£åœ¨ä» Vector DB åŠ è½½ç« èŠ‚æ‘˜è¦æ•°æ®...")

                vector_db_client = VectorDBClient(str(vector_db_path), embedding_model=self.agent.embedding_model)

                chapter_summaries = {}
                chapter_refactors = {}
                raw_data_dict = {}

                if vector_db_client.vector_db and vector_db_client.vector_db.docstore:
                    for doc_id, doc in vector_db_client.vector_db.docstore._dict.items():
                        metadata = doc.metadata
                        doc_type = metadata.get("type")

                        if doc_type == "context":
                            title = metadata.get("title", "")
                            if title:
                                chapter_summaries[title] = doc.page_content
                                chapter_refactors[title] = metadata.get("refactor", "")
                                raw_data_dict[title] = metadata.get("raw_data", {})

                    state["chapter_summaries"] = chapter_summaries
                    state["chapter_refactors"] = chapter_refactors
                    state["raw_data_dict"] = raw_data_dict

                    logger.info(f"   ğŸ“¥ å·²ä» Vector DB åŠ è½½: {len(chapter_summaries)} ä¸ªç« èŠ‚æ‘˜è¦")
                    logger.info(f"   â­ï¸  process_chapters å’Œ build_index éƒ½å°†è·³è¿‡")
                else:
                    logger.warning(f"âš ï¸  [CheckCache] Vector DB åŠ è½½åä¸ºç©º")
                    stage_status["build_index"]["skip"] = False
                    stage_status["process_chapters"]["skip"] = False

            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] ä» Vector DB åŠ è½½æ•°æ®å¤±è´¥: {e}")
                import traceback
                logger.debug(traceback.format_exc())
                stage_status["build_index"]["skip"] = False
                stage_status["process_chapters"]["skip"] = False
                logger.info(f"   âŒ éœ€è¦é‡æ–°æ‰§è¡Œ process_chapters å’Œ build_index")
        else:
            logger.info(f"âŒ [CheckCache] build_index: Vector DBä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")
            logger.info(f"âŒ [CheckCache] process_chapters: éœ€è¦æ‰§è¡Œ")

        # 6. æ£€æŸ¥ generate_summary é˜¶æ®µ
        if Path(summary_txt_path).exists():
            logger.info(f"âœ… [CheckCache] generate_summary: æ‘˜è¦æ–‡ä»¶å­˜åœ¨")
            stage_status["generate_summary"]["skip"] = True
            stage_status["generate_summary"]["files"] = [summary_txt_path]

            try:
                with open(summary_txt_path, 'r', encoding='utf-8') as f:
                    brief_summary = f.read()
                state["brief_summary"] = brief_summary
                logger.info(f"   ğŸ“¥ å·²åŠ è½½æ‘˜è¦: {len(brief_summary)} å­—ç¬¦")
            except Exception as e:
                logger.warning(f"âš ï¸  [CheckCache] æ‘˜è¦åŠ è½½å¤±è´¥: {e}")
                stage_status["generate_summary"]["skip"] = False
        else:
            logger.info(f"âŒ [CheckCache] generate_summary: æ‘˜è¦æ–‡ä»¶ä¸å­˜åœ¨ï¼Œéœ€è¦æ‰§è¡Œ")

        # ä¿å­˜é˜¶æ®µçŠ¶æ€åˆ° state
        state["stage_status"] = stage_status

        # ç»Ÿè®¡ä¿¡æ¯
        skip_count = sum(1 for s in stage_status.values() if s["skip"])
        total_count = len(stage_status)
        logger.info(f"\nğŸ” [CheckCache] æ£€æŸ¥å®Œæˆ: {skip_count}/{total_count} ä¸ªé˜¶æ®µå¯è·³è¿‡")

        # è¯¦ç»†è¾“å‡ºæ¯ä¸ªé˜¶æ®µçš„çŠ¶æ€
        for stage_name, status_info in stage_status.items():
            skip_status = "âœ… è·³è¿‡" if status_info["skip"] else "âŒ æ‰§è¡Œ"
            logger.debug(f"   {stage_name}: {skip_status}")

        # æ›´æ–° registry çŠ¶æ€ï¼ˆåŒæ­¥å·²æœ‰æ–‡ä»¶ï¼‰
        for stage_name, status_info in stage_status.items():
            if status_info["skip"]:
                self.agent.doc_registry.update_stage_status(
                    doc_name=doc_name,
                    stage_name=stage_name,
                    status="completed",
                    output_files=status_info["files"]
                )

        return state

    async def parse_document(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤1ï¼šè§£ææ–‡æ¡£å†…å®¹

        æ ¹æ® check_cache è®¾ç½®çš„æ ‡å¿—å†³å®šæ˜¯å¦è·³è¿‡
        """
        logger.info(f"ğŸ“„ [Parse] ========== æ­¥éª¤1: è§£ææ–‡æ¡£ ==========")
        logger.info(f"ğŸ“„ [Parse] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("parse", {}).get("skip"):
            logger.info(f"â­ï¸  [Parse] å·²æœ‰ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡è§£æ")
            state["status"] = "parsed"
            return state

        logger.info(f"ğŸ“„ [Parse] å¼€å§‹è§£ææ–‡æ¡£...")

        doc_name = state["doc_name"]
        doc_type = state.get("doc_type")

        try:
            doc_path = state["doc_path"]

            # åˆå§‹åŒ– generated_filesï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if "generated_files" not in state:
                state["generated_files"] = {
                    "images": [],
                    "json_data": "",
                    "vector_db": "",
                    "summaries": []
                }

            if doc_type == "pdf":
                logger.info(f"ğŸ“„ [Parse] ä½¿ç”¨PDFæå–å™¨å¤„ç†: {doc_path}")

                pdf_file_name = doc_name
                logger.info(f"ğŸ“„ [Parse] PDFæ–‡ä»¶åï¼ˆæ— æ‰©å±•åï¼‰: {pdf_file_name}")

                # æå–PDFæ•°æ®ï¼ˆè°ƒç”¨å·¥å…·æ–¹æ³•ï¼‰
                logger.info(f"ğŸ“„ [Parse] å¼€å§‹è°ƒç”¨ extract_pdf_data_impl...")
                extract_result = await self.agent.tools.extract_pdf_data_impl(pdf_file_name)
                logger.info(f"ğŸ“„ [Parse] PDFæ•°æ®æå–å®Œæˆ")

                pdf_data_list = extract_result["pdf_data_list"]
                if not pdf_data_list:
                    raise ValueError(f"PDFæå–å¤±è´¥ï¼Œæœªè·å–ä»»ä½•æ•°æ®: {doc_path}")

                # å°†æå–çš„æ•°æ®è½¬æ¢ä¸ºåŸå§‹æ–‡æœ¬
                raw_data = "\n\n".join([
                    f"[Page {item.get('page', i+1)}]\n{item.get('data', '')}"
                    for i, item in enumerate(pdf_data_list)
                ])

                # åˆ›å»º json_data_dictï¼ˆä»¥é¡µç ä¸ºkeyï¼‰
                json_data_dict = {
                    str(item.get("page", i+1)): item.get("data", "")
                    for i, item in enumerate(pdf_data_list)
                }

                # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
                state["raw_data"] = raw_data
                state["pdf_data_list"] = pdf_data_list
                state["json_data_dict"] = json_data_dict
                state["generated_files"]["images"] = extract_result["image_paths"]
                state["generated_files"]["json_data"] = extract_result["json_path"]
                state["status"] = "parsed"

                logger.info(f"âœ… [Parse] PDFè§£æå®Œæˆï¼Œæå– {len(pdf_data_list)} é¡µï¼Œæ€»é•¿åº¦: {len(raw_data)} å­—ç¬¦")
                logger.info(f"ğŸ“ [Parse] ç”Ÿæˆæ–‡ä»¶: å›¾ç‰‡{len(state['generated_files']['images'])}ä¸ª, JSON: {state['generated_files']['json_data']}")

                # æ›´æ–°é˜¶æ®µçŠ¶æ€
                image_folder = extract_result.get("image_folder", "")
                json_path = extract_result.get("json_path", "")
                output_files = []
                if image_folder:
                    output_files.append(image_folder)
                if json_path:
                    output_files.append(json_path)

                self.agent.doc_registry.update_stage_status(
                    doc_name=doc_name,
                    stage_name="parse",
                    status="completed",
                    output_files=output_files
                )

            elif doc_type == "url":
                # TODO: ä½¿ç”¨WebReaderæå–å†…å®¹
                logger.warning("URLç±»å‹æ–‡æ¡£æš‚æœªå®ç°ï¼Œä½¿ç”¨å ä½ç¬¦")
                state["raw_data"] = f"Web content from {doc_path}"
                state["status"] = "parsed"

            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡æ¡£ç±»å‹: {doc_type}")

            return state

        except Exception as e:
            logger.error(f"âŒ [Parse] è§£æå¤±è´¥: {e}")
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="parse",
                status="failed",
                output_files=[]
            )

            return state

    async def extract_structure(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤2ï¼šæå–æ–‡æ¡£ç›®å½•ç»“æ„

        ç­–ç•¥ï¼š
        1. å…ˆå°è¯•ä»å‰ 5-10 é¡µæå–ç›®å½•ï¼ˆå¿«é€Ÿï¼‰
        2. å¦‚æœæ²¡æ‰¾åˆ°ï¼Œåˆ†æå…¨æ–‡ç»“æ„ï¼ˆæ…¢ä½†å…¨é¢ï¼‰
        """
        logger.info(f"ğŸ“š [ExtractStructure] ========== æ­¥éª¤2: æå–æ–‡æ¡£ç»“æ„ ==========")
        logger.info(f"ğŸ“š [ExtractStructure] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        extract_status = stage_status.get("extract_structure", {})
        should_skip = extract_status.get("skip", False)
        if should_skip:
            logger.info(f"â­ï¸  [ExtractStructure] å·²æœ‰ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡ç»“æ„æå–")
            logger.info(f"ğŸ“š [ExtractStructure] å·²æœ‰ {len(state.get('agenda_dict', {}))} ä¸ªç« èŠ‚")
            return state

        logger.info(f"ğŸ“š [ExtractStructure] å¼€å§‹æå–æ–‡æ¡£ç»“æ„...")

        doc_name = state["doc_name"]
        doc_type = state.get("doc_type")

        # ä»…PDFç±»å‹éœ€è¦æå–ç»“æ„
        if doc_type != "pdf":
            logger.info("éPDFæ–‡æ¡£ï¼Œè·³è¿‡ç»“æ„æå–")
            state["has_toc"] = False
            state["agenda_dict"] = {}
            return state

        # å®šä¹‰ç»“æ„æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨æ–‡æ¡£æ–‡ä»¶å¤¹ï¼‰
        doc_json_folder = os.path.join(self.agent.json_data_path, doc_name)
        structure_json_path = os.path.join(doc_json_folder, "structure.json")

        try:
            pdf_data_list = state.get("pdf_data_list", [])
            if not pdf_data_list:
                logger.warning("PDFæ•°æ®ä¸ºç©ºï¼Œæ— æ³•æå–ç»“æ„")
                state["has_toc"] = False
                state["agenda_dict"] = {}
                return state

            # ç­–ç•¥1ï¼šå°è¯•ä»å‰å‡ é¡µå¿«é€Ÿæå–ç›®å½•
            logger.info("ğŸš€ [ExtractStructure] ç­–ç•¥1: å°è¯•ä»å‰10é¡µæå–ç›®å½•")
            agenda_dict, has_toc = await self.agent.tools.extract_toc_from_pages_impl(
                pdf_data_list,
                max_pages=10
            )

            if has_toc and agenda_dict:
                # æˆåŠŸæ‰¾åˆ°ç›®å½•
                logger.info(f"âœ… [ExtractStructure] æ£€æµ‹åˆ°ç›®å½•ç»“æ„: {len(agenda_dict)} ä¸ªç« èŠ‚")
                state["agenda_dict"] = agenda_dict
                state["has_toc"] = True
            else:
                # ç­–ç•¥2ï¼šæ²¡æ‰¾åˆ°ç›®å½•ï¼Œåˆ†æå…¨æ–‡ç»“æ„
                logger.info("ğŸ” [ExtractStructure] ç­–ç•¥2: åˆ†æå…¨æ–‡ç»“æ„")
                agenda_dict = await self.agent.tools.analyze_full_structure_impl(pdf_data_list)

                state["agenda_dict"] = agenda_dict
                state["has_toc"] = False
                logger.info(f"âœ… [ExtractStructure] å…¨æ–‡åˆ†æå®Œæˆ: {len(agenda_dict)} ä¸ªç« èŠ‚")

            # æ‰“å°ç›®å½•ä¿¡æ¯
            logger.info("ğŸ“‘ [ExtractStructure] æ–‡æ¡£ç›®å½•ç»“æ„:")
            for title, pages in list(state["agenda_dict"].items())[:5]:
                logger.info(f"  - {title}: ç¬¬ {pages[0]}-{pages[-1]} é¡µ")
            if len(state["agenda_dict"]) > 5:
                logger.info(f"  ... è¿˜æœ‰ {len(state['agenda_dict']) - 5} ä¸ªç« èŠ‚")

            # ä¿å­˜ç»“æ„æ•°æ®åˆ°æ–‡ä»¶
            structure_data = {
                "agenda_dict": state["agenda_dict"],
                "has_toc": state["has_toc"]
            }
            try:
                os.makedirs(os.path.dirname(structure_json_path), exist_ok=True)
                with open(structure_json_path, 'w', encoding='utf-8') as f:
                    json.dump(structure_data, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ [ExtractStructure] ç»“æ„æ•°æ®å·²ä¿å­˜: {structure_json_path}")
            except Exception as e:
                logger.warning(f"âš ï¸  [ExtractStructure] ä¿å­˜ç»“æ„æ•°æ®å¤±è´¥: {e}")

            # æ›´æ–°é˜¶æ®µçŠ¶æ€
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="extract_structure",
                status="completed",
                output_files=[structure_json_path]
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [ExtractStructure] ç»“æ„æå–å¤±è´¥: {e}")
            # å¤±è´¥æ—¶è®¾ç½®é»˜è®¤å€¼
            state["has_toc"] = False
            state["agenda_dict"] = {}

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="extract_structure",
                status="failed",
                output_files=[]
            )

            return state

    async def chunk_text(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤3ï¼šæ„å»ºç« èŠ‚æ•°æ®åˆ—è¡¨

        ç›´æ¥åŸºäº extract_structure å¾—åˆ°çš„ agenda_dict æ„å»º agenda_data_list
        """
        logger.info(f"ğŸ“¦ [Chunk] ========== æ­¥éª¤3: æ„å»ºç« èŠ‚æ•°æ®åˆ—è¡¨ ==========")
        logger.info(f"ğŸ“¦ [Chunk] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("chunk_text", {}).get("skip"):
            logger.info(f"â­ï¸  [Chunk] å·²æœ‰ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡ç« èŠ‚æ•°æ®æ„å»º")
            logger.info(f"ğŸ“¦ [Chunk] å·²æœ‰ {len(state.get('agenda_data_list', []))} ä¸ªç« èŠ‚")
            return state

        logger.info(f"ğŸ“¦ [Chunk] å¼€å§‹æ„å»ºç« èŠ‚æ•°æ®...")

        doc_name = state["doc_name"]

        # å®šä¹‰ç« èŠ‚æ•°æ®æ–‡ä»¶è·¯å¾„ï¼ˆä½¿ç”¨æ–‡æ¡£æ–‡ä»¶å¤¹ï¼‰
        doc_json_folder = os.path.join(self.agent.json_data_path, doc_name)
        chunk_json_path = os.path.join(doc_json_folder, "chunks.json")

        try:
            agenda_dict = state.get("agenda_dict", {})
            json_data_dict = state.get("json_data_dict", {})

            if not agenda_dict:
                logger.warning("agenda_dict ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç« èŠ‚æ•°æ®")
                state["agenda_data_list"] = []
                state["status"] = "chunked"
                return state

            if not json_data_dict:
                logger.warning("json_data_dict ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç« èŠ‚æ•°æ®")
                state["agenda_data_list"] = []
                state["status"] = "chunked"
                return state

            # ç›´æ¥åŸºäº agenda_dict æ„å»º agenda_data_list
            agenda_data_list = []

            for title, page_numbers in agenda_dict.items():
                # æ”¶é›†è¯¥ç« èŠ‚çš„æ‰€æœ‰é¡µé¢æ•°æ®
                chapter_data = {}

                for page_num in page_numbers:
                    page_key = str(page_num)
                    if page_key in json_data_dict:
                        chapter_data[page_key] = json_data_dict[page_key]
                    else:
                        logger.warning(f"é¡µç  {page_key} ä¸åœ¨ json_data_dict ä¸­")

                if chapter_data:
                    agenda_data_list.append({
                        "title": title,
                        "data": chapter_data,
                        "pages": page_numbers
                    })
                else:
                    logger.warning(f"ç« èŠ‚ '{title}' æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„æ•°æ®")

            logger.info(f"âœ… [Chunk] ç« èŠ‚æ•°æ®æ„å»ºå®Œæˆ: {len(agenda_data_list)} ä¸ªç« èŠ‚")

            # æ‰“å°ç« èŠ‚ä¿¡æ¯
            for item in agenda_data_list:
                title = item.get("title", "æœªçŸ¥")
                pages = item.get("pages", [])
                data_pages = len(item.get("data", {}))
                logger.info(f"  - {title}: {len(pages)} é¡µ (å®é™…æ•°æ®: {data_pages} é¡µ)")

            # ä¿å­˜ç« èŠ‚æ•°æ®åˆ°æ–‡ä»¶
            try:
                os.makedirs(os.path.dirname(chunk_json_path), exist_ok=True)
                with open(chunk_json_path, 'w', encoding='utf-8') as f:
                    json.dump(agenda_data_list, f, ensure_ascii=False, indent=2)
                logger.info(f"ğŸ’¾ [Chunk] ç« èŠ‚æ•°æ®å·²ä¿å­˜: {chunk_json_path}")
            except Exception as e:
                logger.warning(f"âš ï¸  [Chunk] ä¿å­˜ç« èŠ‚æ•°æ®å¤±è´¥: {e}")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["agenda_data_list"] = agenda_data_list
            state["status"] = "chunked"

            # æ›´æ–°é˜¶æ®µçŠ¶æ€
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="chunk_text",
                status="completed",
                output_files=[chunk_json_path]
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [Chunk] ç« èŠ‚æ•°æ®æ„å»ºå¤±è´¥: {e}")
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="chunk_text",
                status="failed",
                output_files=[]
            )

            return state

    async def process_chapters(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤4ï¼šå¤„ç†ç« èŠ‚ï¼ˆå¹¶è¡Œç”Ÿæˆæ‘˜è¦å’Œé‡æ„å†…å®¹ï¼‰

        å¯¹æ¯ä¸ªç« èŠ‚ï¼š
        1. ç”Ÿæˆæ‘˜è¦ï¼ˆsummaryï¼‰
        2. é‡æ„å†…å®¹ï¼ˆrefactorï¼‰
        """
        logger.info(f"ğŸ“ [ProcessChapters] ========== æ­¥éª¤4: å¤„ç†ç« èŠ‚ ==========")
        logger.info(f"ğŸ“ [ProcessChapters] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("process_chapters", {}).get("skip"):
            logger.info(f"â­ï¸  [ProcessChapters] å·²æœ‰ç¼“å­˜æ•°æ®ï¼Œè·³è¿‡ç« èŠ‚å¤„ç†")
            logger.info(f"ğŸ“ [ProcessChapters] å·²æœ‰ {len(state.get('chapter_summaries', {}))} ä¸ªç« èŠ‚æ‘˜è¦")
            return state

        logger.info(f"ğŸ“ [ProcessChapters] å¼€å§‹å¤„ç†ç« èŠ‚...")

        doc_name = state["doc_name"]

        try:
            from src.core.parallel import ChapterProcessor
            from .prompts import IndexingRole
            from src.agents.common.prompts import CommonRole

            agenda_data_list = state.get("agenda_data_list", [])
            logger.info(f"ğŸ“ [ProcessChapters] ç« èŠ‚æ•°é‡: {len(agenda_data_list)}")

            if not agenda_data_list:
                logger.warning("ğŸ“ [ProcessChapters] âš ï¸ agenda_data_list ä¸ºç©ºï¼Œè·³è¿‡ç« èŠ‚å¤„ç†")
                state["chapter_summaries"] = {}
                state["chapter_refactors"] = {}
                state["raw_data_dict"] = {}
                state["status"] = "summarized"
                return state

            # ä½¿ç”¨å¹¶è¡Œå¤„ç†å·¥å…·
            logger.info(f"å¼€å§‹å¹¶è¡Œå¤„ç† {len(agenda_data_list)} ä¸ªç« èŠ‚...")

            # ç›´æ¥ä½¿ç”¨å¼‚æ­¥æ–¹æ³•ï¼ˆå› ä¸ºå½“å‰å·²ç»åœ¨asyncä¸Šä¸‹æ–‡ä¸­ï¼‰
            processor = ChapterProcessor(self.agent.llm, max_concurrent=10)
            chapter_results = await processor.process_chapters_summary_and_refactor(
                agenda_data_list=agenda_data_list,
                summary_role=IndexingRole.CONTENT_SUMMARY,
                refactor_role=CommonRole.CONTENT_MERGE
            )

            # å¤„ç†ç»“æœ
            chapter_summaries = {}
            chapter_refactors = {}
            raw_data_dict = {}

            for title, summary, refactor_content, _, data in chapter_results:
                chapter_summaries[title] = summary
                chapter_refactors[title] = refactor_content
                raw_data_dict[title] = data

                logger.info(f"âœ… ç« èŠ‚å¤„ç†å®Œæˆ: {title}")

            logger.info(f"âœ… [ProcessChapters] æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆ: {len(chapter_summaries)} ä¸ªç« èŠ‚")
            logger.info(f"ğŸ“Œ [ProcessChapters] æ•°æ®å°†åœ¨ build_index é˜¶æ®µå­˜å…¥ Vector DB")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["chapter_summaries"] = chapter_summaries
            state["chapter_refactors"] = chapter_refactors
            state["raw_data_dict"] = raw_data_dict
            state["status"] = "summarized"

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ï¼ˆæ•°æ®å­˜å‚¨åœ¨ vector db ä¸­ï¼Œæ— å•ç‹¬æ–‡ä»¶ï¼‰
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="process_chapters",
                status="completed",
                output_files=[]  # æ•°æ®åœ¨ vector db ä¸­
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [ProcessChapters] ç« èŠ‚å¤„ç†å¤±è´¥: {e}")
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="process_chapters",
                status="failed",
                output_files=[]
            )

            return state

    async def build_index(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤5ï¼šæ„å»ºå‘é‡ç´¢å¼•

        åŸºäºç« èŠ‚æ‘˜è¦æ„å»º Document å¯¹è±¡ï¼š
        1. type="context": ç« èŠ‚æ‘˜è¦ä½œä¸ºæ£€ç´¢å†…å®¹
        2. type="title": ç« èŠ‚æ ‡é¢˜ä½œä¸ºæ£€ç´¢å†…å®¹
        3. type="structure": æ–‡æ¡£ç»“æ„ä¿¡æ¯
        """
        logger.info(f"ğŸ”¨ [BuildIndex] ========== æ­¥éª¤5: æ„å»ºå‘é‡ç´¢å¼• ==========")
        logger.info(f"ğŸ”¨ [BuildIndex] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("build_index", {}).get("skip"):
            logger.info(f"â­ï¸  [BuildIndex] å·²æœ‰ Vector DBï¼Œè·³è¿‡æ„å»º")
            logger.info(f"ğŸ”¨ [BuildIndex] Vector DB è·¯å¾„: {state.get('index_path')}")
            state["status"] = "indexed"
            return state

        logger.info(f"ğŸ”¨ [BuildIndex] å¼€å§‹æ„å»ºå‘é‡ç´¢å¼•...")

        doc_name = state["doc_name"]

        # æ„å»ºç´¢å¼•è·¯å¾„
        from src.config.settings import DATA_ROOT
        index_path = str(Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index")

        try:
            from langchain.docstore.document import Document
            from src.core.vector_db.vector_db_client import VectorDBClient

            chapter_summaries = state.get("chapter_summaries", {})
            chapter_refactors = state.get("chapter_refactors", {})
            raw_data_dict = state.get("raw_data_dict", {})
            agenda_dict = state.get("agenda_dict", {})

            if not chapter_summaries:
                logger.warning("ç« èŠ‚æ‘˜è¦ä¸ºç©ºï¼Œæ— æ³•æ„å»ºç´¢å¼•")
                state["status"] = "error"
                state["error"] = "ç« èŠ‚æ‘˜è¦ä¸ºç©º"
                return state

            # æ„å»º Document åˆ—è¡¨
            vector_db_docs = []

            logger.info("å¼€å§‹æ„å»º Document å¯¹è±¡...")

            # éå†æ¯ä¸ªç« èŠ‚
            for title, summary in chapter_summaries.items():
                refactor_content = chapter_refactors.get(title, "")
                raw_data = raw_data_dict.get(title, {})
                pages = agenda_dict.get(title, [])

                # Document 1: type="context" - æ‘˜è¦ä½œä¸ºæ£€ç´¢å†…å®¹
                vector_db_docs.append(
                    Document(
                        page_content=summary,
                        metadata={
                            "type": "context",
                            "title": title,
                            "pages": pages,
                            "raw_data": raw_data,
                            "refactor": refactor_content,
                        }
                    )
                )

                # Document 2: type="title" - æ ‡é¢˜ä½œä¸ºæ£€ç´¢å†…å®¹
                vector_db_docs.append(
                    Document(
                        page_content=title,
                        metadata={
                            "type": "title",
                            "pages": pages,
                            "summary": summary,
                            "raw_data": raw_data,
                            "refactor": refactor_content,
                        }
                    )
                )

            # Document 3: type="structure" - æ–‡æ¡£ç»“æ„ä¿¡æ¯
            structure_doc = Document(
                page_content="Document Structure",
                metadata={
                    "type": "structure",
                    "agenda_dict": agenda_dict,
                    "doc_name": doc_name,
                    "total_chapters": len(agenda_dict)
                }
            )
            vector_db_docs.append(structure_doc)

            logger.info(f"Document å¯¹è±¡æ„å»ºå®Œæˆ: {len(vector_db_docs)} ä¸ªæ–‡æ¡£")

            # æ„å»ºç´¢å¼•è·¯å¾„
            index_dir = Path(DATA_ROOT) / "vector_db" / f"{doc_name}_data_index"
            index_dir.mkdir(parents=True, exist_ok=True)
            index_path = str(index_dir)

            # åˆ›å»º VectorDBClient å¹¶æ„å»ºå‘é‡æ•°æ®åº“
            vector_db_client = VectorDBClient(index_path, embedding_model=self.agent.embedding_model)
            vector_db_client.build_vector_db(vector_db_docs)

            logger.info(f"âœ… [BuildIndex] å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ: {index_path}")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["index_path"] = index_path
            state["vector_db_docs"] = vector_db_docs
            if "generated_files" not in state:
                state["generated_files"] = {
                    "images": [],
                    "json_data": "",
                    "vector_db": "",
                    "summaries": []
                }
            state["generated_files"]["vector_db"] = index_path
            state["status"] = "indexed"

            # æ›´æ–°é˜¶æ®µçŠ¶æ€
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="build_index",
                status="completed",
                output_files=[index_path]
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [BuildIndex] ç´¢å¼•æ„å»ºå¤±è´¥: {e}")
            import traceback
            logger.error(traceback.format_exc())
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="build_index",
                status="failed",
                output_files=[]
            )
            return state

    async def generate_brief_summary(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤6ï¼šç”Ÿæˆç®€è¦æ‘˜è¦ï¼ˆåŸºäºæ‰€æœ‰ç« èŠ‚æ‘˜è¦ï¼‰

        è¿™æ˜¯æœ€åä¸€æ­¥æ‘˜è¦ç”Ÿæˆï¼Œæ•´åˆæ‰€æœ‰ç« èŠ‚çš„æ‘˜è¦
        """
        logger.info(f"ğŸ“ [BriefSummary] ========== æ­¥éª¤6: ç”Ÿæˆç®€è¦æ‘˜è¦ ==========")
        logger.info(f"ğŸ“ [BriefSummary] æ–‡æ¡£åç§°: {state['doc_name']}")

        # æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡
        stage_status = state.get("stage_status", {})
        if stage_status.get("generate_summary", {}).get("skip"):
            logger.info(f"â­ï¸  [BriefSummary] å·²æœ‰æ‘˜è¦æ–‡ä»¶ï¼Œè·³è¿‡ç”Ÿæˆ")
            logger.info(f"ğŸ“ [BriefSummary] æ‘˜è¦é•¿åº¦: {len(state.get('brief_summary', ''))} å­—ç¬¦")
            return state

        logger.info(f"ğŸ“ [BriefSummary] å¼€å§‹ç”Ÿæˆç®€è¦æ‘˜è¦...")

        doc_name = state["doc_name"]

        # å®šä¹‰æ‘˜è¦æ–‡ä»¶è·¯å¾„
        from src.config.settings import DATA_ROOT
        summary_txt_path = os.path.join(DATA_ROOT, "output", f"{doc_name}_brief_summary.md")

        try:
            chapter_summaries = state.get("chapter_summaries", {})
            logger.info(f"ğŸ“ [BriefSummary] ç« èŠ‚æ‘˜è¦æ•°é‡: {len(chapter_summaries)}")

            if not chapter_summaries:
                logger.warning("ç« èŠ‚æ‘˜è¦ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆç®€è¦æ‘˜è¦")
                state["brief_summary"] = ""
                return state

            # å¤ç”¨ generate_summary_implï¼Œä¼ å…¥ç« èŠ‚æ‘˜è¦
            answer = await self.agent.tools.generate_summary_impl(
                context_data=chapter_summaries,
                doc_name=doc_name,
                session_id="brief_summary"
            )

            logger.info(f"âœ… [BriefSummary] ç®€è¦æ‘˜è¦ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)} å­—ç¬¦")

            # ä¿å­˜æ‘˜è¦åˆ°æ–‡ä»¶
            try:
                os.makedirs(os.path.dirname(summary_txt_path), exist_ok=True)
                with open(summary_txt_path, 'w', encoding='utf-8') as f:
                    f.write(answer)
                logger.info(f"ğŸ’¾ [BriefSummary] ç®€è¦æ‘˜è¦å·²ä¿å­˜: {summary_txt_path}")

                # æ›´æ–° generated_files
                if "generated_files" not in state:
                    state["generated_files"] = {"images": [], "json_data": "", "vector_db": "", "summaries": []}
                if "summaries" not in state["generated_files"]:
                    state["generated_files"]["summaries"] = []
                state["generated_files"]["summaries"].append(summary_txt_path)

            except Exception as e:
                logger.warning(f"âš ï¸  [BriefSummary] ä¿å­˜ç®€è¦æ‘˜è¦å¤±è´¥: {e}")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["brief_summary"] = answer

            # æ›´æ–°é˜¶æ®µçŠ¶æ€
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="generate_summary",
                status="completed",
                output_files=[summary_txt_path]
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [BriefSummary] ç®€è¦æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
            state["brief_summary"] = f"æ–‡æ¡£ {state['doc_name']} çš„ç®€è¦æ‘˜è¦ï¼ˆç”Ÿæˆé”™è¯¯: {str(e)}ï¼‰"

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="generate_summary",
                status="failed",
                output_files=[]
            )

            return state

    async def register_document(self, state: IndexingState) -> IndexingState:
        """
        æ­¥éª¤7ï¼šæ³¨å†Œåˆ°æ–‡æ¡£åº“
        """
        logger.info(f"ğŸ“‹ [Register] ========== æ­¥éª¤7: æ³¨å†Œæ–‡æ¡£ ==========")
        logger.info(f"ğŸ“‹ [Register] æ–‡æ¡£åç§°: {state['doc_name']}")

        doc_name = state["doc_name"]

        try:
            # è·å–ç”Ÿæˆçš„æ–‡ä»¶ä¿¡æ¯
            generated_files = state.get("generated_files", {
                "images": [],
                "json_data": "",
                "vector_db": "",
                "summaries": []
            })

            # æ³¨å†Œæ–‡æ¡£
            doc_id = self.agent.doc_registry.register(
                doc_name=state["doc_name"],
                doc_path=state["doc_path"],
                doc_type=state["doc_type"],
                index_path=state.get("index_path", ""),
                brief_summary=state.get("brief_summary", ""),
                metadata={},
                generated_files=generated_files
            )

            logger.info(f"âœ… [Register] æ–‡æ¡£æ³¨å†Œå®Œæˆ: {doc_id}")
            logger.info(f"ğŸ“ [Register] å…³è”æ–‡ä»¶ç»Ÿè®¡:")
            logger.info(f"  - å›¾ç‰‡: {len(generated_files.get('images', []))} ä¸ª")
            logger.info(f"  - JSON: {1 if generated_files.get('json_data') else 0} ä¸ª")
            logger.info(f"  - å‘é‡DB: {1 if generated_files.get('vector_db') else 0} ä¸ª")
            logger.info(f"  - æ‘˜è¦: {len(generated_files.get('summaries', []))} ä¸ª")

            # ========== æå–å¹¶å­˜å‚¨å…ƒæ•°æ®ï¼ˆç”¨äºå¤šPDFæ£€ç´¢ï¼‰ ==========
            logger.info(f"")
            logger.info(f"ğŸ“‹ [Register] ========== æå–æ–‡æ¡£å…ƒæ•°æ® ==========")
            try:
                from .components import MetadataExtractor

                # æå–å…ƒæ•°æ®
                extractor = MetadataExtractor(self.agent.llm)
                metadata_enhanced = await extractor.extract_metadata(
                    doc_name=doc_name,
                    brief_summary=state.get("brief_summary", ""),
                    structure=state.get("agenda_dict", {})
                )

                # ä¿å­˜å…ƒæ•°æ®åˆ°doc_registryï¼ˆä½¿ç”¨å¹¶å‘å®‰å…¨çš„æ–¹æ³•ï¼‰
                success = self.agent.doc_registry.update_metadata(
                    doc_id=doc_id,
                    metadata_key="metadata_enhanced",
                    metadata_value=metadata_enhanced
                )
                if success:
                    logger.info(f"âœ… [Register] å…ƒæ•°æ®å·²ä¿å­˜åˆ°æ–‡æ¡£æ³¨å†Œè¡¨")
                else:
                    logger.warning(f"âš ï¸ [Register] å…ƒæ•°æ®ä¿å­˜å¤±è´¥ï¼Œæ–‡æ¡£IDä¸å­˜åœ¨: {doc_id}")

                # æ·»åŠ åˆ°å…ƒæ•°æ®å‘é‡æ•°æ®åº“
                from src.core.vector_db.metadata_db import MetadataVectorDB

                metadata_db = MetadataVectorDB()
                metadata_db.add_document(
                    doc_id=doc_id,
                    doc_name=doc_name,
                    embedding_summary=metadata_enhanced.get("embedding_summary", "")
                )

                logger.info(f"âœ… [Register] å…ƒæ•°æ®å·²æ·»åŠ åˆ°å‘é‡æ•°æ®åº“")

            except Exception as e:
                logger.error(f"âŒ [Register] å…ƒæ•°æ®æå–/å­˜å‚¨å¤±è´¥: {e}")
                logger.warning(f"âš ï¸  [Register] è·³è¿‡å…ƒæ•°æ®å¤„ç†ï¼Œç»§ç»­å®Œæˆæ³¨å†Œ")

            logger.info(f"")

            # ç›´æ¥åœ¨ state ä¸Šä¿®æ”¹
            state["doc_id"] = doc_id
            state["status"] = "completed"
            state["is_complete"] = True  # âœ… è®¾ç½®å®Œæˆæ ‡å¿—

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ (æ³¨å†Œé˜¶æ®µå®Œæˆå°±æ„å‘³ç€æ•´ä¸ªæµç¨‹å®Œæˆ)
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="register",
                status="completed",
                output_files=[]  # æ³¨å†Œæœ¬èº«ä¸ç”Ÿæˆæ–‡ä»¶
            )

            return state

        except Exception as e:
            logger.error(f"âŒ [Register] æ–‡æ¡£æ³¨å†Œå¤±è´¥: {e}")
            state["status"] = "error"
            state["error"] = str(e)

            # æ›´æ–°é˜¶æ®µçŠ¶æ€ä¸ºå¤±è´¥
            self.agent.doc_registry.update_stage_status(
                doc_name=doc_name,
                stage_name="register",
                status="failed",
                output_files=[]
            )

            return state
