"""
RetrievalAgent å·¥å…·æ–¹æ³•å®ç°

æ‰€æœ‰æ£€ç´¢ç›¸å…³çš„å·¥å…·æ–¹æ³•
"""

from typing import List, Dict, TYPE_CHECKING
import logging
import json

if TYPE_CHECKING:
    from .agent import RetrievalAgent

logger = logging.getLogger(__name__)


class RetrievalTools:
    """RetrievalAgent å·¥å…·æ–¹æ³•é›†åˆ"""

    def __init__(self, agent: 'RetrievalAgent'):
        """
        Args:
            agent: RetrievalAgentå®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.agent = agent

    async def search_by_context(self, query: str) -> Dict:
        """
        åŸºäºä¸Šä¸‹æ–‡çš„è¯­ä¹‰æ£€ç´¢æ–¹æ³•

        é€šè¿‡å‘é‡ç›¸ä¼¼åº¦æœç´¢åœ¨æ–‡æ¡£ä¸­æŸ¥æ‰¾ä¸æŸ¥è¯¢è¯­ä¹‰ç›¸å…³çš„å†…å®¹æ®µè½ã€‚

        Args:
            query: æœç´¢æŸ¥è¯¢å­—ç¬¦ä¸²ï¼Œåº”æè¿°è¦æŸ¥æ‰¾çš„å†…å®¹è¯­ä¹‰

        Returns:
            æ ‡å‡†æ ¼å¼çš„å·¥å…·å“åº”ï¼š
            {
                "type": "content",
                "tool": "search_by_context",
                "items": [{"content": ..., "title": ..., "pages": ..., "raw_data": ...}, ...]
            }
        """
        from ..common.tool_response_format import create_content_response
        logger.info(f"ğŸ” [Tool:search_by_context] ---------- è¯­ä¹‰æ£€ç´¢ ----------")
        logger.info(f"ğŸ” [Tool:search_by_context] æŸ¥è¯¢å†…å®¹: {query}")

        if not query or not query.strip():
            logger.warning("ğŸ” [Tool:search_by_context] âŒ æŸ¥è¯¢å­—ç¬¦ä¸²ä¸ºç©º")
            return []

        if not self.agent.vector_db_client:
            logger.error("ğŸ” [Tool:search_by_context] âŒ å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–")
            return []

        try:
            logger.info(f"ğŸ” [Tool:search_by_context] æ‰§è¡Œå‘é‡æ£€ç´¢ (k=3, type=context)")
            # ä½¿ç”¨ type='context' è¿‡æ»¤å™¨è¿›è¡Œè¯­ä¹‰æœç´¢ï¼Œå¯ç”¨å»é‡
            doc_res = self.agent.vector_db_client.search_with_metadata_filter(
                query=query,
                k=3,
                field_name="type",
                field_value="context",
                enable_dedup=True
            )
            logger.info(f"ğŸ” [Tool:search_by_context] å‘é‡æ£€ç´¢è¿”å›: {len(doc_res) if doc_res else 0} ä¸ªç»“æœ")

            context_data = []
            chapter_info_list = []  # å­˜å‚¨ç« èŠ‚ä¿¡æ¯ç”¨äºæ±‡æ€»

            if doc_res and len(doc_res) > 0:
                for idx, doc_item in enumerate(doc_res):
                    try:
                        # è§£ææ–‡æ¡£ç»“æ„
                        document = doc_item[0] if isinstance(doc_item, tuple) else doc_item
                        metadata = document.metadata

                        refactor_data = metadata.get("refactor", "")
                        raw_data = metadata.get("raw_data", {})
                        page_number = list(raw_data.keys()) if isinstance(raw_data, dict) else []

                        # æå–ç« èŠ‚æ ‡é¢˜ä¿¡æ¯
                        chapter_title = metadata.get("title", "æœªçŸ¥ç« èŠ‚")

                        # æ•´ç†å¹¶è¿”å›æ£€ç´¢åˆ°çš„æ•°æ®ï¼ˆåŒ…å«å…ƒæ•°æ®ï¼‰
                        if refactor_data and refactor_data.strip():
                            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹ï¼ˆå»é‡ï¼‰
                            existing_contents = [item["content"] for item in context_data]
                            if refactor_data not in existing_contents:
                                # è¿”å›ç»“æ„åŒ–æ•°æ®ï¼šåŒ…å«å†…å®¹ã€å…ƒæ•°æ®å’ŒåŸå§‹æ•°æ®
                                context_data.append({
                                    "content": refactor_data,  # refactor åçš„å†…å®¹ï¼Œç”¨äº evaluate
                                    "title": chapter_title,
                                    "pages": sorted(page_number, key=lambda x: int(x) if str(x).isdigit() else 0) if page_number else [],
                                    "raw_data": raw_data  # åŸå§‹æ•°æ®ï¼Œç”¨äº format ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                                })

                                # è®°å½•ç« èŠ‚ä¿¡æ¯ç”¨äºæ—¥å¿—æ±‡æ€»
                                chapter_info_list.append({
                                    "title": chapter_title,
                                    "pages": sorted(page_number, key=lambda x: int(x) if str(x).isdigit() else 0) if page_number else []
                                })

                    except Exception as e:
                        logger.error(f"âŒ [Tool:search_by_context] å¤„ç†ç¬¬ {idx+1} ä¸ªæ–‡æ¡£æ—¶å‡ºé”™: {e}")
                        continue

                # ========== æ±‡æ€»æ—¥å¿— ==========
                logger.info("")
                logger.info("=" * 60)
                logger.info("âœ… [CONTEXT RETRIEVAL] ä¸Šä¸‹æ–‡æ£€ç´¢ç»“æœ")
                logger.info("=" * 60)
                logger.info(f"ğŸ“Š è¿”å› {len(context_data)} æ¡å†…å®¹ç‰‡æ®µ")

                # æ˜¾ç¤ºæœ¬æ¬¡è¿”å›å†…å®¹å¯¹åº”çš„ç« èŠ‚å’Œé¡µç 
                if chapter_info_list:
                    logger.info("ğŸ“š æ£€ç´¢åˆ°çš„ç« èŠ‚:")
                    for idx, chapter in enumerate(chapter_info_list, 1):
                        pages_str = f"é¡µç : {', '.join(map(str, chapter['pages']))}" if chapter['pages'] else "æ— é¡µç "
                        logger.info(f"   {idx}. {chapter['title']} ({pages_str})")
                else:
                    logger.info("ğŸ“š æœªæ£€ç´¢åˆ°ä»»ä½•ç« èŠ‚")

                logger.info("=" * 60)
                logger.info("")
            else:
                logger.warning("âš ï¸ [Tool:search_by_context] åœ¨å‘é‡æ•°æ®åº“ä¸­æœªæ‰¾åˆ°ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹")

            # è¿”å›æ ‡å‡†æ ¼å¼
            return create_content_response("search_by_context", context_data)

        except Exception as e:
            logger.error(f"âŒ [Tool:search_by_context] é€šè¿‡ä¸Šä¸‹æ–‡æ£€ç´¢æ•°æ®æ—¶å‡ºé”™: {e}", exc_info=True)
            # é”™è¯¯æ—¶è¿”å›ç©ºç»“æœçš„æ ‡å‡†æ ¼å¼
            return create_content_response("search_by_context", [])

    async def extract_titles_from_structure(self, query: str) -> Dict:
        """
        ä»æ–‡æ¡£ç»“æ„ä¸­æå–ç›¸å…³æ ‡é¢˜åˆ—è¡¨ï¼ˆå¸¦é€‰æ‹©åŸå› ï¼‰

        æ ¹æ®ç”¨æˆ·æŸ¥è¯¢ï¼Œä» type="structure" æ–‡æ¡£ä¸­è·å– agenda_dictï¼Œ
        ç„¶åä½¿ç”¨ LLM æ™ºèƒ½æå–ä¸æŸ¥è¯¢ç›¸å…³çš„ç« èŠ‚æ ‡é¢˜ï¼Œå¹¶è¯´æ˜é€‰æ‹©åŸå› ã€‚

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢å­—ç¬¦ä¸²

        Returns:
            æ ‡å‡†æ ¼å¼çš„å·¥å…·å“åº”ï¼š
            {
                "type": "metadata",
                "tool": "extract_titles_from_structure",
                "items": ["ç« èŠ‚1", "ç« èŠ‚2", ...],
                "metadata": {"reason": "é€‰æ‹©è¿™äº›ç« èŠ‚çš„åŸå› "}
            }
        """
        from src.utils.helpers import extract_data_from_LLM_res
        from src.agents.common.prompts import CommonRole
        from ..common.tool_response_format import create_metadata_response

        logger.info(f"ğŸ“‹ [Tool:extract_titles_from_structure] ä»ç»“æ„ä¸­æå–æ ‡é¢˜: {query[:50]}...")

        if not query or not query.strip():
            logger.warning("âŒ [Tool:extract_titles_from_structure] æŸ¥è¯¢å­—ç¬¦ä¸²ä¸ºç©º")
            return create_metadata_response("extract_titles_from_structure", [], {"reason": "æŸ¥è¯¢ä¸ºç©º"})

        if not self.agent.vector_db_client:
            logger.error("âŒ [Tool:extract_titles_from_structure] VectorDBClient æœªåˆå§‹åŒ–")
            return create_metadata_response("extract_titles_from_structure", [], {"reason": "å‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–"})

        try:
            # æ­¥éª¤1: ä»å‘é‡æ•°æ®åº“è·å– agenda_dict
            agenda_dict = self.agent.utils.get_agenda_dict_from_vector_db()

            if not agenda_dict:
                logger.warning("âš ï¸ [Tool:extract_titles_from_structure] æœªæ‰¾åˆ°æ–‡æ¡£ç»“æ„ä¿¡æ¯")
                return create_metadata_response("extract_titles_from_structure", [], {"reason": "æœªæ‰¾åˆ°æ–‡æ¡£ç»“æ„ä¿¡æ¯"})

            # æ­¥éª¤2: ä½¿ç”¨ LLM æå–æ ‡é¢˜åˆ—è¡¨å’ŒåŸå› 
            response = self.agent.llm.call_llm_chain(
                CommonRole.CHAPTER_MATCHER,
                query,
                "chapter_matcher",
                system_format_dict={
                    "agenda_dict": agenda_dict
                }
            )

            response_data = extract_data_from_LLM_res(response)
            title_list = response_data.get("title", [])
            reason = response_data.get("reason", "æœªæä¾›é€‰æ‹©åŸå› ")

            # éªŒè¯ç»“æœ
            if not isinstance(title_list, list):
                logger.warning("âš ï¸ [Tool:extract_titles_from_structure] æ ‡é¢˜åˆ—è¡¨æ ¼å¼æ— æ•ˆ")
                return create_metadata_response("extract_titles_from_structure", [], {"reason": "æ ‡é¢˜åˆ—è¡¨æ ¼å¼æ— æ•ˆ"})

            logger.info(f"âœ… [Tool:extract_titles_from_structure] æå–åˆ° {len(title_list)} ä¸ªæ ‡é¢˜")
            logger.info(f"ğŸ“‹ [Tool:extract_titles_from_structure]   - æ ‡é¢˜: {title_list}")
            logger.info(f"ğŸ“‹ [Tool:extract_titles_from_structure]   - åŸå› : {reason}")

            # è¿”å›æ ‡å‡†æ ¼å¼
            return create_metadata_response(
                "extract_titles_from_structure",
                title_list,
                {"reason": reason}
            )

        except Exception as e:
            logger.error(f"âŒ [Tool:extract_titles_from_structure] æå–æ ‡é¢˜å¤±è´¥: {e}", exc_info=True)
            return create_metadata_response("extract_titles_from_structure", [], {"reason": f"æå–å¤±è´¥: {str(e)}"})

    async def search_by_title(self, title_list: str) -> Dict:
        """
        åŸºäºæ ‡é¢˜åˆ—è¡¨çš„ç²¾ç¡®æ£€ç´¢å·¥å…·

        æ ¹æ®ç»™å®šçš„æ ‡é¢˜åˆ—è¡¨ï¼Œåœ¨å‘é‡æ•°æ®åº“ä¸­ç²¾ç¡®åŒ¹é…è¿™äº›æ ‡é¢˜æ¥æ£€ç´¢å¯¹åº”çš„æ–‡æ¡£å†…å®¹ã€‚

        Args:
            title_list: æ ‡é¢˜åˆ—è¡¨ï¼ˆJSONæ ¼å¼å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰

        Returns:
            æ ‡å‡†æ ¼å¼çš„å·¥å…·å“åº”ï¼š
            {
                "type": "content",
                "tool": "search_by_title",
                "items": [{"content": ..., "title": ..., "pages": ..., "raw_data": ...}, ...]
            }
        """
        from ..common.tool_response_format import create_content_response
        logger.info(f"ğŸ“‘ [Tool:search_by_title] ---------- æ ‡é¢˜æ£€ç´¢ ----------")
        logger.info(f"ğŸ“‘ [Tool:search_by_title] è¾“å…¥æ ‡é¢˜: {title_list}")

        if not self.agent.vector_db_client:
            logger.error("ğŸ“‘ [Tool:search_by_title] âŒ VectorDBClient æœªåˆå§‹åŒ–")
            return []

        try:
            # è§£æ title_listï¼ˆå¯èƒ½æ˜¯å­—ç¬¦ä¸²æˆ–åˆ—è¡¨ï¼‰
            if isinstance(title_list, str):
                # å°è¯•è§£æä¸ºJSON
                try:
                    parsed_list = json.loads(title_list)
                    if isinstance(parsed_list, list):
                        title_list = parsed_list
                    else:
                        logger.warning("âš ï¸ [Tool:search_by_title] è§£æåçš„æ•°æ®ä¸æ˜¯åˆ—è¡¨")
                        return []
                except json.JSONDecodeError:
                    # å¦‚æœä¸æ˜¯JSONï¼ŒæŒ‰é€—å·åˆ†å‰²
                    title_list = [t.strip() for t in title_list.split(',') if t.strip()]

            # è¾“å…¥éªŒè¯
            if not isinstance(title_list, list):
                logger.warning("âš ï¸ [Tool:search_by_title] æ ‡é¢˜åˆ—è¡¨æ ¼å¼æ— æ•ˆï¼ŒæœŸæœ›listç±»å‹")
                return []

            if len(title_list) == 0:
                logger.info("â„¹ï¸ [Tool:search_by_title] æ ‡é¢˜åˆ—è¡¨ä¸ºç©ºï¼Œè¿”å›ç©ºç»“æœ")
                return []

            logger.info(f"ğŸ“ [Tool:search_by_title] å¤„ç† {len(title_list)} ä¸ªæ ‡é¢˜: {title_list}")

        except Exception as e:
            logger.error(f"âŒ [Tool:search_by_title] è§£ææ ‡é¢˜åˆ—è¡¨å¤±è´¥: {e}")
            return []

        # éå†æ ‡é¢˜åˆ—è¡¨ï¼Œæ£€ç´¢å¯¹åº”å†…å®¹
        context_data = []
        successful_retrievals = 0
        cache_hits = 0
        returned_titles = []  # è¿½è¸ªå®é™…è¿”å›çš„æ ‡é¢˜

        for title in title_list:
            if not title or not isinstance(title, str):
                continue

            title = title.strip()
            if not title:
                continue

            try:
                refactor_data = ""
                page_number = []
                raw_data = {}
                is_from_cache = False

                # æ£€æŸ¥ç¼“å­˜
                if title in self.agent.retrieval_data_dict:
                    cached_data = self.agent.retrieval_data_dict[title]
                    refactor_data = cached_data.get("data", "")
                    page_number = cached_data.get("page", [])
                    raw_data = cached_data.get("raw_data", {})
                    cache_hits += 1
                    is_from_cache = True
                else:
                    # ä»å‘é‡æ•°æ®åº“æ£€ç´¢ï¼ˆä»…æ£€ç´¢ type='title' çš„æ–‡æ¡£ï¼‰
                    try:
                        doc_res = self.agent.vector_db_client.search_by_title(
                            title,
                            doc_type="title",
                            enable_dedup=True
                        )

                        if doc_res and len(doc_res) > 0:
                            # å¤„ç†è¿”å›çš„åˆ—è¡¨ä¸­çš„æ¯ä¸ªæ–‡æ¡£
                            all_refactor_data = []
                            all_page_numbers = []
                            merged_raw_data = {}  # åˆå¹¶æ‰€æœ‰æ–‡æ¡£çš„ raw_data

                            for doc_item in doc_res:
                                document = doc_item[0] if isinstance(doc_item, tuple) else doc_item
                                metadata = document.metadata

                                item_refactor_data = metadata.get("refactor", "")
                                item_raw_data = metadata.get("raw_data", {})
                                item_page_numbers = list(item_raw_data.keys()) if isinstance(item_raw_data, dict) else []

                                if item_refactor_data and item_refactor_data.strip():
                                    all_refactor_data.append(item_refactor_data)

                                if item_page_numbers:
                                    all_page_numbers.extend(item_page_numbers)

                                # åˆå¹¶ raw_data
                                if isinstance(item_raw_data, dict):
                                    merged_raw_data.update(item_raw_data)

                            # åˆå¹¶æ‰€æœ‰æ£€ç´¢åˆ°çš„æ•°æ®
                            refactor_data = "\n\n".join(all_refactor_data) if all_refactor_data else ""
                            page_number = list(set(all_page_numbers))  # å»é‡é¡µé¢ç¼–å·

                            # ç¼“å­˜æ£€ç´¢ç»“æœï¼ˆåŒ…å« raw_dataï¼‰
                            self.agent.retrieval_data_dict[title] = {
                                "data": refactor_data,
                                "page": page_number,
                                "raw_data": merged_raw_data
                            }

                            successful_retrievals += 1
                        else:
                            logger.warning(f"âš ï¸ [Tool:search_by_title] ç« èŠ‚ '{title}' åœ¨å‘é‡æ•°æ®åº“ä¸­æœªæ‰¾åˆ°")

                    except Exception as e:
                        logger.error(f"âŒ [Tool:search_by_title] æ£€ç´¢ç« èŠ‚ '{title}' æ—¶å‡ºé”™: {e}")
                        continue

                # æ·»åŠ åˆ°ä¸Šä¸‹æ–‡æ•°æ®ï¼ˆå»é‡ï¼‰ï¼ŒåŒ…å«å…ƒæ•°æ®
                if refactor_data and refactor_data.strip():
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒå†…å®¹
                    existing_contents = [item["content"] if isinstance(item, dict) else item for item in context_data]
                    if refactor_data not in existing_contents:
                        # è¿”å›ç»“æ„åŒ–æ•°æ®ï¼šåŒ…å«å†…å®¹ã€å…ƒæ•°æ®å’ŒåŸå§‹æ•°æ®
                        context_data.append({
                            "content": refactor_data,  # refactor åçš„å†…å®¹ï¼Œç”¨äº evaluate
                            "title": title,
                            "pages": sorted(page_number, key=lambda x: int(x) if str(x).isdigit() else 0) if page_number else [],
                            "raw_data": raw_data  # åŸå§‹æ•°æ®ï¼Œç”¨äº format ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ
                        })
                        # è®°å½•ç”¨äºæ—¥å¿—æ±‡æ€»
                        returned_titles.append({
                            "title": title,
                            "pages": page_number,
                            "from_cache": is_from_cache
                        })

            except Exception as e:
                logger.error(f"âŒ [Tool:search_by_title] å¤„ç†ç« èŠ‚ '{title}' æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                continue

        # ========== æ±‡æ€»æ—¥å¿— ==========
        logger.info("")
        logger.info("=" * 60)
        logger.info("âœ… [TITLE RETRIEVAL] æ ‡é¢˜æ£€ç´¢ç»“æœ")
        logger.info("=" * 60)
        logger.info(f"ğŸ“Š è¿”å› {len(context_data)} æ¡å†…å®¹ç‰‡æ®µ (æ–°æ£€ç´¢: {successful_retrievals}, ç¼“å­˜: {cache_hits})")

        # æ˜¾ç¤ºæœ¬æ¬¡å®é™…è¿”å›çš„ç« èŠ‚å’Œé¡µç 
        if returned_titles:
            logger.info("ğŸ“š æœ¬æ¬¡è¿”å›çš„ç« èŠ‚:")
            for item in returned_titles:
                title = item["title"]
                pages = item["pages"]
                from_cache = item.get("from_cache", False)

                cache_tag = " [ç¼“å­˜]" if from_cache else " [æ–°æ£€ç´¢]"

                if pages:
                    sorted_pages = sorted(pages, key=lambda x: int(x) if str(x).isdigit() else 0)
                    pages_str = f"é¡µç : {', '.join(map(str, sorted_pages))}"
                else:
                    pages_str = "æ— é¡µç "
                logger.info(f"   âœ“ {title} ({pages_str}){cache_tag}")
        else:
            logger.info("ğŸ“š æœªæ£€ç´¢åˆ°ä»»ä½•å†…å®¹")

        logger.info("=" * 60)
        logger.info("")

        # è¿”å›æ ‡å‡†æ ¼å¼
        return create_content_response("search_by_title", context_data)

    async def get_document_structure(self, query: str = "") -> Dict:
        """
        è·å–æ–‡æ¡£çš„ç›®å½•ç»“æ„å·¥å…·

        ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ type="structure" çš„ç‰¹æ®Šæ–‡æ¡£ï¼Œè·å–æ–‡æ¡£ç»“æ„ä¿¡æ¯ã€‚

        Args:
            query: æŸ¥è¯¢å‚æ•°ï¼ˆæ­¤å·¥å…·ä¸éœ€è¦å…·ä½“æŸ¥è¯¢å†…å®¹ï¼Œä¿ç•™ç”¨äºæ¥å£å…¼å®¹ï¼‰

        Returns:
            æ ‡å‡†æ ¼å¼çš„å·¥å…·å“åº”ï¼š
            {
                "type": "structure",
                "tool": "get_document_structure",
                "items": ["ç¬¬1ç«  å¼•è¨€", "ç¬¬2ç«  èƒŒæ™¯", ...]
            }
        """
        from ..common.tool_response_format import create_structure_response

        _ = query  # å‚æ•°ä¿ç•™ç”¨äºæ¥å£å…¼å®¹ï¼Œå®é™…ä¸ä½¿ç”¨
        logger.info(f"ğŸ“š [Tool:get_document_structure] ---------- è·å–æ–‡æ¡£ç»“æ„ ----------")

        if not self.agent.vector_db_client:
            logger.error("ğŸ“š [Tool:get_document_structure] âŒ VectorDBClient æœªåˆå§‹åŒ–")
            return create_structure_response("get_document_structure", ["æ–‡æ¡£ç»“æ„ä¿¡æ¯ä¸å¯ç”¨ï¼ˆå‘é‡æ•°æ®åº“æœªåˆå§‹åŒ–ï¼‰"])

        try:
            # è·å– agenda_dict
            agenda_dict = self.agent.utils.get_agenda_dict_from_vector_db()

            if not agenda_dict:
                logger.warning("âš ï¸ [Tool:get_document_structure] æ–‡æ¡£ç»“æ„ä¿¡æ¯ä¸ºç©º")
                return create_structure_response("get_document_structure", ["æ–‡æ¡£ç›®å½•ä¿¡æ¯ä¸å¯ç”¨"])

            # æ ¼å¼åŒ–ç›®å½•ç»“æ„
            structure_list = []
            structure_list.append("=" * 60)
            structure_list.append("ğŸ“‘ æ–‡æ¡£ç›®å½•ç»“æ„")
            structure_list.append("=" * 60)

            for title, page_info in agenda_dict.items():
                if isinstance(page_info, list):
                    if len(page_info) == 0:
                        page_str = "é¡µç æœªçŸ¥"
                    elif len(page_info) == 1:
                        page_str = f"é¡µç : {page_info[0]}"
                    else:
                        sorted_pages = sorted(page_info, key=lambda x: int(x) if str(x).isdigit() else 0)
                        page_str = f"é¡µç : {sorted_pages[0]}-{sorted_pages[-1]}"
                else:
                    page_str = f"é¡µç : {page_info}"

                structure_list.append(f"{title} ({page_str})")

            structure_list.append("=" * 60)

            logger.info(f"âœ… [Tool:get_document_structure] è·å–åˆ° {len(agenda_dict)} ä¸ªç« èŠ‚")
            # è¿”å›æ ‡å‡†æ ¼å¼
            return create_structure_response("get_document_structure", structure_list)

        except Exception as e:
            logger.error(f"âŒ [Tool:get_document_structure] è·å–å¤±è´¥: {e}", exc_info=True)
            return create_structure_response("get_document_structure", ["æ–‡æ¡£ç»“æ„ä¿¡æ¯ä¸å¯ç”¨"])
