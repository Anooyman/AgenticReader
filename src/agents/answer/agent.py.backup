"""
Answer Agent - ç”¨æˆ·å¯¹è¯æ¥å£Agent

è´Ÿè´£ï¼š
1. åˆ†æç”¨æˆ·æ„å›¾
2. å†³å®šæ˜¯å¦éœ€è¦æ£€ç´¢
3. è°ƒç”¨Retrieval Agentè·å–ä¸Šä¸‹æ–‡
4. ç”Ÿæˆæœ€ç»ˆå›ç­”
"""

from langgraph.graph import StateGraph, END
import logging
import json
import re

from ..base import AgentBase
from .state import AnswerState
from src.config.prompts.answer_prompts import AnswerRole

logger = logging.getLogger(__name__)


class AnswerAgent(AgentBase):
    """
    å¯¹è¯Agent

    å·¥ä½œæµç¨‹ï¼š
    1. analyze_intent - åˆ†æç”¨æˆ·æ„å›¾ï¼ˆåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢ï¼‰
    2. retrieve (å¯é€‰) - è°ƒç”¨Retrieval Agentæ£€ç´¢æ–‡æ¡£ä¸Šä¸‹æ–‡
    3. generate_answer - ç»“åˆæ£€ç´¢ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœ‰ï¼‰å’Œå†å²å¯¹è¯ç”Ÿæˆæœ€ç»ˆå›ç­”

    å·¥å…·æ–¹æ³•ï¼š
    - call_retrieval_impl - è°ƒç”¨æ£€ç´¢Agentè·å–æ–‡æ¡£ä¸Šä¸‹æ–‡

    æ³¨æ„ï¼š
    - å†å²å¯¹è¯ç”±LLM Clientè‡ªåŠ¨ç®¡ç†ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†
    - æ£€ç´¢ç»“æœä½œä¸ºæ–‡æ¡£ä¸Šä¸‹æ–‡ï¼Œè€Œéæœ€ç»ˆç­”æ¡ˆ
    - æ‰€æœ‰å›ç­”éƒ½ç»“åˆå†å²å¯¹è¯ä¸Šä¸‹æ–‡ç”Ÿæˆ
    """

    def __init__(self, doc_name: str = None):
        super().__init__(name="AnswerAgent")

        # å»¶è¿ŸåŠ è½½Retrieval Agent
        self.retrieval_agent = None

        # å½“å‰æ–‡æ¡£ä¸Šä¸‹æ–‡
        self.current_doc = doc_name

        self.graph = self.build_graph()

    # ==================== å·¥å…·æ–¹æ³•å®ç° ====================

    async def call_retrieval_impl(self, query: str) -> str:
        """
        è°ƒç”¨Retrieval Agentæ£€ç´¢æ–‡æ¡£å†…å®¹ï¼ˆå·¥å…·æ–¹æ³•ï¼‰

        Args:
            query: ç”¨æˆ·æŸ¥è¯¢

        Returns:
            æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å†…å®¹
        """
        logger.info(f"ğŸ” [Tool:call_retrieval] è°ƒç”¨æ£€ç´¢: {query[:50]}...")

        try:
            # å»¶è¿ŸåŠ è½½Retrieval Agent
            if self.retrieval_agent is None:
                from ..retrieval import RetrievalAgent
                self.retrieval_agent = RetrievalAgent(doc_name=self.current_doc)
                logger.info("âœ… Retrieval Agentå·²åŠ è½½")

            # è°ƒç”¨Retrieval Agentçš„graphï¼ˆå‚è€ƒ test_retrieval_agent.pyï¼‰
            result = await self.retrieval_agent.graph.ainvoke({
                "query": query,
                "doc_name": self.current_doc,
                "max_iterations": 10,
                "current_iteration": 0,
                "is_complete": False,
                "thoughts": [],
                "actions": [],
                "observations": [],
                "retrieved_content": []
            })

            # æå–æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡
            context = result.get("final_summary", "")

            logger.info(f"âœ… [Tool:call_retrieval] æ£€ç´¢å®Œæˆï¼Œä¸Šä¸‹æ–‡é•¿åº¦: {len(context)}")
            return context

        except Exception as e:
            logger.error(f"âŒ [Tool:call_retrieval] æ£€ç´¢å¤±è´¥: {e}")
            return ""

    # ==================== WorkflowèŠ‚ç‚¹æ–¹æ³• ====================

    def build_graph(self) -> StateGraph:
        """æ„å»ºworkflow"""
        workflow = StateGraph(AnswerState)

        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("analyze", self.analyze_intent)
        workflow.add_node("retrieve", self.call_retrieval)
        workflow.add_node("generate", self.generate_answer)

        # æ·»åŠ æ¡ä»¶è¾¹ï¼šæ ¹æ®æ˜¯å¦éœ€è¦æ£€ç´¢é€‰æ‹©è·¯å¾„
        workflow.add_conditional_edges(
            "analyze",
            self.route_by_intent,
            {
                "retrieve": "retrieve",  # éœ€è¦æ£€ç´¢
                "direct": "generate"  # ç›´æ¥å›ç­”
            }
        )

        workflow.add_edge("retrieve", "generate")
        workflow.add_edge("generate", END)

        # è®¾ç½®å…¥å£
        workflow.set_entry_point("analyze")

        return workflow.compile()

    async def analyze_intent(self, state: AnswerState) -> AnswerState:
        """
        æ­¥éª¤1ï¼šåˆ†æç”¨æˆ·æ„å›¾

        åŸºäºå¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢æ–‡æ¡£å†…å®¹æ¥å›ç­”å½“å‰é—®é¢˜
        æ³¨æ„ï¼šå¯¹è¯å†å²å·²ç”± LLM Client è‡ªåŠ¨ç®¡ç†ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†
        """
        logger.info(f"ğŸ¤” [Analyze] åˆ†ææ„å›¾: {state['user_query'][:50]}...")

        try:
            # ç®€åŒ–çš„ promptï¼ˆå¯¹è¯å†å²ç”± LLM Client ç®¡ç†ï¼‰
            prompt = f"""
å½“å‰ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

è¯·åˆ¤æ–­æ˜¯å¦éœ€è¦ä»æ–‡æ¡£ä¸­æ£€ç´¢æ–°ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚

è¿”å›JSONæ ¼å¼ï¼š
{{
    "needs_retrieval": true/false,
    "reason": "ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±ï¼ˆ20å­—ä»¥å†…ï¼‰"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

            # ä½¿ç”¨ä¸“é—¨çš„æ„å›¾åˆ†æ Role
            response = await self.llm.async_call_llm_chain(
                role=AnswerRole.INTENT_ANALYZER,
                input_prompt=prompt,
                session_id="analyze_intent"
            )

            # è§£æJSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                needs_retrieval = result.get("needs_retrieval", True)
                reason = result.get("reason", "")
            else:
                # é»˜è®¤éœ€è¦æ£€ç´¢
                logger.warning("âš ï¸ [Analyze] JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
                needs_retrieval = True
                reason = "JSONè§£æå¤±è´¥ï¼Œé»˜è®¤æ£€ç´¢"

            logger.info(
                f"âœ… [Analyze] æ„å›¾åˆ†æå®Œæˆ: "
                f"{'éœ€è¦æ£€ç´¢' if needs_retrieval else 'ç›´æ¥å›ç­”'} - {reason}"
            )

            # æ›´æ–° state å¹¶è¿”å›
            state["needs_retrieval"] = needs_retrieval
            state["analysis_reason"] = reason
            return state

        except Exception as e:
            logger.error(f"âŒ [Analyze] æ„å›¾åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())

            # å¤±è´¥æ—¶é»˜è®¤éœ€è¦æ£€ç´¢ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            state["needs_retrieval"] = True
            state["analysis_reason"] = "åˆ†æå¤±è´¥ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥"
            return state

    def route_by_intent(self, state: AnswerState) -> str:
        """
        æ ¹æ®æ„å›¾è·¯ç”±åˆ°ä¸åŒèŠ‚ç‚¹

        Returns:
            "retrieve" æˆ– "direct"
        """
        if state.get("needs_retrieval", False):
            return "retrieve"
        else:
            return "direct"

    async def call_retrieval(self, state: AnswerState) -> AnswerState:
        """
        æ­¥éª¤2ï¼šè°ƒç”¨Retrieval Agentæ£€ç´¢ï¼ˆä½¿ç”¨å·¥å…·æ–¹æ³•ï¼‰

        ç¼–æ’Retrieval Agentè¿›è¡Œå†…å®¹æ£€ç´¢
        """
        logger.info(f"ğŸ” [Retrieve] è°ƒç”¨Retrieval Agent")

        try:
            # æ›´æ–°å½“å‰æ–‡æ¡£ä¸Šä¸‹æ–‡
            self.current_doc = state.get("current_doc")

            # è°ƒç”¨å·¥å…·æ–¹æ³•
            context = await self.call_retrieval_impl(state["user_query"])

            # æ›´æ–° state å¹¶è¿”å›
            state["context"] = context
            return state

        except Exception as e:
            logger.error(f"âŒ [Retrieve] æ£€ç´¢å¤±è´¥: {e}")

            # æ›´æ–° state å¹¶è¿”å›
            state["context"] = ""
            return state

    async def generate_answer(self, state: AnswerState) -> AnswerState:
        """
        æ­¥éª¤3ï¼šç”Ÿæˆæœ€ç»ˆå›ç­”

        ç»“åˆæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœ‰ï¼‰å’Œå†å²å¯¹è¯ï¼ˆç”±LLM Clientè‡ªåŠ¨ç®¡ç†ï¼‰ç”Ÿæˆå›ç­”
        """
        logger.info(f"ğŸ’¬ [Generate] ç”Ÿæˆå›ç­”")

        try:
            context = state.get("context", "")
            user_query = state['user_query']

            if context:
                # æœ‰æ£€ç´¢ä¸Šä¸‹æ–‡ - æä¾›æ–‡æ¡£å‚è€ƒå†…å®¹
                prompt = f"""
ç”¨æˆ·é—®é¢˜ï¼š{user_query}

æ–‡æ¡£å‚è€ƒå†…å®¹ï¼š
{context}
"""
                logger.info(f"ğŸ“š [Generate] ä½¿ç”¨æ–‡æ¡£ä¸Šä¸‹æ–‡ + å†å²å¯¹è¯å›ç­”")
            else:
                # æ— æ£€ç´¢ä¸Šä¸‹æ–‡ - ä»…æä¾›ç”¨æˆ·é—®é¢˜
                prompt = f"""
ç”¨æˆ·é—®é¢˜ï¼š{user_query}
"""
                logger.info(f"ğŸ’¬ [Generate] ä»…ä½¿ç”¨å†å²å¯¹è¯å›ç­”")

            # ä½¿ç”¨ä¸“é—¨çš„å¯¹è¯å¼é—®ç­” roleï¼ˆå†å²å¯¹è¯ç”± LLM Client è‡ªåŠ¨ç®¡ç†ï¼‰
            answer = await self.llm.async_call_llm_chain(
                role=AnswerRole.CONVERSATIONAL_QA,
                input_prompt=prompt,
                session_id="generate_answer"
            )

            logger.info(f"âœ… [Generate] å›ç­”ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)}")

            # æ›´æ–° state å¹¶è¿”å›
            state["final_answer"] = answer
            state["is_complete"] = True
            return state

        except Exception as e:
            logger.error(f"âŒ [Generate] å›ç­”ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())

            # æ›´æ–° state å¹¶è¿”å›
            state["final_answer"] = f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
            state["is_complete"] = True
            return state
