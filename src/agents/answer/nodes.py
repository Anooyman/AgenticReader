"""
AnswerAgent WorkflowèŠ‚ç‚¹æ–¹æ³•

æ‰€æœ‰workflowèŠ‚ç‚¹çš„å®ç°
"""

from __future__ import annotations
from typing import Dict, TYPE_CHECKING
import logging
import json
import re

from .state import AnswerState

if TYPE_CHECKING:
    from .agent import AnswerAgent

logger = logging.getLogger(__name__)


class AnswerNodes:
    """AnswerAgent WorkflowèŠ‚ç‚¹æ–¹æ³•é›†åˆ"""

    def __init__(self, agent: 'AnswerAgent'):
        """
        Args:
            agent: AnswerAgentå®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        """
        self.agent = agent

    async def analyze_intent(self, state: AnswerState) -> AnswerState:
        """
        æ­¥éª¤1ï¼šåˆ†æç”¨æˆ·æ„å›¾

        åŸºäºå¯¹è¯å†å²å’Œä¸Šä¸‹æ–‡ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ£€ç´¢æ–‡æ¡£å†…å®¹æ¥å›ç­”å½“å‰é—®é¢˜
        æ³¨æ„ï¼šå¯¹è¯å†å²å·²ç”± LLM Client è‡ªåŠ¨ç®¡ç†ï¼Œæ— éœ€æ‰‹åŠ¨å¤„ç†
        """
        from src.config.prompts.answer_prompts import AnswerRole

        logger.info(f"ğŸ¤” [Analyze] åˆ†ææ„å›¾: {state['user_query'][:50]}...")

        try:
            # ç®€åŒ–çš„ promptï¼ˆå¯¹è¯å†å²ç”± LLM Client ç®¡ç†ï¼‰
            prompt = f"""
å½“å‰ç”¨æˆ·é—®é¢˜ï¼š{state['user_query']}

è¯·åˆ¤æ–­æ˜¯å¦éœ€è¦ä»æ–‡æ¡£ä¸­æ£€ç´¢æ–°ä¿¡æ¯æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚

è¿”å›JSONæ ¼å¼ï¼š
{{
    "needs_retrieval": true/false,
    "reason": "ç®€è¦è¯´æ˜åˆ¤æ–­ç†ç”±ï¼ˆ20å­—ä»¥å†…ï¼‰"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ã€‚
"""

            # ä½¿ç”¨ä¸“é—¨çš„æ„å›¾åˆ†æ Role
            response = await self.agent.llm.async_call_llm_chain(
                role=AnswerRole.INTENT_ANALYZER,
                input_prompt=prompt,
                session_id="analyze_intent"
            )

            # è§£æJSON
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                needs_retrieval = result.get("needs_retrieval", True)
                reason = result.get("reason", "")
            else:
                # é»˜è®¤éœ€è¦æ£€ç´¢
                logger.warning("âš ï¸ [Analyze] JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤ç­–ç•¥")
                needs_retrieval = True
                reason = "JSONè§£æå¤±è´¥ï¼Œé»˜è®¤æ£€ç´¢"

            logger.info(
                f"âœ… [Analyze] æ„å›¾åˆ†æå®Œæˆ: "
                f"{'éœ€è¦æ£€ç´¢' if needs_retrieval else 'ç›´æ¥å›ç­”'} - {reason}"
            )

            # æ›´æ–° state å¹¶è¿”å›
            state["needs_retrieval"] = needs_retrieval
            state["analysis_reason"] = reason
            return state

        except Exception as e:
            logger.error(f"âŒ [Analyze] æ„å›¾åˆ†æå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())

            # å¤±è´¥æ—¶é»˜è®¤éœ€è¦æ£€ç´¢ï¼ˆä¿å®ˆç­–ç•¥ï¼‰
            state["needs_retrieval"] = True
            state["analysis_reason"] = "åˆ†æå¤±è´¥ï¼Œé‡‡ç”¨ä¿å®ˆç­–ç•¥"
            return state

    async def call_retrieval(self, state: AnswerState) -> AnswerState:
        """
        æ­¥éª¤2ï¼šè°ƒç”¨Retrieval Agentæ£€ç´¢ï¼ˆä½¿ç”¨å·¥å…·æ–¹æ³•ï¼‰

        ç¼–æ’Retrieval Agentè¿›è¡Œå†…å®¹æ£€ç´¢
        """
        logger.info(f"ğŸ” [Retrieve] è°ƒç”¨Retrieval Agent")

        try:
            # æ›´æ–°å½“å‰æ–‡æ¡£ä¸Šä¸‹æ–‡
            self.agent.current_doc = state.get("current_doc")

            # è°ƒç”¨å·¥å…·æ–¹æ³•
            context = await self.agent.tools.call_retrieval_impl(state["user_query"])

            # æ›´æ–° state å¹¶è¿”å›
            state["context"] = context
            return state

        except Exception as e:
            logger.error(f"âŒ [Retrieve] æ£€ç´¢å¤±è´¥: {e}")

            # æ›´æ–° state å¹¶è¿”å›
            state["context"] = ""
            return state

    async def generate_answer(self, state: AnswerState) -> AnswerState:
        """
        æ­¥éª¤3ï¼šç”Ÿæˆæœ€ç»ˆå›ç­”

        ç»“åˆæ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœ‰ï¼‰å’Œå†å²å¯¹è¯ï¼ˆç”±LLM Clientè‡ªåŠ¨ç®¡ç†ï¼‰ç”Ÿæˆå›ç­”
        """
        from src.config.prompts.answer_prompts import AnswerRole

        logger.info(f"ğŸ’¬ [Generate] ç”Ÿæˆå›ç­”")

        try:
            context = state.get("context", "")
            user_query = state['user_query']

            if context:
                # æœ‰æ£€ç´¢ä¸Šä¸‹æ–‡ - æä¾›æ–‡æ¡£å‚è€ƒå†…å®¹
                prompt = f"""
ç”¨æˆ·é—®é¢˜ï¼š{user_query}

æ–‡æ¡£å‚è€ƒå†…å®¹ï¼š
{context}
"""
                logger.info(f"ğŸ“š [Generate] ä½¿ç”¨æ–‡æ¡£ä¸Šä¸‹æ–‡ + å†å²å¯¹è¯å›ç­”")
            else:
                # æ— æ£€ç´¢ä¸Šä¸‹æ–‡ - ä»…æä¾›ç”¨æˆ·é—®é¢˜
                prompt = f"""
ç”¨æˆ·é—®é¢˜ï¼š{user_query}
"""
                logger.info(f"ğŸ’¬ [Generate] ä»…ä½¿ç”¨å†å²å¯¹è¯å›ç­”")

            # ä½¿ç”¨ä¸“é—¨çš„å¯¹è¯å¼é—®ç­” roleï¼ˆå†å²å¯¹è¯ç”± LLM Client è‡ªåŠ¨ç®¡ç†ï¼‰
            answer = await self.agent.llm.async_call_llm_chain(
                role=AnswerRole.CONVERSATIONAL_QA,
                input_prompt=prompt,
                session_id="generate_answer"
            )

            logger.info(f"âœ… [Generate] å›ç­”ç”Ÿæˆå®Œæˆï¼Œé•¿åº¦: {len(answer)}")

            # æ›´æ–° state å¹¶è¿”å›
            state["final_answer"] = answer
            state["is_complete"] = True
            return state

        except Exception as e:
            logger.error(f"âŒ [Generate] å›ç­”ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            logger.debug(traceback.format_exc())

            # æ›´æ–° state å¹¶è¿”å›
            state["final_answer"] = f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}"
            state["is_complete"] = True
            return state

    def route_by_intent(self, state: AnswerState) -> str:
        """
        æ ¹æ®æ„å›¾è·¯ç”±åˆ°ä¸åŒèŠ‚ç‚¹

        Returns:
            "retrieve" æˆ– "direct"
        """
        if state.get("needs_retrieval", False):
            return "retrieve"
        else:
            return "direct"
