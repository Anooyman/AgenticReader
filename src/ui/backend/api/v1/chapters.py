"""ç« èŠ‚ç®¡ç† API"""

from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any
from pydantic import BaseModel
import json
from pathlib import Path

from ...config import settings, get_logger

logger = get_logger(__name__)

router = APIRouter()


class ChapterInfo(BaseModel):
    """ç« èŠ‚ä¿¡æ¯æ¨¡å‹"""
    title: str
    pages: List[int]
    start_page: int
    end_page: int


class ChapterUpdate(BaseModel):
    """ç« èŠ‚æ›´æ–°æ¨¡å‹"""
    title: str
    pages: List[int]


@router.get("/documents/{doc_name}/chapters")
async def get_document_chapters(doc_name: str) -> Dict[str, Any]:
    """è·å–æ–‡æ¡£çš„ç« èŠ‚ä¿¡æ¯
    
    Args:
        doc_name: æ–‡æ¡£åç§°
        
    Returns:
        åŒ…å«ç« èŠ‚åˆ—è¡¨çš„å­—å…¸
    """
    try:
        json_path = settings.data_dir / "json_data" / f"{doc_name}.json"
        
        if not json_path.exists():
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_name}")
        
        # è¯»å–JSONæ•°æ®
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        if not isinstance(json_data, list):
            raise HTTPException(status_code=400, detail="æ–‡æ¡£æ ¼å¼ä¸æ­£ç¡®")
        
        # ä¼˜å…ˆä»agendaç¼“å­˜è¯»å–ç« èŠ‚ä¿¡æ¯ï¼ˆç”¨æˆ·å¯èƒ½å·²ç»ä¿®æ”¹ï¼‰
        agenda_cache_path = settings.data_dir / "agenda" / f"{doc_name}_agenda.json"
        chapters = []
        chapter_dict = {}
        
        if agenda_cache_path.exists():
            # ä»ç¼“å­˜è¯»å–
            try:
                with open(agenda_cache_path, 'r', encoding='utf-8') as f:
                    chapter_dict = json.load(f)
                
                for title, data in chapter_dict.items():
                    pages = data.get('pages', [])
                    if pages:
                        chapters.append({
                            "title": title,
                            "pages": pages,
                            "start_page": min(pages),
                            "end_page": max(pages),
                            "page_count": len(pages)
                        })
                logger.info(f"ğŸ“š ä»ç¼“å­˜åŠ è½½äº† {len(chapters)} ä¸ªç« èŠ‚")
            except Exception as cache_error:
                logger.warning(f"ä»ç¼“å­˜æå–ç« èŠ‚ä¿¡æ¯å¤±è´¥: {cache_error}")
        
        # å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œå°è¯•ä»å‘é‡æ•°æ®åº“è¯»å–
        if not chapters:
            vector_db_path = settings.data_dir / "vector_db" / f"{doc_name}_data_index"
            
            if vector_db_path.exists():
                # ä»å‘é‡æ•°æ®åº“å…ƒæ•°æ®ä¸­æå–ç« èŠ‚ä¿¡æ¯
                try:
                    from langchain_community.vectorstores import FAISS
                    from src.core.llm.client import get_embeddings
                    
                    # åŠ è½½å‘é‡æ•°æ®åº“
                    embeddings = get_embeddings()
                    vectorstore = FAISS.load_local(
                        str(vector_db_path),
                        embeddings,
                        allow_dangerous_deserialization=True
                    )
                    
                    # ä»å‘é‡æ•°æ®åº“æ–‡æ¡£ä¸­æå–ç« èŠ‚ä¿¡æ¯
                    docs = vectorstore.docstore._dict
                    
                    for doc_id, doc in docs.items():
                        metadata = doc.metadata
                        if metadata.get('type') == 'context':  # åªå¤„ç†å†…å®¹ç±»å‹çš„æ–‡æ¡£
                            title = metadata.get('title', 'æœªçŸ¥ç« èŠ‚')
                            pages = metadata.get('pages', [])
                            
                            if title and pages:
                                chapter_dict[title] = {
                                    'pages': pages,
                                    'data': metadata.get('raw_data', {})
                                }
                    
                    # æ„å»ºç« èŠ‚åˆ—è¡¨
                    for title, data in chapter_dict.items():
                        pages = data.get('pages', [])
                        if pages:
                            chapters.append({
                                "title": title,
                                "pages": pages,
                                "start_page": min(pages),
                                "end_page": max(pages),
                                "page_count": len(pages)
                            })
                    
                    # ä¿å­˜åˆ°ç¼“å­˜ä»¥ä¾¿åç»­ä¿®æ”¹ï¼ˆå­˜å‚¨åˆ°agendaç›®å½•ï¼‰
                    agenda_cache_path = settings.data_dir / "agenda" / f"{doc_name}_agenda.json"
                    agenda_cache_path.parent.mkdir(parents=True, exist_ok=True)
                    with open(agenda_cache_path, 'w', encoding='utf-8') as f:
                        json.dump(chapter_dict, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"ğŸ“š ä»å‘é‡æ•°æ®åº“åŠ è½½äº† {len(chapters)} ä¸ªç« èŠ‚å¹¶ä¿å­˜åˆ°ç¼“å­˜")
                        
                except Exception as e:
                    logger.warning(f"ä»å‘é‡æ•°æ®åº“æå–ç« èŠ‚ä¿¡æ¯å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ç« èŠ‚ä¿¡æ¯ï¼Œè¿”å›åŸºäºé¡µç çš„é»˜è®¤ç»“æ„
        if not chapters:
            total_pages = len(json_data)
            chapters = [{
                "title": f"å®Œæ•´æ–‡æ¡£",
                "pages": list(range(1, total_pages + 1)),
                "start_page": 1,
                "end_page": total_pages,
                "page_count": total_pages
            }]
            logger.info(f"ğŸ“„ ä½¿ç”¨é»˜è®¤ç« èŠ‚ç»“æ„ï¼Œå…± {total_pages} é¡µ")
        
        return {
            "success": True,
            "doc_name": doc_name,
            "total_chapters": len(chapters),
            "chapters": sorted(chapters, key=lambda x: x['start_page'])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£ç« èŠ‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.put("/documents/{doc_name}/chapters/{chapter_index}")
async def update_chapter(
    doc_name: str,
    chapter_index: int,
    chapter_data: ChapterUpdate
) -> Dict[str, Any]:
    """æ›´æ–°æŒ‡å®šç« èŠ‚çš„ä¿¡æ¯
    
    Args:
        doc_name: æ–‡æ¡£åç§°
        chapter_index: ç« èŠ‚ç´¢å¼•
        chapter_data: æ›´æ–°çš„ç« èŠ‚æ•°æ®
        
    Returns:
        æ›´æ–°ç»“æœ
    """
    try:
        json_path = settings.data_dir / "json_data" / f"{doc_name}.json"
        
        if not json_path.exists():
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_name}")
        
        # è¯»å–JSONæ•°æ®éªŒè¯
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        total_pages = len(json_data)
        
        # éªŒè¯é¡µç èŒƒå›´
        if not all(1 <= page <= total_pages for page in chapter_data.pages):
            raise HTTPException(
                status_code=400, 
                detail=f"é¡µç è¶…å‡ºèŒƒå›´ (1-{total_pages})"
            )
        
        # å°è¯•æ›´æ–°PDFé˜…è¯»å™¨ä¸­çš„ç« èŠ‚ä¿¡æ¯
        try:
            from src.readers.pdf import PDFReader
            
            # åŠ è½½ç°æœ‰çš„ç« èŠ‚ä¿¡æ¯
            agenda_cache_path = settings.data_dir / "agenda" / f"{doc_name}_agenda.json"
            
            if agenda_cache_path.exists():
                with open(agenda_cache_path, 'r', encoding='utf-8') as f:
                    agenda_dict = json.load(f)
                
                # æŒ‰ç…§ start_page æ’åºç« èŠ‚ï¼ˆä¸ GET è¯·æ±‚ä¿æŒä¸€è‡´ï¼‰
                sorted_chapters = []
                for title, data in agenda_dict.items():
                    pages = data.get('pages', [])
                    if pages:
                        sorted_chapters.append({
                            'title': title,
                            'start_page': min(pages)
                        })
                sorted_chapters.sort(key=lambda x: x['start_page'])
                
                if 0 <= chapter_index < len(sorted_chapters):
                    old_title = sorted_chapters[chapter_index]['title']
                    
                    # å¦‚æœæ ‡é¢˜æ”¹å˜ï¼Œéœ€è¦é‡æ–°åˆ›å»ºæ¡ç›®
                    if chapter_data.title != old_title:
                        # ä¿å­˜æ—§æ•°æ®
                        old_data = agenda_dict.pop(old_title)
                        # åˆ›å»ºæ–°æ¡ç›®
                        agenda_dict[chapter_data.title] = {
                            'pages': chapter_data.pages,
                            'data': old_data.get('data', {})
                        }
                    else:
                        # åªæ›´æ–°é¡µç 
                        agenda_dict[old_title]['pages'] = chapter_data.pages
                    
                    # ä¿å­˜æ›´æ–°åçš„æ•°æ®
                    with open(agenda_cache_path, 'w', encoding='utf-8') as f:
                        json.dump(agenda_dict, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"âœ… ç« èŠ‚ {chapter_index} å·²æ›´æ–°: {chapter_data.title}")
                    
                    return {
                        "success": True,
                        "message": f"ç« èŠ‚å·²æ›´æ–°: {chapter_data.title}",
                        "chapter": {
                            "title": chapter_data.title,
                            "pages": chapter_data.pages,
                            "start_page": min(chapter_data.pages),
                            "end_page": max(chapter_data.pages)
                        }
                    }
                else:
                    raise HTTPException(status_code=404, detail="ç« èŠ‚ç´¢å¼•è¶…å‡ºèŒƒå›´")
            else:
                # å¦‚æœç¼“å­˜ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„
                logger.info(f"åˆ›å»ºæ–°çš„agendaç¼“å­˜: {doc_name}")
                agenda_dict = {
                    chapter_data.title: {
                        'pages': chapter_data.pages,
                        'data': {}
                    }
                }
                
                # ä¿å­˜åˆ°æ–‡ä»¶
                agenda_cache_path.parent.mkdir(parents=True, exist_ok=True)
                with open(agenda_cache_path, 'w', encoding='utf-8') as f:
                    json.dump(agenda_dict, f, ensure_ascii=False, indent=2)
                
                return {
                    "success": True,
                    "message": f"ç« èŠ‚å·²åˆ›å»º: {chapter_data.title}",
                    "chapter": {
                        "title": chapter_data.title,
                        "pages": chapter_data.pages,
                        "start_page": min(chapter_data.pages),
                        "end_page": max(chapter_data.pages)
                    }
                }
                
        except Exception as e:
            logger.error(f"æ›´æ–°ç« èŠ‚å¤±è´¥: {e}")
            raise HTTPException(status_code=500, detail=f"æ›´æ–°ç« èŠ‚å¤±è´¥: {str(e)}")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ›´æ–°ç« èŠ‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/documents/{doc_name}/chapters")
async def add_chapter(
    doc_name: str,
    chapter_data: ChapterUpdate
) -> Dict[str, Any]:
    """æ·»åŠ æ–°ç« èŠ‚
    
    Args:
        doc_name: æ–‡æ¡£åç§°
        chapter_data: æ–°ç« èŠ‚æ•°æ®
        
    Returns:
        æ·»åŠ ç»“æœ
    """
    try:
        json_path = settings.data_dir / "json_data" / f"{doc_name}.json"
        
        if not json_path.exists():
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_name}")
        
        # éªŒè¯é¡µç 
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        total_pages = len(json_data)
        if not all(1 <= page <= total_pages for page in chapter_data.pages):
            raise HTTPException(
                status_code=400,
                detail=f"é¡µç è¶…å‡ºèŒƒå›´ (1-{total_pages})"
            )
        
        # åŠ è½½æˆ–åˆ›å»ºagendaç¼“å­˜
        agenda_cache_path = settings.data_dir / "agenda" / f"{doc_name}_agenda.json"
        
        if agenda_cache_path.exists():
            with open(agenda_cache_path, 'r', encoding='utf-8') as f:
                agenda_dict = json.load(f)
        else:
            agenda_dict = {}
            agenda_cache_path.parent.mkdir(parents=True, exist_ok=True)
        
        # æ£€æŸ¥ç« èŠ‚æ ‡é¢˜æ˜¯å¦å·²å­˜åœ¨
        if chapter_data.title in agenda_dict:
            raise HTTPException(
                status_code=400,
                detail=f"ç« èŠ‚æ ‡é¢˜å·²å­˜åœ¨: {chapter_data.title}"
            )
        
        # æ·»åŠ æ–°ç« èŠ‚
        agenda_dict[chapter_data.title] = {
            'pages': chapter_data.pages,
            'data': {}
        }
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        with open(agenda_cache_path, 'w', encoding='utf-8') as f:
            json.dump(agenda_dict, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ“ æ·»åŠ æ–°ç« èŠ‚: {chapter_data.title} (é¡µç : {chapter_data.pages})")
        
        return {
            "success": True,
            "message": f"ç« èŠ‚å·²æ·»åŠ : {chapter_data.title}",
            "chapter": {
                "title": chapter_data.title,
                "pages": chapter_data.pages,
                "start_page": min(chapter_data.pages),
                "end_page": max(chapter_data.pages)
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"æ·»åŠ ç« èŠ‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/documents/{doc_name}/chapters/{chapter_index}")
async def delete_chapter(
    doc_name: str,
    chapter_index: int
) -> Dict[str, Any]:
    """åˆ é™¤æŒ‡å®šç« èŠ‚
    
    Args:
        doc_name: æ–‡æ¡£åç§°
        chapter_index: ç« èŠ‚ç´¢å¼•
        
    Returns:
        åˆ é™¤ç»“æœ
    """
    try:
        agenda_cache_path = settings.data_dir / "agenda" / f"{doc_name}_agenda.json"
        
        if not agenda_cache_path.exists():
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ç« èŠ‚ç¼“å­˜ä¸å­˜åœ¨: {doc_name}")
        
        # è¯»å–ç°æœ‰ç« èŠ‚
        with open(agenda_cache_path, 'r', encoding='utf-8') as f:
            agenda_dict = json.load(f)
        
        # æŒ‰ç…§ start_page æ’åºç« èŠ‚ï¼ˆä¸ GET è¯·æ±‚ä¿æŒä¸€è‡´ï¼‰
        sorted_chapters = []
        for title, data in agenda_dict.items():
            pages = data.get('pages', [])
            if pages:
                sorted_chapters.append({
                    'title': title,
                    'start_page': min(pages)
                })
        sorted_chapters.sort(key=lambda x: x['start_page'])
        
        if not (0 <= chapter_index < len(sorted_chapters)):
            raise HTTPException(status_code=404, detail="ç« èŠ‚ç´¢å¼•è¶…å‡ºèŒƒå›´")
        
        # è·å–è¦åˆ é™¤çš„ç« èŠ‚æ ‡é¢˜ï¼ˆä½¿ç”¨æ’åºåçš„ç´¢å¼•ï¼‰
        chapter_title = sorted_chapters[chapter_index]['title']
        
        # åˆ é™¤ç« èŠ‚
        del agenda_dict[chapter_title]
        
        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
        with open(agenda_cache_path, 'w', encoding='utf-8') as f:
            json.dump(agenda_dict, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤ç« èŠ‚: {chapter_title} (ç´¢å¼•: {chapter_index})")
        
        return {
            "success": True,
            "message": f"ç« èŠ‚å·²åˆ é™¤: {chapter_title}"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"åˆ é™¤ç« èŠ‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/documents/{doc_name}/rebuild")
async def rebuild_document_data(
    doc_name: str,
    rebuild_vectordb: bool = True,
    rebuild_summary: bool = True
) -> Dict[str, Any]:
    """æ ¹æ®ä¿®æ”¹åçš„ç« èŠ‚ä¿¡æ¯é‡å»ºå‘é‡æ•°æ®åº“å’Œæ‘˜è¦
    
    Args:
        doc_name: æ–‡æ¡£åç§°
        rebuild_vectordb: æ˜¯å¦é‡å»ºå‘é‡æ•°æ®åº“
        rebuild_summary: æ˜¯å¦é‡å»ºæ‘˜è¦
        
    Returns:
        é‡å»ºç»“æœ
    """
    try:
        from src.readers.pdf import PDFReader
        from langchain.docstore.document import Document
        
        logger.info(f"ğŸ”„ å¼€å§‹é‡å»ºæ–‡æ¡£æ•°æ®: {doc_name}")
        logger.info(f"  - é‡å»ºå‘é‡æ•°æ®åº“: {rebuild_vectordb}")
        logger.info(f"  - é‡å»ºæ‘˜è¦: {rebuild_summary}")
        
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        json_path = settings.data_dir / "json_data" / f"{doc_name}.json"
        if not json_path.exists():
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_name}")
        
        # è¯»å–JSONæ•°æ®
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data_list = json.load(f)
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ {page: data}
        json_data_dict = {int(item['page']): item.get('data', '') for item in json_data_list}
        
        # è¯»å–ä¿®æ”¹åçš„ç« èŠ‚ä¿¡æ¯
        agenda_cache_path = settings.data_dir / "agenda" / f"{doc_name}_agenda.json"
        if not agenda_cache_path.exists():
            raise HTTPException(status_code=400, detail="ç« èŠ‚ä¿¡æ¯ç¼“å­˜ä¸å­˜åœ¨ï¼Œè¯·å…ˆç¼–è¾‘ç« èŠ‚")
        
        with open(agenda_cache_path, 'r', encoding='utf-8') as f:
            agenda_dict_unsorted = json.load(f)
        
        # æŒ‰ç…§èµ·å§‹é¡µç æ’åºagenda_dictï¼Œç¡®ä¿è¾“å‡ºé¡ºåºæ­£ç¡®
        sorted_items = sorted(
            agenda_dict_unsorted.items(),
            key=lambda x: min(x[1].get('pages', [float('inf')]))
        )
        agenda_dict = dict(sorted_items)
        
        # å°†æ’åºåçš„agendaä¿å­˜å›æ–‡ä»¶
        with open(agenda_cache_path, 'w', encoding='utf-8') as f:
            json.dump(agenda_dict, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“‹ ç« èŠ‚å·²æŒ‰é¡µç æ’åºï¼Œå…± {len(agenda_dict)} ä¸ªç« èŠ‚")
        
        # åˆå§‹åŒ–PDFé˜…è¯»å™¨
        pdf_reader = PDFReader(provider="openai")
        pdf_reader.agenda_dict = agenda_dict
        pdf_reader.output_path = settings.data_dir / "output" / doc_name
        pdf_reader.output_path.mkdir(parents=True, exist_ok=True)
        
        rebuild_results = {}
        
        # é‡å»ºå‘é‡æ•°æ®åº“
        if rebuild_vectordb:
            logger.info("ğŸ“Š å¼€å§‹é‡å»ºå‘é‡æ•°æ®åº“...")
            try:
                vector_db_content_docs = []
                total_summary = {}
                
                for title, chapter_info in agenda_dict.items():
                    pages = chapter_info.get('pages', [])
                    # ä»JSONæ•°æ®ä¸­æå–è¯¥ç« èŠ‚çš„åŸå§‹æ•°æ®
                    raw_data = {page: json_data_dict.get(page, '') for page in pages if page in json_data_dict}
                    
                    # ç”Ÿæˆç« èŠ‚å†…å®¹å’Œæ‘˜è¦
                    logger.info(f"  - å¤„ç†ç« èŠ‚: {title} (é¡µç : {min(pages)}-{max(pages)})")
                    content_list = [raw_data[page] for page in sorted(raw_data.keys()) if raw_data.get(page)]
                    
                    if not content_list:
                        logger.warning(f"  âš ï¸ ç« èŠ‚ '{title}' æ²¡æœ‰å†…å®¹ï¼Œè·³è¿‡")
                        continue
                    
                    summary = pdf_reader.summary_content(title, content_list)
                    refactor = pdf_reader.refactor_content(title, content_list)
                    total_summary[title] = summary
                    
                    # æ„å»ºå‘é‡æ•°æ®åº“æ–‡æ¡£
                    vector_db_content_docs.append(
                        Document(
                            page_content=summary,
                            metadata={
                                "type": "context",
                                "title": title,
                                "pages": pages,
                                "raw_data": raw_data,
                                "refactor": refactor,
                            }
                        )
                    )
                    vector_db_content_docs.append(
                        Document(
                            page_content=title,
                            metadata={
                                "type": "title",
                                "pages": pages,
                                "summary": summary,
                                "raw_data": raw_data,
                                "refactor": refactor,
                            }
                        )
                    )
                
                # ä¿å­˜æ€»æ‘˜è¦åˆ°PDFé˜…è¯»å™¨
                pdf_reader.total_summary = total_summary
                
                # åˆå§‹åŒ–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯ï¼ˆä¿®æ­£å‚æ•°åï¼‰
                from src.core.vector_db.vector_db_client import VectorDBClient
                vector_db_path = str(settings.data_dir / "vector_db" / f"{doc_name}_data_index")
                vector_db_client = VectorDBClient(db_path=vector_db_path, provider="openai")
                
                # é‡å»ºå‘é‡æ•°æ®åº“
                logger.info(f"  - å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“ï¼Œå…± {len(vector_db_content_docs)} ä¸ªæ–‡æ¡£")
                vector_db_client.build_vector_db(vector_db_content_docs)
                
                rebuild_results['vectordb'] = {
                    'success': True,
                    'chapters_processed': len([t for t in agenda_dict.keys()]),
                    'documents_created': len(vector_db_content_docs)
                }
                logger.info(f"âœ… å‘é‡æ•°æ®åº“é‡å»ºå®Œæˆ")
                
            except Exception as e:
                logger.error(f"âŒ å‘é‡æ•°æ®åº“é‡å»ºå¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
                rebuild_results['vectordb'] = {
                    'success': False,
                    'error': str(e)
                }
        
        # é‡å»ºæ‘˜è¦æ–‡ä»¶
        if rebuild_summary:
            logger.info("ğŸ“ å¼€å§‹é‡å»ºæ‘˜è¦æ–‡ä»¶...")
            try:
                # å¦‚æœåœ¨é‡å»ºå‘é‡æ•°æ®åº“æ—¶å·²ç»ç”Ÿæˆäº†æ‘˜è¦ï¼Œç›´æ¥ä½¿ç”¨
                if not hasattr(pdf_reader, 'total_summary') or not pdf_reader.total_summary:
                    logger.info("  - é‡æ–°ç”Ÿæˆç« èŠ‚æ‘˜è¦")
                    total_summary = {}
                    for title, chapter_info in agenda_dict.items():
                        pages = chapter_info.get('pages', [])
                        raw_data = {page: json_data_dict.get(page, '') for page in pages if page in json_data_dict}
                        content_list = [raw_data[page] for page in sorted(raw_data.keys()) if raw_data.get(page)]
                        
                        if not content_list:
                            continue
                        
                        summary = pdf_reader.summary_content(title, content_list)
                        total_summary[title] = summary
                    
                    pdf_reader.total_summary = total_summary
                
                # è®¾ç½®raw_data_dictç”¨äºè¯¦ç»†æ‘˜è¦
                pdf_reader.raw_data_dict = {}
                for title, chapter_info in agenda_dict.items():
                    pages = chapter_info.get('pages', [])
                    raw_data = {page: json_data_dict.get(page, '') for page in pages if page in json_data_dict}
                    if raw_data:
                        pdf_reader.raw_data_dict[title] = raw_data
                
                logger.info(f"  - ç”Ÿæˆç®€è¦æ‘˜è¦ (brief_summary.md)")
                # ç”Ÿæˆç®€è¦æ‘˜è¦
                pdf_reader.get_brief_summary(file_type_list=["md"])
                
                logger.info(f"  - ç”Ÿæˆè¯¦ç»†æ‘˜è¦ (detail_summary.md)")
                # ç”Ÿæˆè¯¦ç»†æ‘˜è¦ - ä¼ å…¥raw_data_dictæ ¼å¼: {title: {page: content}}
                pdf_reader.get_detail_summary(pdf_reader.raw_data_dict, file_type_list=["md"])
                
                rebuild_results['summary'] = {
                    'success': True,
                    'output_path': str(pdf_reader.output_path),
                    'files_generated': ['brief_summary.md', 'detail_summary.md']
                }
                logger.info(f"âœ… æ‘˜è¦æ–‡ä»¶é‡å»ºå®Œæˆï¼Œä¿å­˜åˆ°: {pdf_reader.output_path}")
                
            except Exception as e:
                logger.error(f"âŒ æ‘˜è¦æ–‡ä»¶é‡å»ºå¤±è´¥: {e}")
                import traceback
                logger.error(traceback.format_exc())
                rebuild_results['summary'] = {
                    'success': False,
                    'error': str(e)
                }
        
        return {
            "success": True,
            "message": "æ–‡æ¡£æ•°æ®é‡å»ºå®Œæˆ",
            "doc_name": doc_name,
            "results": rebuild_results
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"é‡å»ºæ–‡æ¡£æ•°æ®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
