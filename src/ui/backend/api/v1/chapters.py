""" ç« èŠ‚ç®¡ç† API"""

from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any
from pydantic import BaseModel
import json
from pathlib import Path

from ...config import settings, get_logger
from .config import get_current_provider, get_current_pdf_preset
from src.core.processing.parallel_processor import ChapterProcessor

logger = get_logger(__name__)

router = APIRouter()


class ChapterInfo(BaseModel):
    """ç« èŠ‚ä¿¡æ¯æ¨¡å‹"""
    title: str
    pages: List[int]
    start_page: int
    end_page: int


class ChapterUpdate(BaseModel):
    """ç« èŠ‚æ›´æ–°æ¨¡å‹"""
    title: str
    pages: List[int]


@router.get("/documents/{doc_name}/chapters")
async def get_document_chapters(doc_name: str) -> Dict[str, Any]:
    """è·å–æ–‡æ¡£çš„ç« èŠ‚ä¿¡æ¯
    
    Args:
        doc_name: æ–‡æ¡£åç§°
        
    Returns:
        åŒ…å«ç« èŠ‚åˆ—è¡¨çš„å­—å…¸
    """
    try:
        json_path = settings.data_dir / "json_data" / doc_name / "data.json"
        
        if not json_path.exists():
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_name}")
        
        # è¯»å–JSONæ•°æ®
        with open(json_path, 'r', encoding='utf-8') as f:
            json_data = json.load(f)
        
        if not isinstance(json_data, list):
            raise HTTPException(status_code=400, detail="æ–‡æ¡£æ ¼å¼ä¸æ­£ç¡®")
        
        # ä¼˜å…ˆä»agendaç¼“å­˜è¯»å–ç« èŠ‚ä¿¡æ¯ï¼ˆç”¨æˆ·å¯èƒ½å·²ç»ä¿®æ”¹ï¼‰
        agenda_cache_path = settings.data_dir / "agenda" / f"{doc_name}_agenda.json"
        chapters = []
        chapter_dict = {}

        if agenda_cache_path.exists():
            # ä»ç¼“å­˜è¯»å–
            try:
                with open(agenda_cache_path, 'r', encoding='utf-8') as f:
                    chapter_dict = json.load(f)
                
                for title, data in chapter_dict.items():
                    pages = data.get('pages', [])
                    if pages:
                        chapters.append({
                            "title": title,
                            "pages": pages,
                            "start_page": min(pages),
                            "end_page": max(pages),
                            "page_count": len(pages)
                        })
                logger.info(f"ğŸ“š ä»ç¼“å­˜åŠ è½½äº† {len(chapters)} ä¸ªç« èŠ‚")
            except Exception as cache_error:
                logger.warning(f"ä»ç¼“å­˜æå–ç« èŠ‚ä¿¡æ¯å¤±è´¥: {cache_error}")

        # è‹¥æ— ç¼“å­˜ï¼Œå°è¯•è¯»å–æœ¬åœ°ç»“æ„æ–‡ä»¶ï¼ˆdata/json_data/<doc>/structure.jsonï¼‰
        if not chapters:
            structure_path = settings.data_dir / "json_data" / doc_name / "structure.json"
            if structure_path.exists():
                try:
                    with open(structure_path, 'r', encoding='utf-8') as f:
                        structure_data = json.load(f)
                    agenda_dict = structure_data.get("agenda_dict", {}) if isinstance(structure_data, dict) else {}

                    for title, pages in agenda_dict.items():
                        if not pages:
                            continue
                        # å»é‡å¹¶æ’åºï¼Œç¡®ä¿ä¸ºæ•´æ•°é¡µç 
                        unique_pages = sorted({int(p) for p in pages if isinstance(p, (int, float, str))})
                        if not unique_pages:
                            continue
                        chapter_dict[title] = {"pages": unique_pages}
                        chapters.append({
                            "title": title,
                            "pages": unique_pages,
                            "start_page": min(unique_pages),
                            "end_page": max(unique_pages),
                            "page_count": len(unique_pages)
                        })

                    # å†™å…¥ç¼“å­˜ï¼Œæ–¹ä¾¿åç»­ç¼–è¾‘
                    if chapters:
                        agenda_cache_path.parent.mkdir(parents=True, exist_ok=True)
                        with open(agenda_cache_path, 'w', encoding='utf-8') as f:
                            json.dump(chapter_dict, f, ensure_ascii=False, indent=2)
                        logger.info(f"ğŸ“ è¯»å–æœ¬åœ°ç»“æ„æ–‡ä»¶å¹¶ç¼“å­˜ {len(chapters)} ä¸ªç« èŠ‚")
                except Exception as structure_error:
                    logger.warning(f"è¯»å–ç»“æ„æ–‡ä»¶å¤±è´¥: {structure_error}")

        # å¦‚æœä»æ— ç« èŠ‚ä¿¡æ¯ï¼Œå°è¯•ä»å‘é‡æ•°æ®åº“è¯»å–
        if not chapters:
            vector_db_path = settings.data_dir / "vector_db" / f"{doc_name}_data_index"
            
            if vector_db_path.exists():
                # ä»å‘é‡æ•°æ®åº“å…ƒæ•°æ®ä¸­æå–ç« èŠ‚ä¿¡æ¯
                try:
                    from langchain_community.vectorstores import FAISS
                    from src.core.llm.client import get_embeddings
                    
                    # åŠ è½½å‘é‡æ•°æ®åº“
                    embeddings = get_embeddings()
                    vectorstore = FAISS.load_local(
                        str(vector_db_path),
                        embeddings,
                        allow_dangerous_deserialization=True
                    )
                    
                    # ä»å‘é‡æ•°æ®åº“æ–‡æ¡£ä¸­æå–ç« èŠ‚ä¿¡æ¯
                    docs = vectorstore.docstore._dict
                    
                    for doc_id, doc in docs.items():
                        metadata = doc.metadata
                        if metadata.get('type') == 'context':  # åªå¤„ç†å†…å®¹ç±»å‹çš„æ–‡æ¡£
                            title = metadata.get('title', 'æœªçŸ¥ç« èŠ‚')
                            pages = metadata.get('pages', [])
                            
                            if title and pages:
                                chapter_dict[title] = {
                                    'pages': pages,
                                    'data': metadata.get('raw_data', {})
                                }
                    
                    # æ„å»ºç« èŠ‚åˆ—è¡¨
                    for title, data in chapter_dict.items():
                        pages = data.get('pages', [])
                        if pages:
                            chapters.append({
                                "title": title,
                                "pages": pages,
                                "start_page": min(pages),
                                "end_page": max(pages),
                                "page_count": len(pages)
                            })
                    
                    # ä¿å­˜åˆ°ç¼“å­˜ä»¥ä¾¿åç»­ä¿®æ”¹ï¼ˆå­˜å‚¨åˆ°agendaç›®å½•ï¼‰
                    agenda_cache_path = settings.data_dir / "agenda" / f"{doc_name}_agenda.json"
                    agenda_cache_path.parent.mkdir(parents=True, exist_ok=True)
                    with open(agenda_cache_path, 'w', encoding='utf-8') as f:
                        json.dump(chapter_dict, f, ensure_ascii=False, indent=2)
                    
                    logger.info(f"ğŸ“š ä»å‘é‡æ•°æ®åº“åŠ è½½äº† {len(chapters)} ä¸ªç« èŠ‚å¹¶ä¿å­˜åˆ°ç¼“å­˜")
                        
                except Exception as e:
                    logger.warning(f"ä»å‘é‡æ•°æ®åº“æå–ç« èŠ‚ä¿¡æ¯å¤±è´¥: {e}")
        
        # å¦‚æœæ²¡æœ‰ç« èŠ‚ä¿¡æ¯ï¼Œè¿”å›åŸºäºé¡µç çš„é»˜è®¤ç»“æ„
        if not chapters:
            total_pages = len(json_data)
            chapters = [{
                "title": f"å®Œæ•´æ–‡æ¡£",
                "pages": list(range(1, total_pages + 1)),
                "start_page": 1,
                "end_page": total_pages,
                "page_count": total_pages
            }]
            logger.info(f"ğŸ“„ ä½¿ç”¨é»˜è®¤ç« èŠ‚ç»“æ„ï¼Œå…± {total_pages} é¡µ")
        
        return {
            "success": True,
            "doc_name": doc_name,
            "total_chapters": len(chapters),
            "chapters": sorted(chapters, key=lambda x: x['start_page'])
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£ç« èŠ‚å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))




@router.put("/documents/{doc_name}/chapters/{chapter_index}")
async def update_chapter(
    doc_name: str,
    chapter_index: int,
    chapter_data: ChapterUpdate
) -> Dict[str, str]:
    """æ›´æ–°ç« èŠ‚ä¿¡æ¯ï¼ˆåŠŸèƒ½æš‚æœªå®ç°ï¼‰"""
    raise HTTPException(status_code=501, detail="ç« èŠ‚ä¿®æ”¹åŠŸèƒ½æš‚æœªå®ç°ï¼Œè¯·ä½¿ç”¨ IndexingAgent é‡æ–°ç´¢å¼•æ–‡æ¡£")


@router.post("/documents/{doc_name}/chapters")
async def add_chapter(
    doc_name: str,
    chapter_data: ChapterUpdate
) -> Dict[str, str]:
    """æ·»åŠ æ–°ç« èŠ‚ï¼ˆåŠŸèƒ½æš‚æœªå®ç°ï¼‰"""
    raise HTTPException(status_code=501, detail="æ·»åŠ ç« èŠ‚åŠŸèƒ½æš‚æœªå®ç°ï¼Œè¯·ä½¿ç”¨ IndexingAgent é‡æ–°ç´¢å¼•æ–‡æ¡£")


@router.delete("/documents/{doc_name}/chapters/{chapter_index}")
async def delete_chapter(
    doc_name: str,
    chapter_index: int
) -> Dict[str, str]:
    """åˆ é™¤ç« èŠ‚ï¼ˆåŠŸèƒ½æš‚æœªå®ç°ï¼‰"""
    raise HTTPException(status_code=501, detail="åˆ é™¤ç« èŠ‚åŠŸèƒ½æš‚æœªå®ç°ï¼Œè¯·ä½¿ç”¨ IndexingAgent é‡æ–°ç´¢å¼•æ–‡æ¡£")


@router.post("/documents/{doc_name}/rebuild")
async def rebuild_document_data(
    doc_name: str,
    rebuild_vectordb: bool = True,
    rebuild_summary: bool = False
) -> Dict[str, Any]:
    """é‡å»ºæ–‡æ¡£æ•°æ®ï¼ˆåŠŸèƒ½æš‚æœªå®ç°ï¼‰"""
    raise HTTPException(status_code=501, detail="é‡å»ºåŠŸèƒ½æš‚æœªå®ç°ï¼Œè¯·ä½¿ç”¨ IndexingAgent é‡æ–°ç´¢å¼•æ–‡æ¡£")
