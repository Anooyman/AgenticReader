"""æ•°æ®ç®¡ç† API

æä¾›æ–‡æ¡£æ•°æ®ã€ä¼šè¯æ•°æ®å’Œå­˜å‚¨ç®¡ç†åŠŸèƒ½
"""

from fastapi import APIRouter, HTTPException, Query, BackgroundTasks
from pydantic import BaseModel
from typing import List, Dict, Any, Optional
from pathlib import Path
import shutil
import json
from datetime import datetime, timedelta

from ...config import (
    PDF_DIR, JSON_DATA_DIR, VECTOR_DB_DIR,
    PDF_IMAGE_DIR, OUTPUT_DIR, DATA_DIR
)
from src.core.document_management import DocumentRegistry
from ...services.task_service import task_manager

router = APIRouter()


# ==================== Pydantic Models ====================

class StorageOverview(BaseModel):
    """å­˜å‚¨æ¦‚è§ˆ"""
    total_documents: int
    total_sessions: int
    total_storage_mb: float
    last_cleanup: Optional[str] = None
    breakdown: Dict[str, Dict[str, Any]]


class DocumentDetail(BaseModel):
    """æ–‡æ¡£è¯¦ç»†ä¿¡æ¯ï¼ˆå«å…ƒæ•°æ®ï¼‰"""
    doc_id: str
    doc_name: str
    doc_type: str

    # ä» metadata_enhanced ä¸­æå–çš„å­—æ®µ
    title: Optional[str] = None
    abstract: Optional[str] = None
    keywords: Optional[List[str]] = None
    topics: Optional[List[str]] = None
    extended_summary: Optional[str] = None

    # æ•°æ®å¤§å°ä¿¡æ¯
    json_size_mb: float = 0.0
    vector_db_size_mb: float = 0.0
    images_size_mb: float = 0.0
    summary_size_mb: float = 0.0
    total_size_mb: float = 0.0

    # æ–‡ä»¶å­˜åœ¨çŠ¶æ€
    has_json: bool = False
    has_vector_db: bool = False
    has_images: bool = False
    has_summary: bool = False

    # æ—¶é—´ä¿¡æ¯
    created_at: str
    indexed_at: Optional[str] = None


class SessionStats(BaseModel):
    """ä¼šè¯ç»Ÿè®¡ä¿¡æ¯"""
    total_sessions: int
    by_mode: Dict[str, int]
    total_messages: int
    last_activity: Optional[str] = None


class DeletePartsRequest(BaseModel):
    """åˆ é™¤éƒ¨åˆ†æ•°æ®çš„è¯·æ±‚"""
    parts: List[str]  # ["json", "vector_db", "images", "summary", "all"]


class BatchDeleteRequest(BaseModel):
    """æ‰¹é‡åˆ é™¤è¯·æ±‚"""
    doc_names: List[str]


class PendingPDF(BaseModel):
    """å¾…ç´¢å¼•PDFä¿¡æ¯"""
    filename: str
    file_path: str
    size_mb: float
    created_at: str


# ==================== Helper Functions ====================

def get_dir_size(path: Path) -> float:
    """
    è®¡ç®—ç›®å½•å¤§å°ï¼ˆMBï¼‰

    Args:
        path: ç›®å½•è·¯å¾„

    Returns:
        ç›®å½•å¤§å°ï¼ˆMBï¼‰
    """
    if not path.exists():
        return 0.0

    total_size = 0
    if path.is_file():
        total_size = path.stat().st_size
    elif path.is_dir():
        for item in path.rglob('*'):
            if item.is_file():
                try:
                    total_size += item.stat().st_size
                except (OSError, PermissionError):
                    continue

    return total_size / (1024 * 1024)  # Convert to MB


def get_document_data_sizes(doc_name: str, doc_info: Dict) -> Dict[str, float]:
    """
    è·å–æ–‡æ¡£å„éƒ¨åˆ†æ•°æ®å¤§å°

    Args:
        doc_name: æ–‡æ¡£åç§°
        doc_info: æ–‡æ¡£ä¿¡æ¯å­—å…¸

    Returns:
        å„éƒ¨åˆ†å¤§å°å­—å…¸ï¼ˆMBï¼‰
    """
    # Strip .pdf extension once at the beginning for all path lookups
    doc_name_base = doc_name.replace('.pdf', '') if doc_name.endswith('.pdf') else doc_name

    sizes = {
        "json_size_mb": 0.0,
        "vector_db_size_mb": 0.0,
        "images_size_mb": 0.0,
        "summary_size_mb": 0.0,
        "has_json": False,
        "has_vector_db": False,
        "has_images": False,
        "has_summary": False
    }

    # JSON data (in json_data/{doc_name_base}/ directory)
    json_dir = JSON_DATA_DIR / doc_name_base
    if json_dir.exists():
        sizes["json_size_mb"] = get_dir_size(json_dir)
        sizes["has_json"] = True

    # Vector DB (in vector_db/{doc_name_base}_data_index/ directory)
    vector_db_path = VECTOR_DB_DIR / f"{doc_name_base}_data_index"
    if vector_db_path.exists():
        sizes["vector_db_size_mb"] = get_dir_size(vector_db_path)
        sizes["has_vector_db"] = True

    # Images (in pdf_image/{doc_name_base}/ directory)
    images_dir = PDF_IMAGE_DIR / doc_name_base
    if images_dir.exists():
        sizes["images_size_mb"] = get_dir_size(images_dir)
        sizes["has_images"] = True

    # Summary files (MD and PDF in output directory)
    # Summary files are named as {doc_name_base}_brief_summary.md or {doc_name_base}_summary.md
    for suffix in ['_brief_summary', '_summary', '']:
        for ext in ['.md', '.pdf']:
            summary_file = OUTPUT_DIR / f"{doc_name_base}{suffix}{ext}"
            if summary_file.exists():
                sizes["summary_size_mb"] += get_dir_size(summary_file)
                sizes["has_summary"] = True

    return sizes


def count_sessions() -> Dict[str, Any]:
    """
    ç»Ÿè®¡ä¼šè¯ä¿¡æ¯

    Returns:
        ä¼šè¯ç»Ÿè®¡å­—å…¸
    """
    sessions_dir = DATA_DIR / "sessions"

    if not sessions_dir.exists():
        return {
            "total": 0,
            "by_mode": {"single": 0, "cross": 0, "manual": 0},
            "total_messages": 0,
            "last_activity": None
        }

    stats = {
        "total": 0,
        "by_mode": {"single": 0, "cross": 0, "manual": 0},
        "total_messages": 0,
        "last_activity": None
    }

    last_update = None

    for mode in ["single", "cross", "manual"]:
        mode_dir = sessions_dir / mode
        if mode_dir.exists():
            session_files = list(mode_dir.glob("*.json"))
            stats["by_mode"][mode] = len(session_files)
            stats["total"] += len(session_files)

            # Count messages and track last activity
            for session_file in session_files:
                try:
                    with open(session_file, 'r', encoding='utf-8') as f:
                        session_data = json.load(f)
                        messages = session_data.get("messages", [])
                        stats["total_messages"] += len(messages)

                        updated_at = session_data.get("updated_at")
                        if updated_at:
                            if last_update is None or updated_at > last_update:
                                last_update = updated_at
                except Exception:
                    continue

    stats["last_activity"] = last_update
    return stats


# ==================== API Endpoints ====================

@router.get("/overview", response_model=StorageOverview)
async def get_storage_overview():
    """
    è·å–å­˜å‚¨æ¦‚è§ˆ

    Returns:
        å­˜å‚¨æ¦‚è§ˆä¿¡æ¯
    """
    try:
        registry = DocumentRegistry()
        doc_count = registry.count()

        session_stats = count_sessions()

        # Calculate storage breakdown
        breakdown = {
            "documents": {
                "count": doc_count,
                "size_mb": get_dir_size(PDF_DIR)
            },
            "json_data": {
                "size_mb": get_dir_size(JSON_DATA_DIR)
            },
            "vector_db": {
                "size_mb": get_dir_size(VECTOR_DB_DIR)
            },
            "images": {
                "size_mb": get_dir_size(PDF_IMAGE_DIR)
            },
            "summaries": {
                "size_mb": get_dir_size(OUTPUT_DIR)
            },
            "sessions": {
                "count": session_stats["total"],
                "size_mb": get_dir_size(DATA_DIR / "sessions")
            }
        }

        total_storage = sum(
            item.get("size_mb", 0) for item in breakdown.values()
        )

        return StorageOverview(
            total_documents=doc_count,
            total_sessions=session_stats["total"],
            total_storage_mb=total_storage,
            last_cleanup=None,  # TODO: Track cleanup history
            breakdown=breakdown
        )

    except Exception as e:
        print(f"âŒ è·å–å­˜å‚¨æ¦‚è§ˆå¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/documents", response_model=List[DocumentDetail])
async def get_documents_detailed():
    """
    è·å–æ–‡æ¡£è¯¦ç»†åˆ—è¡¨ï¼ˆåŒ…å«å…ƒæ•°æ®å’Œå¤§å°ä¿¡æ¯ï¼‰

    Returns:
        æ–‡æ¡£è¯¦ç»†ä¿¡æ¯åˆ—è¡¨
    """
    try:
        registry = DocumentRegistry()
        all_docs = registry.list_all(sort_by="indexed_at")

        detailed_docs = []

        for doc in all_docs:
            doc_name = doc.get("doc_name", "")

            # è·å– metadata_enhanced
            metadata_enhanced = doc.get("metadata_enhanced", {})

            # è·å–æ•°æ®å¤§å°
            sizes = get_document_data_sizes(doc_name, doc)

            # æ„å»ºè¯¦ç»†ä¿¡æ¯
            detail = DocumentDetail(
                doc_id=doc.get("doc_id", ""),
                doc_name=doc_name,
                doc_type=doc.get("doc_type", "pdf"),

                # å…ƒæ•°æ®
                title=metadata_enhanced.get("title"),
                abstract=metadata_enhanced.get("abstract"),
                keywords=metadata_enhanced.get("keywords", []),
                topics=metadata_enhanced.get("topics", []),
                extended_summary=metadata_enhanced.get("extended_summary"),

                # å¤§å°ä¿¡æ¯
                json_size_mb=sizes["json_size_mb"],
                vector_db_size_mb=sizes["vector_db_size_mb"],
                images_size_mb=sizes["images_size_mb"],
                summary_size_mb=sizes["summary_size_mb"],
                total_size_mb=(
                    sizes["json_size_mb"] +
                    sizes["vector_db_size_mb"] +
                    sizes["images_size_mb"] +
                    sizes["summary_size_mb"]
                ),

                # å­˜åœ¨çŠ¶æ€
                has_json=sizes["has_json"],
                has_vector_db=sizes["has_vector_db"],
                has_images=sizes["has_images"],
                has_summary=sizes["has_summary"],

                # æ—¶é—´ä¿¡æ¯
                created_at=doc.get("created_at", ""),
                indexed_at=doc.get("indexed_at")
            )

            detailed_docs.append(detail)

        return detailed_docs

    except Exception as e:
        print(f"âŒ è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/documents/{doc_name}/summary")
async def get_document_summary(doc_name: str):
    """
    è·å–æ–‡æ¡£çš„brief_summary

    Args:
        doc_name: æ–‡æ¡£åç§°

    Returns:
        åŒ…å«brief_summaryçš„å­—å…¸
    """
    try:
        registry = DocumentRegistry()
        doc = registry.get_by_name(doc_name)

        if not doc:
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_name}")

        brief_summary = doc.get("brief_summary", "æš‚æ— æ‘˜è¦ä¿¡æ¯")

        return {
            "doc_name": doc_name,
            "brief_summary": brief_summary
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ è·å–æ–‡æ¡£æ‘˜è¦å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/documents/pending", response_model=List[PendingPDF])
async def get_pending_pdfs():
    """
    è·å–å¾…ç´¢å¼•çš„PDFæ–‡ä»¶åˆ—è¡¨

    Returns:
        å¾…ç´¢å¼•PDFåˆ—è¡¨
    """
    try:
        registry = DocumentRegistry()
        indexed_pdfs = set()

        # è·å–æ‰€æœ‰å·²ç´¢å¼•çš„PDFæ–‡ä»¶å
        all_docs = registry.list_all()
        for doc in all_docs:
            doc_name = doc.get("doc_name", "")
            # ç¡®ä¿æ·»åŠ .pdfæ‰©å±•å
            if not doc_name.endswith('.pdf'):
                doc_name += '.pdf'
            indexed_pdfs.add(doc_name)

        # æ‰«æPDFç›®å½•
        pending_pdfs = []
        if PDF_DIR.exists():
            for pdf_file in PDF_DIR.glob("*.pdf"):
                if pdf_file.name not in indexed_pdfs:
                    # æœªç´¢å¼•çš„PDF
                    stat = pdf_file.stat()
                    pending_pdfs.append(PendingPDF(
                        filename=pdf_file.name,
                        file_path=str(pdf_file),
                        size_mb=stat.st_size / (1024 * 1024),
                        created_at=datetime.fromtimestamp(stat.st_ctime).isoformat()
                    ))

        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        pending_pdfs.sort(key=lambda x: x.created_at, reverse=True)

        return pending_pdfs

    except Exception as e:
        print(f"âŒ è·å–å¾…ç´¢å¼•PDFåˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


async def _index_pdf_background(task_id: str, filename: str, pdf_path: Path):
    """
    åå°ç´¢å¼•ä»»åŠ¡

    Args:
        task_id: ä»»åŠ¡ID
        filename: PDFæ–‡ä»¶å
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
    """
    try:
        from src.agents.indexing import IndexingAgent

        doc_name_base = filename.replace('.pdf', '') if filename.endswith('.pdf') else filename

        # æ›´æ–°ä»»åŠ¡è¿›åº¦
        task_manager.update_task(task_id, progress=10, status="running")

        # åˆ›å»ºç´¢å¼•agent
        indexing_agent = IndexingAgent()
        task_manager.update_task(task_id, progress=20)

        print(f"ğŸ”„ åå°ç´¢å¼•ä»»åŠ¡å¼€å§‹: {filename} (task_id: {task_id})")

        # æ‰§è¡Œç´¢å¼•
        result = await indexing_agent.graph.ainvoke({
            "doc_name": doc_name_base,
            "doc_path": str(pdf_path),
            "doc_type": "pdf",
            "is_complete": False,
            "status": "pending"
        })

        task_manager.update_task(task_id, progress=90)

        if result.get("is_complete"):
            task_manager.complete_task(task_id, success=True)
            print(f"âœ… åå°ç´¢å¼•ä»»åŠ¡å®Œæˆ: {filename}")
        else:
            error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
            task_manager.complete_task(task_id, success=False, error=error_msg)
            print(f"âŒ åå°ç´¢å¼•ä»»åŠ¡å¤±è´¥: {filename}, é”™è¯¯: {error_msg}")

    except Exception as e:
        error_msg = str(e)
        task_manager.complete_task(task_id, success=False, error=error_msg)
        print(f"âŒ åå°ç´¢å¼•ä»»åŠ¡å¼‚å¸¸: {filename}, é”™è¯¯: {error_msg}")
        import traceback
        traceback.print_exc()


@router.post("/documents/{filename}/index")
async def index_pdf(filename: str, background_tasks: BackgroundTasks):
    """
    å¯åŠ¨PDFç´¢å¼•åå°ä»»åŠ¡

    Args:
        filename: PDFæ–‡ä»¶å
        background_tasks: FastAPIåå°ä»»åŠ¡

    Returns:
        ä»»åŠ¡ä¿¡æ¯
    """
    try:
        # æ£€æŸ¥PDFæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        pdf_path = PDF_DIR / filename
        if not pdf_path.exists():
            raise HTTPException(status_code=404, detail=f"PDFæ–‡ä»¶ä¸å­˜åœ¨: {filename}")

        # æ£€æŸ¥æ˜¯å¦å·²ç»ç´¢å¼•
        registry = DocumentRegistry()
        doc_name_base = filename.replace('.pdf', '') if filename.endswith('.pdf') else filename
        if registry.get_by_name(doc_name_base):
            raise HTTPException(status_code=400, detail=f"æ–‡æ¡£å·²ç´¢å¼•: {filename}")

        # åˆ›å»ºåå°ä»»åŠ¡
        task_id = task_manager.create_task(
            task_type="pdf_index",
            filename=filename,
            doc_name=doc_name_base
        )

        # æ·»åŠ åå°ä»»åŠ¡
        background_tasks.add_task(_index_pdf_background, task_id, filename, pdf_path)

        print(f"ğŸ“‹ ç´¢å¼•ä»»åŠ¡å·²åˆ›å»º: {filename} (task_id: {task_id})")

        return {
            "status": "started",
            "task_id": task_id,
            "filename": filename,
            "message": f"ç´¢å¼•ä»»åŠ¡å·²å¯åŠ¨"
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ åˆ›å»ºç´¢å¼•ä»»åŠ¡å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/tasks/{task_id}")
async def get_task_status(task_id: str):
    """
    æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€

    Args:
        task_id: ä»»åŠ¡ID

    Returns:
        ä»»åŠ¡çŠ¶æ€ä¿¡æ¯
    """
    task = task_manager.get_task(task_id)
    if not task:
        raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
    return task


@router.get("/tasks")
async def get_all_tasks():
    """
    è·å–æ‰€æœ‰ä»»åŠ¡ï¼ˆè¿è¡Œä¸­ + æœ€è¿‘å®Œæˆï¼‰

    Returns:
        ä»»åŠ¡åˆ—è¡¨
    """
    running = task_manager.get_running_tasks()
    completed = task_manager.get_recent_completed_tasks(limit=20)

    return {
        "running": running,
        "recent_completed": completed,
        "total_running": len(running)
    }


@router.delete("/documents/{doc_name}/parts")
async def delete_document_parts(doc_name: str, request: DeletePartsRequest):
    """
    åˆ é™¤æ–‡æ¡£çš„æŒ‡å®šéƒ¨åˆ†æ•°æ®ï¼ˆç²’åº¦æ§åˆ¶ï¼‰

    Args:
        doc_name: æ–‡æ¡£åç§°
        request: åˆ é™¤è¯·æ±‚ï¼ˆåŒ…å«è¦åˆ é™¤çš„éƒ¨åˆ†ï¼‰

    Returns:
        åˆ é™¤ç»“æœ
    """
    try:
        registry = DocumentRegistry()
        doc_info = registry.get_by_name(doc_name)

        # æ³¨æ„ï¼šå³ä½¿Registryä¸­æ²¡æœ‰è®°å½•ï¼Œä¹Ÿç»§ç»­å°è¯•åˆ é™¤æ–‡ä»¶
        # è¿™æ ·å¯ä»¥æ¸…ç†å­¤ç«‹çš„æ–‡ä»¶ï¼ˆRegistryå·²è¢«åˆ é™¤ä½†æ–‡ä»¶è¿˜åœ¨çš„æƒ…å†µï¼‰

        # Strip .pdf extension for correct path lookups
        doc_name_base = doc_name.replace('.pdf', '') if doc_name.endswith('.pdf') else doc_name

        deleted_items = []
        failed_items = []
        freed_space_mb = 0.0

        parts = request.parts

        # å¦‚æœåŒ…å« "all"ï¼Œåˆ™åˆ é™¤æ‰€æœ‰éƒ¨åˆ†
        if "all" in parts:
            parts = ["json", "vector_db", "images", "summary", "registry"]

        # åˆ é™¤ JSON æ•°æ®
        if "json" in parts:
            json_dir = JSON_DATA_DIR / doc_name_base
            if json_dir.exists():
                try:
                    size = get_dir_size(json_dir)
                    shutil.rmtree(json_dir)
                    deleted_items.append(f"JSON æ•°æ® ({size:.2f} MB)")
                    freed_space_mb += size
                except Exception as e:
                    failed_items.append(f"JSON æ•°æ®: {e}")

        # åˆ é™¤å‘é‡æ•°æ®åº“
        if "vector_db" in parts:
            vector_db_path = VECTOR_DB_DIR / f"{doc_name_base}_data_index"
            if vector_db_path.exists():
                try:
                    size = get_dir_size(vector_db_path)
                    shutil.rmtree(vector_db_path)
                    deleted_items.append(f"å‘é‡æ•°æ®åº“ ({size:.2f} MB)")
                    freed_space_mb += size
                except Exception as e:
                    failed_items.append(f"å‘é‡æ•°æ®åº“: {e}")

        # åˆ é™¤å›¾ç‰‡
        if "images" in parts:
            images_dir = PDF_IMAGE_DIR / doc_name_base
            if images_dir.exists():
                try:
                    size = get_dir_size(images_dir)
                    shutil.rmtree(images_dir)
                    deleted_items.append(f"å›¾ç‰‡ ({size:.2f} MB)")
                    freed_space_mb += size
                except Exception as e:
                    failed_items.append(f"å›¾ç‰‡: {e}")

        # åˆ é™¤æ‘˜è¦æ–‡ä»¶
        if "summary" in parts:
            # Check for different summary file naming patterns
            for suffix in ['_brief_summary', '_summary', '']:
                for ext in ['.md', '.pdf']:
                    summary_file = OUTPUT_DIR / f"{doc_name_base}{suffix}{ext}"
                    if summary_file.exists():
                        try:
                            size = get_dir_size(summary_file)
                            summary_file.unlink()
                            deleted_items.append(f"æ‘˜è¦ ({ext}) ({size:.2f} MB)")
                            freed_space_mb += size
                        except Exception as e:
                            failed_items.append(f"æ‘˜è¦ ({ext}): {e}")

        # å¦‚æœåˆ é™¤äº†æ‰€æœ‰éƒ¨åˆ†ï¼Œåˆ™ä»æ³¨å†Œè¡¨å’Œå…ƒæ•°æ®å‘é‡æ•°æ®åº“ä¸­åˆ é™¤
        if "registry" in parts or set(parts) >= {"json", "vector_db", "images", "summary"}:
            # åªæœ‰å½“Registryä¸­æœ‰è®°å½•æ—¶æ‰å°è¯•åˆ é™¤Registryå’ŒMetadataDB
            if doc_info:
                doc_id = doc_info["doc_id"]

                # 1. å…ˆä» MetadataVectorDB ä¸­åˆ é™¤å…ƒæ•°æ®ï¼ˆåœ¨ Registry åˆ é™¤ä¹‹å‰ï¼‰
                # è¿™æ · MetadataDB.delete_document() å¯ä»¥ä» Registry ä¸­è·å– doc_name ç”¨äºæ—¥å¿—
                try:
                    from src.core.vector_db.metadata_db import MetadataVectorDB
                    metadata_db = MetadataVectorDB()
                    if metadata_db.delete_document(doc_id):
                        deleted_items.append("å…ƒæ•°æ®å‘é‡æ•°æ®åº“è®°å½•")
                        print(f"âœ“ å·²ä» MetadataVectorDB ä¸­åˆ é™¤å…ƒæ•°æ®: {doc_name}")
                    else:
                        failed_items.append("å…ƒæ•°æ®å‘é‡æ•°æ®åº“: åˆ é™¤æœªå®Œå…¨æˆåŠŸ")
                except Exception as meta_e:
                    failed_items.append(f"å…ƒæ•°æ®å‘é‡æ•°æ®åº“: {meta_e}")
                    print(f"âš ï¸ ä» MetadataVectorDB åˆ é™¤å¤±è´¥: {meta_e}")

                # 2. å†ä» DocumentRegistry ä¸­åˆ é™¤
                try:
                    registry.delete(doc_id)
                    deleted_items.append("æ³¨å†Œè¡¨è®°å½•")
                except Exception as e:
                    failed_items.append(f"æ³¨å†Œè¡¨è®°å½•: {e}")
            else:
                # Registryä¸­æ²¡æœ‰è®°å½•ï¼Œè®°å½•ä¸€ä¸ªè­¦å‘Šä½†ä¸ç®—å¤±è´¥
                print(f"âš ï¸  æ–‡æ¡£ {doc_name} åœ¨Registryä¸­ä¸å­˜åœ¨ï¼Œè·³è¿‡Registryåˆ é™¤")

        # å¦‚æœä»€ä¹ˆéƒ½æ²¡åˆ é™¤ï¼ˆæ–‡ä»¶ä¸å­˜åœ¨ä¸”Registryä¹Ÿæ²¡è®°å½•ï¼‰ï¼Œè¿”å›404
        if len(deleted_items) == 0 and not doc_info:
            raise HTTPException(status_code=404, detail=f"æ–‡æ¡£ä¸å­˜åœ¨: {doc_name}")

        return {
            "status": "success" if len(failed_items) == 0 else "partial",
            "doc_name": doc_name,
            "deleted": deleted_items,
            "failed": failed_items,
            "freed_space_mb": round(freed_space_mb, 2)
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ åˆ é™¤æ–‡æ¡£éƒ¨åˆ†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/documents")
async def batch_delete_documents(request: BatchDeleteRequest):
    """
    æ‰¹é‡åˆ é™¤æ–‡æ¡£ï¼ˆå®Œæ•´åˆ é™¤ï¼‰

    Args:
        request: æ‰¹é‡åˆ é™¤è¯·æ±‚

    Returns:
        åˆ é™¤ç»“æœæ±‡æ€»
    """
    try:
        registry = DocumentRegistry()

        results = {
            "total": len(request.doc_names),
            "success": 0,
            "failed": 0,
            "details": [],
            "total_freed_mb": 0.0
        }

        for doc_name in request.doc_names:
            doc_info = registry.get_by_name(doc_name)

            if not doc_info:
                results["failed"] += 1
                results["details"].append({
                    "doc_name": doc_name,
                    "status": "failed",
                    "reason": "æ–‡æ¡£ä¸å­˜åœ¨"
                })
                continue

            try:
                # åˆ é™¤æ‰€æœ‰éƒ¨åˆ†
                delete_result = await delete_document_parts(
                    doc_name,
                    DeletePartsRequest(parts=["all"])
                )

                results["success"] += 1
                results["total_freed_mb"] += delete_result["freed_space_mb"]
                results["details"].append({
                    "doc_name": doc_name,
                    "status": "success",
                    "freed_mb": delete_result["freed_space_mb"]
                })

            except Exception as e:
                results["failed"] += 1
                results["details"].append({
                    "doc_name": doc_name,
                    "status": "failed",
                    "reason": str(e)
                })

        return results

    except Exception as e:
        print(f"âŒ æ‰¹é‡åˆ é™¤å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/cache/{cache_type}")
async def get_cache_info(cache_type: str):
    """
    è·å–ç¼“å­˜ä¿¡æ¯

    Args:
        cache_type: ç¼“å­˜ç±»å‹ (pdf_image, vector_db, json_data)

    Returns:
        ç¼“å­˜ä¿¡æ¯
    """
    try:
        cache_dirs = {
            "pdf_image": PDF_IMAGE_DIR,
            "vector_db": VECTOR_DB_DIR,
            "json_data": JSON_DATA_DIR
        }

        if cache_type not in cache_dirs:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ•ˆçš„ç¼“å­˜ç±»å‹: {cache_type}"
            )

        cache_dir = cache_dirs[cache_type]

        if not cache_dir.exists():
            return {
                "type": cache_type,
                "size_mb": 0.0,
                "items": 0
            }

        # Count items (subdirectories or files)
        items = list(cache_dir.iterdir())

        return {
            "type": cache_type,
            "size_mb": round(get_dir_size(cache_dir), 2),
            "items": len(items)
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ è·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.delete("/cache/{cache_type}")
async def clear_cache(cache_type: str):
    """
    æ¸…ç©ºç¼“å­˜

    Args:
        cache_type: ç¼“å­˜ç±»å‹ (pdf_image, vector_db, json_data)

    Returns:
        æ¸…ç©ºç»“æœ
    """
    try:
        cache_dirs = {
            "pdf_image": PDF_IMAGE_DIR,
            "vector_db": VECTOR_DB_DIR,
            "json_data": JSON_DATA_DIR
        }

        if cache_type not in cache_dirs:
            raise HTTPException(
                status_code=400,
                detail=f"æ— æ•ˆçš„ç¼“å­˜ç±»å‹: {cache_type}"
            )

        cache_dir = cache_dirs[cache_type]

        if not cache_dir.exists():
            return {
                "status": "success",
                "message": "ç¼“å­˜ç›®å½•ä¸å­˜åœ¨",
                "freed_mb": 0.0
            }

        # Calculate size before deletion
        size_before = get_dir_size(cache_dir)

        # Delete and recreate directory
        shutil.rmtree(cache_dir)
        cache_dir.mkdir(parents=True, exist_ok=True)

        return {
            "status": "success",
            "message": f"å·²æ¸…ç©º {cache_type} ç¼“å­˜",
            "freed_mb": round(size_before, 2)
        }

    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.get("/sessions/stats", response_model=SessionStats)
async def get_session_stats():
    """
    è·å–ä¼šè¯ç»Ÿè®¡ä¿¡æ¯

    Returns:
        ä¼šè¯ç»Ÿè®¡
    """
    try:
        stats = count_sessions()

        return SessionStats(
            total_sessions=stats["total"],
            by_mode=stats["by_mode"],
            total_messages=stats["total_messages"],
            last_activity=stats["last_activity"]
        )

    except Exception as e:
        print(f"âŒ è·å–ä¼šè¯ç»Ÿè®¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/cleanup/smart")
async def smart_cleanup(days: int = Query(default=30, ge=1, le=365)):
    """
    æ™ºèƒ½æ¸…ç†æ—§æ•°æ®

    Args:
        days: æ¸…ç†å¤šå°‘å¤©å‰çš„æ•°æ®ï¼ˆé»˜è®¤30å¤©ï¼‰

    Returns:
        æ¸…ç†ç»“æœ
    """
    try:
        cutoff_date = datetime.now() - timedelta(days=days)
        cutoff_str = cutoff_date.isoformat()

        registry = DocumentRegistry()
        all_docs = registry.list_all()

        deleted_docs = []
        freed_space_mb = 0.0

        for doc in all_docs:
            indexed_at = doc.get("indexed_at", doc.get("created_at", ""))

            if indexed_at and indexed_at < cutoff_str:
                doc_name = doc.get("doc_name", "")

                # Delete this document
                try:
                    result = await delete_document_parts(
                        doc_name,
                        DeletePartsRequest(parts=["all"])
                    )

                    deleted_docs.append(doc_name)
                    freed_space_mb += result["freed_space_mb"]

                except Exception as e:
                    print(f"âš ï¸  æ¸…ç†æ–‡æ¡£ {doc_name} å¤±è´¥: {e}")

        return {
            "status": "success",
            "cutoff_date": cutoff_str,
            "deleted_documents": deleted_docs,
            "count": len(deleted_docs),
            "freed_mb": round(freed_space_mb, 2)
        }

    except Exception as e:
        print(f"âŒ æ™ºèƒ½æ¸…ç†å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/backup")
async def create_backup():
    """
    åˆ›å»ºæ•°æ®å¤‡ä»½

    Returns:
        å¤‡ä»½ä¿¡æ¯
    """
    try:
        backup_dir = DATA_DIR / "backups" / datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_dir.mkdir(parents=True, exist_ok=True)

        backed_up = []

        # Backup sessions
        sessions_src = DATA_DIR / "sessions"
        if sessions_src.exists():
            shutil.copytree(sessions_src, backup_dir / "sessions", dirs_exist_ok=True)
            backed_up.append("sessions")

        # Backup doc registry
        registry_file = DATA_DIR / "doc_registry.json"
        if registry_file.exists():
            shutil.copy2(registry_file, backup_dir / "doc_registry.json")
            backed_up.append("doc_registry")

        # Backup output (summaries)
        if OUTPUT_DIR.exists():
            shutil.copytree(OUTPUT_DIR, backup_dir / "output", dirs_exist_ok=True)
            backed_up.append("output")

        backup_size = get_dir_size(backup_dir)

        return {
            "status": "success",
            "backup_path": str(backup_dir),
            "backed_up": backed_up,
            "size_mb": round(backup_size, 2),
            "created_at": datetime.now().isoformat()
        }

    except Exception as e:
        print(f"âŒ åˆ›å»ºå¤‡ä»½å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@router.post("/reset")
async def full_system_reset(confirm: str = Query(...)):
    """
    å®Œå…¨é‡ç½®ç³»ç»Ÿï¼ˆå±é™©æ“ä½œï¼‰

    Args:
        confirm: ç¡®è®¤å­—ç¬¦ä¸²ï¼ˆå¿…é¡»æ˜¯ "CONFIRM_RESET"ï¼‰

    Returns:
        é‡ç½®ç»“æœ
    """
    if confirm != "CONFIRM_RESET":
        raise HTTPException(
            status_code=400,
            detail="å¿…é¡»æä¾›ç¡®è®¤å­—ç¬¦ä¸² 'CONFIRM_RESET'"
        )

    try:
        # Create backup first
        backup_result = await create_backup()

        deleted_items = []
        freed_space_mb = 0.0

        # Delete all data directories
        data_dirs = [
            JSON_DATA_DIR,
            VECTOR_DB_DIR,
            PDF_IMAGE_DIR,
            OUTPUT_DIR,
            DATA_DIR / "sessions"
        ]

        for dir_path in data_dirs:
            if dir_path.exists():
                size = get_dir_size(dir_path)
                shutil.rmtree(dir_path)
                dir_path.mkdir(parents=True, exist_ok=True)
                deleted_items.append(str(dir_path.name))
                freed_space_mb += size

        # Reset document registry
        registry_file = DATA_DIR / "doc_registry.json"
        if registry_file.exists():
            registry_file.unlink()
            deleted_items.append("doc_registry.json")

        return {
            "status": "success",
            "message": "ç³»ç»Ÿå·²å®Œå…¨é‡ç½®",
            "backup": backup_result,
            "deleted": deleted_items,
            "freed_mb": round(freed_space_mb, 2)
        }

    except Exception as e:
        print(f"âŒ ç³»ç»Ÿé‡ç½®å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))
