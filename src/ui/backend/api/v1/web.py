"""Webå†…å®¹å¤„ç†ç›¸å…³APIç«¯ç‚¹"""

import sys
import logging
import asyncio
from pathlib import Path
from typing import Optional
from urllib.parse import urlparse
from fastapi import APIRouter, HTTPException, Depends, Body
from pydantic import BaseModel

# æ·»åŠ é¡¹ç›®æ ¹è·¯å¾„åˆ°sys.path
PROJECT_ROOT = Path(__file__).resolve().parents[5]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from ...config import get_logger, settings
from ...services.chat_service import chat_service

router = APIRouter(prefix="/web", tags=["Web"])
logger = get_logger(__name__)


class WebProcessRequest(BaseModel):
    url: str
    save_outputs: bool = True


@router.post("/process")
async def process_web_url(request: WebProcessRequest):
    """å¤„ç†ç½‘é¡µURL"""
    try:
        url = request.url.strip()

        if not url or not (url.startswith('http://') or url.startswith('https://')):
            raise HTTPException(status_code=400, detail="è¯·è¾“å…¥æœ‰æ•ˆçš„URL")

        logger.info(f"ğŸŒ å¼€å§‹å¤„ç†ç½‘é¡µ: {url}")

        # å¯¼å…¥WebReader
        from src.readers.web import WebReader
        from src.utils.helpers import extract_name_from_url

        # ç”Ÿæˆæ–‡æ¡£åï¼ˆåŸºäºURLï¼‰
        doc_name = extract_name_from_url(url)
        logger.info(f"ğŸ“ ç”Ÿæˆæ–‡æ¡£å: {doc_name}")

        # åˆå§‹åŒ–WebReaderï¼ˆä½¿ç”¨settingsä¸­é…ç½®çš„providerï¼Œé»˜è®¤ä¸ºopenaiï¼‰
        provider = getattr(settings, 'llm_provider', 'openai')
        web_reader = WebReader(provider=provider)

        # å¤„ç†ç½‘é¡µå†…å®¹ï¼ˆå¼‚æ­¥è°ƒç”¨ï¼‰
        await web_reader.process_web(url, save_data_flag=request.save_outputs)

        logger.info(f"âœ… ç½‘é¡µå†…å®¹å¤„ç†å®Œæˆ: {url} -> {doc_name}")

        # æ£€æŸ¥ç”Ÿæˆçš„æ–‡ä»¶
        json_path = settings.data_dir / "json_data" / f"{doc_name}.json"
        has_json = json_path.exists()

        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æ‘˜è¦æ–‡ä»¶
        output_path = settings.data_dir / "output" / doc_name
        has_summary = output_path.exists()

        # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†å‘é‡æ•°æ®åº“
        vector_db_path = settings.data_dir / "vector_db" / f"{doc_name}_vector_db"
        has_vector_db = vector_db_path.exists()

        return {
            "status": "success",
            "message": f"ç½‘é¡µå†…å®¹å¤„ç†å®Œæˆ: {url}",
            "doc_name": doc_name,
            "url": url,
            "save_outputs": request.save_outputs,
            "files_generated": {
                "json_data": has_json,
                "summary": has_summary,
                "vector_db": has_vector_db
            }
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"âŒ å¤„ç†ç½‘é¡µå†…å®¹å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"å¤„ç†ç½‘é¡µå†…å®¹å¤±è´¥: {str(e)}")


@router.get("/summary/{doc_name}")
async def get_web_summary(doc_name: str, summary_type: str = "brief"):
    """è·å–Webæ‘˜è¦ - æ‰«æç›®å½•æŸ¥æ‰¾ä»»æ„ .md æ–‡ä»¶"""
    try:
        import glob

        # Web å†…å®¹å­˜å‚¨åœ¨ output/<doc_name>/ ç›®å½•ä¸‹
        doc_output_dir = settings.data_dir / "output" / doc_name

        # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
        if not doc_output_dir.exists():
            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å‘é‡æ•°æ®åº“ï¼ˆå¤§æ–‡ä»¶æ¨¡å¼ï¼‰
            vector_db_path = settings.data_dir / "vector_db" / f"{doc_name}_vector_db"
            if vector_db_path.exists():
                return {
                    "status": "not_ready",
                    "message": f"è¯¥ç½‘é¡µå†…å®¹è¾ƒå¤§ï¼Œå·²ä½¿ç”¨å‘é‡æ•°æ®åº“å­˜å‚¨ï¼Œæš‚ä¸æ”¯æŒæ‘˜è¦æ˜¾ç¤ºã€‚è¯·ç›´æ¥ä½¿ç”¨èŠå¤©åŠŸèƒ½è¿›è¡Œé—®ç­”ã€‚",
                    "content": "",
                    "is_large_file": True
                }
            else:
                return {
                    "status": "not_ready",
                    "message": f"æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨ï¼Œè¯·å…ˆå¤„ç†ç½‘é¡µå†…å®¹",
                    "content": "",
                    "is_large_file": False
                }

        # æ‰«æç›®å½•ä¸‹çš„æ‰€æœ‰ .md æ–‡ä»¶
        md_files = list(doc_output_dir.glob("*.md"))

        if not md_files:
            return {
                "status": "not_ready",
                "message": f"æ‘˜è¦æ–‡ä»¶å°šæœªç”Ÿæˆï¼Œè¯·ç­‰å¾…å¤„ç†å®Œæˆ",
                "content": "",
                "is_large_file": False
            }

        # ä¼˜å…ˆçº§åŒ¹é…ï¼šæ ¹æ® summary_type é€‰æ‹©æ–‡ä»¶
        summary_file = None

        if summary_type == "brief":
            # ä¼˜å…ˆæŸ¥æ‰¾ brief_summary.md æˆ– summary.md
            for pattern in ["brief_summary.md", "summary.md"]:
                for f in md_files:
                    if f.name == pattern:
                        summary_file = f
                        break
                if summary_file:
                    break
        else:
            # æŸ¥æ‰¾ detail_summary.md
            for f in md_files:
                if f.name == "detail_summary.md":
                    summary_file = f
                    break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª .md æ–‡ä»¶
        if not summary_file and md_files:
            summary_file = md_files[0]
            logger.info(f"æœªæ‰¾åˆ°ç‰¹å®šæ‘˜è¦æ–‡ä»¶ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ª .md æ–‡ä»¶: {summary_file.name}")

        if not summary_file or not summary_file.exists():
            return {
                "status": "not_ready",
                "message": f"æ‘˜è¦æ–‡ä»¶ä¸å¯ç”¨",
                "content": "",
                "is_large_file": False
            }

        # è¯»å–æ‘˜è¦å†…å®¹
        content = summary_file.read_text(encoding='utf-8')
        
        # ğŸ”¥ ä¿®å¤ï¼šå»é™¤ LLM ç”Ÿæˆçš„ä»£ç å—åŒ…è£¹ç¬¦å·
        # æŸäº› LLM ä¼šå°†æ•´ä¸ª Markdown å†…å®¹åŒ…è£¹åœ¨ ```markdown``` æˆ– ``` ä¸­
        content = content.strip()
        if content.startswith('```'):
            # å»é™¤å¼€å¤´çš„ ``` æˆ– ```markdown
            lines = content.split('\n')
            if lines[0].strip().startswith('```'):
                lines = lines[1:]  # å»æ‰ç¬¬ä¸€è¡Œ
            # å»é™¤ç»“å°¾çš„ ```
            if lines and lines[-1].strip() == '```':
                lines = lines[:-1]  # å»æ‰æœ€åä¸€è¡Œ
            content = '\n'.join(lines)

        logger.info(f"âœ… æˆåŠŸåŠ è½½Webæ‘˜è¦: {summary_file.name}")

        return {
            "status": "success",
            "summary_type": summary_type,
            "content": content,
            "file": str(summary_file),
            "file_name": summary_file.name
        }

    except Exception as e:
        logger.error(f"âŒ è·å–Webæ‘˜è¦å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"è·å–Webæ‘˜è¦å¤±è´¥: {str(e)}")


class WebInitializeRequest(BaseModel):
    url: Optional[str] = None


@router.post("/initialize/{doc_name}")
async def initialize_web_reader(
    doc_name: str, 
    request: WebInitializeRequest = Body(default=None)
):
    """åˆå§‹åŒ–Webé˜…è¯»å™¨ï¼ˆç”¨äºèŠå¤©æœåŠ¡ï¼‰"""
    try:
        # æå– URLï¼ˆå¦‚æœæä¾›ï¼‰
        url = request.url if request and request.url else None

        # ğŸ”¥ å‘åå…¼å®¹ï¼šæ£€æŸ¥å¤šç§å¯èƒ½çš„æ–‡ä»¶åæ ¼å¼
        json_data_dir = settings.data_dir / "json_data"
        json_path = json_data_dir / f"{doc_name}.json"
        
        # å¦‚æœæ ‡å‡†æ–‡ä»¶åä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾åŒ…å«ç‰¹æ®Šå­—ç¬¦çš„æ—§æ–‡ä»¶å
        if not json_path.exists():
            logger.warning(f"æ ‡å‡†æ–‡ä»¶åä¸å­˜åœ¨: {json_path.name}")
            logger.info(f"ğŸ” å°è¯•åœ¨ {json_data_dir} ä¸­æŸ¥æ‰¾åŒ¹é…çš„æ–‡ä»¶...")
            
            # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½åŒ¹é…çš„ JSON æ–‡ä»¶
            if json_data_dir.exists():
                # è§„èŒƒåŒ– doc_name ç”¨äºæ¯”è¾ƒ
                doc_name_normalized = doc_name.replace(' ', '').lower()
                
                for candidate in json_data_dir.glob("*.json"):
                    # è§„èŒƒåŒ–å€™é€‰æ–‡ä»¶åç”¨äºæ¯”è¾ƒ
                    candidate_normalized = candidate.stem.replace(' ', '').lower()
                    
                    # å¦‚æœå€™é€‰æ–‡ä»¶åä»¥ doc_name å¼€å¤´ï¼ˆå¿½ç•¥ç‰¹æ®Šå­—ç¬¦ï¼‰
                    if candidate_normalized.startswith(doc_name_normalized):
                        json_path = candidate
                        logger.info(f"âœ… æ‰¾åˆ°åŒ¹é…æ–‡ä»¶: {json_path.name}")
                        # æ›´æ–° doc_name ä¸ºå®é™…æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
                        doc_name = candidate.stem
                        break
        
        if not json_path.exists():
            logger.warning(f"âš ï¸ JSONæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•åˆå§‹åŒ–Webé˜…è¯»å™¨: {doc_name}")
            return {
                "status": "needs_processing",
                "message": f"Webå†…å®¹ {doc_name} éœ€è¦é‡æ–°å¤„ç†",
                "doc_name": doc_name,
                "has_json": False
            }

        # åˆå§‹åŒ–èŠå¤©æœåŠ¡
        logger.info(f"ğŸ”„ æ­£åœ¨åˆå§‹åŒ–WebèŠå¤©æœåŠ¡: {doc_name}")
        logger.info(f"ğŸ“Š åˆå§‹åŒ–å‰ChatServiceçŠ¶æ€: {chat_service.get_status()}")

        # è°ƒç”¨å¼‚æ­¥åˆå§‹åŒ–æ–¹æ³•
        success = await chat_service.initialize_web_reader(doc_name, url=url, provider="openai")

        logger.info(f"ğŸ“Š åˆå§‹åŒ–åChatServiceçŠ¶æ€: {chat_service.get_status()}")

        if success:
            logger.info(f"âœ… Webé˜…è¯»å™¨å’ŒèŠå¤©æœåŠ¡åˆå§‹åŒ–æˆåŠŸ: {doc_name}")
            return {
                "status": "success",
                "message": f"Webé˜…è¯»å™¨å·²åˆå§‹åŒ–: {doc_name}",
                "doc_name": doc_name,
                "has_json": True,
                "chat_initialized": True
            }
        else:
            logger.warning(f"âš ï¸ WebèŠå¤©æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {doc_name}")
            return {
                "status": "partial_success",
                "message": f"Webå†…å®¹æ£€æŸ¥å®Œæˆï¼Œä½†èŠå¤©æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {doc_name}",
                "doc_name": doc_name,
                "has_json": True,
                "chat_initialized": False
            }

    except Exception as e:
        logger.error(f"âŒ åˆå§‹åŒ–Webé˜…è¯»å™¨å¤±è´¥: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"åˆå§‹åŒ–Webé˜…è¯»å™¨å¤±è´¥: {str(e)}")