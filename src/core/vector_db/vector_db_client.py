import logging
import os
import hashlib
from typing import List, Dict, Optional, Any, Callable, Set

from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS

# æ³¨æ„ï¼šä¸å†ç»§æ‰¿ LLMBaseï¼Œæ”¹ç”¨ç»„åˆæ¨¡å¼ï¼ˆä¾èµ–æ³¨å…¥ï¼‰

logging.basicConfig(
    level=logging.INFO,  # å¯æ ¹æ®éœ€è¦æ”¹ä¸º DEBUG
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)

class VectorDBClient:
    """
    å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯ï¼ˆçº¯æ•°æ®è®¿é—®å±‚ï¼‰

    èŒè´£ï¼š
    - FAISSå‘é‡æ•°æ®åº“çš„CRUDæ“ä½œ
    - å…ƒæ•°æ®è¿‡æ»¤æ£€ç´¢
    - å»é‡æœºåˆ¶
    - æ•°æ®æŒä¹…åŒ–

    ä½¿ç”¨ç»„åˆæ¨¡å¼ï¼Œé€šè¿‡ä¾èµ–æ³¨å…¥è·å– embedding_model
    """
    def __init__(self, db_path: str, embedding_model) -> None:
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“å®¢æˆ·ç«¯

        Args:
            db_path: å‘é‡æ•°æ®åº“å­˜å‚¨è·¯å¾„
            embedding_model: å¤–éƒ¨ä¼ å…¥çš„ embedding æ¨¡å‹å®ä¾‹
                           ï¼ˆé€šå¸¸æ¥è‡ª LLMBase.embedding_modelï¼‰
        """
        self.db_path = db_path
        self.embedding_model = embedding_model  # ç»„åˆæ¨¡å¼ï¼Œæ³¨å…¥ä¾èµ–
        self.vector_db: Optional[FAISS] = None

        # ç”¨äºå­˜å‚¨å·²æ£€ç´¢æ–‡æ¡£çš„å“ˆå¸Œå€¼ï¼Œé˜²æ­¢é‡å¤æ£€ç´¢
        self._retrieved_doc_hashes: Set[str] = set()

        # å°è¯•è‡ªåŠ¨åŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“
        # æ£€æŸ¥ index.faiss æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼ˆè€Œä¸ä»…ä»…æ˜¯ç›®å½•ï¼‰
        index_file = os.path.join(db_path, "index.faiss")
        if os.path.exists(index_file):
            try:
                self.load_vector_db()
                logger.info(f"âœ… æˆåŠŸåŠ è½½å·²å­˜åœ¨çš„å‘é‡æ•°æ®åº“: {db_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥ï¼ˆå¯èƒ½å·²æŸåï¼‰: {e}")
                self.vector_db = None
        elif os.path.exists(db_path):
            logger.debug(f"ğŸ“ å‘é‡æ•°æ®åº“ç›®å½•å­˜åœ¨ä½†ç´¢å¼•æ–‡ä»¶æœªåˆ›å»º: {db_path}")

    def build_vector_db(self, content_docs: List[Document]) -> FAISS:
        """
        æ„å»ºå‘é‡æ•°æ®åº“ã€‚
        Build a vector database from document list.
        Args:
            content_docs (List[Document]): æ–‡æ¡£åˆ—è¡¨ã€‚
        Returns:
            FAISS: æ„å»ºçš„å‘é‡æ•°æ®åº“å¯¹è±¡ã€‚
        """
        logger.info(f"å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“ï¼Œæ–‡æ¡£æ•°: {len(content_docs)}")
        self.vector_db = FAISS.from_documents(content_docs, self.embedding_model)
        self.vector_db.save_local(self.db_path)
        logger.info(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜åˆ°: {self.db_path}")
 
    def load_vector_db(self) -> FAISS:
        """
        åŠ è½½æœ¬åœ°å‘é‡æ•°æ®åº“ã€‚
        Load local vector database.
        Args:
            vector_db_path: å‘é‡æ•°æ®åº“è·¯å¾„ã€‚
        Returns:
            FAISS: åŠ è½½çš„å‘é‡æ•°æ®åº“å¯¹è±¡ã€‚
        """
        logger.info(f"åŠ è½½æœ¬åœ°å‘é‡æ•°æ®åº“: {self.db_path}")
        try:
            self.vector_db = FAISS.load_local(self.db_path, self.embedding_model, allow_dangerous_deserialization=True)

            # éªŒè¯embeddingç»´åº¦æ˜¯å¦åŒ¹é…
            if hasattr(self.vector_db, 'index') and hasattr(self.vector_db.index, 'd'):
                stored_dim = self.vector_db.index.d
                # è·å–å½“å‰embeddingæ¨¡å‹çš„ç»´åº¦
                test_embedding = self.embedding_model.embed_query("test")
                current_dim = len(test_embedding)

                if stored_dim != current_dim:
                    error_msg = (
                        f"âŒ Embeddingç»´åº¦ä¸åŒ¹é…ï¼\n"
                        f"å‘é‡æ•°æ®åº“ç»´åº¦: {stored_dim}\n"
                        f"å½“å‰Embeddingæ¨¡å‹ç»´åº¦: {current_dim}\n"
                        f"æ•°æ®åº“è·¯å¾„: {self.db_path}\n\n"
                        f"å¯èƒ½åŸå› ï¼š\n"
                        f"1. ç´¢å¼•æ—¶ä½¿ç”¨çš„embeddingæ¨¡å‹ä¸å½“å‰é…ç½®çš„æ¨¡å‹ä¸åŒ\n"
                        f"2. embeddingæ¨¡å‹é…ç½®å·²æ›´æ”¹\n\n"
                        f"è§£å†³æ–¹æ¡ˆï¼š\n"
                        f"1. ä½¿ç”¨ä¸ç´¢å¼•æ—¶ç›¸åŒçš„embeddingæ¨¡å‹é…ç½®\n"
                        f"2. åˆ é™¤æ—§çš„å‘é‡æ•°æ®åº“å¹¶é‡æ–°ç´¢å¼•æ–‡æ¡£\n"
                        f"   åˆ é™¤è·¯å¾„: {self.db_path}\n"
                        f"   é‡æ–°è¿è¡Œç´¢å¼•å‘½ä»¤"
                    )
                    logger.error(error_msg)
                    raise ValueError(error_msg)

                logger.info(f"âœ… Embeddingç»´åº¦éªŒè¯é€šè¿‡: {current_dim}")
        except ValueError:
            # é‡æ–°æŠ›å‡ºç»´åº¦ä¸åŒ¹é…é”™è¯¯
            raise
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å‘é‡æ•°æ®åº“å¤±è´¥: {e}")
            raise

    def add_data(self, vector_db, data_docs):
        """
        å‘ç°æœ‰å‘é‡æ•°æ®åº“æ·»åŠ æ–°æ–‡æ¡£

        Args:
            vector_db (FAISS): å‘é‡æ•°æ®åº“å®ä¾‹
            data_docs: è¦æ·»åŠ çš„æ–‡æ¡£åˆ—è¡¨
        """
        vector_db.add_documents(data_docs)
        vector_db.save_local(self.db_path)
        # å¦‚æœä¼ å…¥çš„æ˜¯å½“å‰å®ä¾‹çš„ vector_dbï¼Œä¿æŒåŒæ­¥
        if vector_db is self.vector_db:
            logger.info(f"å·²æ·»åŠ  {len(data_docs)} ä¸ªæ–‡æ¡£åˆ°å‘é‡æ•°æ®åº“")

    def make_metadata_filter(self, field_name: str, target_value: str) -> Callable[[Dict[str, Any]], bool]:
        """
        åˆ›å»ºå…ƒæ•°æ®è¿‡æ»¤å‡½æ•°

        ç”Ÿæˆä¸€ä¸ªç”¨äºå‘é‡æ•°æ®åº“æ£€ç´¢çš„å…ƒæ•°æ®è¿‡æ»¤å‡½æ•°ï¼Œ
        è¯¥å‡½æ•°æ£€æŸ¥æŒ‡å®šå­—æ®µæ˜¯å¦åŒ…å«ç›®æ ‡å€¼ã€‚

        Args:
            field_name (str): è¦è¿‡æ»¤çš„å…ƒæ•°æ®å­—æ®µå (å¦‚ "pdf_name", "location", "person")
            target_value (str): ç›®æ ‡å€¼

        Returns:
            Callable: å…ƒæ•°æ®è¿‡æ»¤å‡½æ•°ï¼Œæ¥å—metadataå‚æ•°ï¼Œè¿”å›boolå€¼

        ä½¿ç”¨ç¤ºä¾‹:
        ```python
        filter_func = client.make_metadata_filter("pdf_name", "research_paper")
        results = vector_db.similarity_search_with_score(
            query, k=5, filter=filter_func
        )
        ```
        """
        def metadata_filter(metadata: Dict[str, Any]) -> bool:
            field_data = metadata.get(field_name, [])

            # å¤„ç†å­—ç¬¦ä¸²å’Œåˆ—è¡¨ä¸¤ç§æ ¼å¼
            if isinstance(target_value, str):
                # å•ä¸ªç›®æ ‡å€¼
                if isinstance(field_data, str):
                    return target_value == field_data
                elif isinstance(field_data, list):
                    return target_value in field_data
                else:
                    return False

            elif isinstance(target_value, list):
                # å¤šä¸ªç›®æ ‡å€¼ï¼Œåªè¦åŒ¹é…å…¶ä¸­ä¸€ä¸ªå³å¯
                if isinstance(field_data, str):
                    return field_data in target_value
                elif isinstance(field_data, list):
                    return any(item in field_data for item in target_value)
                else:
                    return False
            else:
                return False

        logger.debug(f"åˆ›å»ºå…ƒæ•°æ®è¿‡æ»¤å™¨ - å­—æ®µ: {field_name}, ç›®æ ‡å€¼: {target_value}")
        return metadata_filter

    def make_dedup_filter(self) -> Callable[[Dict[str, Any]], bool]:
        """
        åˆ›å»ºå»é‡è¿‡æ»¤å‡½æ•°

        ç”Ÿæˆä¸€ä¸ªç”¨äºå‘é‡æ•°æ®åº“æ£€ç´¢çš„å»é‡è¿‡æ»¤å‡½æ•°ï¼Œ
        è¯¥å‡½æ•°æ£€æŸ¥æ–‡æ¡£å†…å®¹æ˜¯å¦å·²è¢«æ£€ç´¢è¿‡ã€‚

        Returns:
            Callable: å»é‡è¿‡æ»¤å‡½æ•°ï¼Œæ¥å—metadataå‚æ•°ï¼Œè¿”å›boolå€¼

        ä½¿ç”¨ç¤ºä¾‹:
        ```python
        dedup_filter = client.make_dedup_filter()
        results = vector_db.similarity_search_with_score(
            query, k=5, filter=dedup_filter
        )
        ```
        """
        def dedup_filter(metadata: Dict[str, Any]) -> bool:
            # è·å–æ–‡æ¡£å†…å®¹
            refactor_data = metadata.get("refactor", "")

            # å¦‚æœå†…å®¹ä¸ºç©ºï¼Œå…è®¸é€šè¿‡
            if not refactor_data:
                return True

            # æ£€æŸ¥æ˜¯å¦å·²æ£€ç´¢è¿‡
            return not self.is_document_retrieved(refactor_data)

        logger.debug("åˆ›å»ºå»é‡è¿‡æ»¤å™¨")
        return dedup_filter

    def combine_filters(self, *filters: Callable[[Dict[str, Any]], bool]) -> Callable[[Dict[str, Any]], bool]:
        """
        ç»„åˆå¤šä¸ªè¿‡æ»¤å‡½æ•°

        å°†å¤šä¸ªè¿‡æ»¤å‡½æ•°ç»„åˆæˆä¸€ä¸ªï¼Œæ‰€æœ‰è¿‡æ»¤æ¡ä»¶å¿…é¡»åŒæ—¶æ»¡è¶³ï¼ˆAND é€»è¾‘ï¼‰ã€‚

        Args:
            *filters: å¤šä¸ªè¿‡æ»¤å‡½æ•°

        Returns:
            Callable: ç»„åˆåçš„è¿‡æ»¤å‡½æ•°

        ä½¿ç”¨ç¤ºä¾‹:
        ```python
        metadata_filter = client.make_metadata_filter("type", "context")
        dedup_filter = client.make_dedup_filter()
        combined_filter = client.combine_filters(metadata_filter, dedup_filter)
        results = vector_db.similarity_search_with_score(
            query, k=5, filter=combined_filter
        )
        ```
        """
        def combined_filter(metadata: Dict[str, Any]) -> bool:
            # æ‰€æœ‰è¿‡æ»¤å™¨éƒ½å¿…é¡»é€šè¿‡
            return all(f(metadata) for f in filters)

        logger.debug(f"ç»„åˆäº† {len(filters)} ä¸ªè¿‡æ»¤å™¨")
        return combined_filter

    def search_with_metadata_filter(
        self,
        query: str,
        k: int = 5,
        field_name: Optional[str] = None,
        field_value: Optional[Any] = None,
        fetch_k: Optional[int] = None,
        enable_dedup: bool = True
    ) -> List[tuple]:
        """
        ä½¿ç”¨å…ƒæ•°æ®è¿‡æ»¤è¿›è¡Œå‘é‡æ£€ç´¢

        Args:
            query (str): æœç´¢æŸ¥è¯¢
            k (int): è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤5
            field_name (str, optional): å­—æ®µå
            field_value (Any, optional): å­—æ®µå€¼
            fetch_k (int, optional): è¿‡æ»¤å‰è·å–çš„æ–‡æ¡£æ•°é‡ã€‚å¦‚æœä½¿ç”¨è¿‡æ»¤å™¨ä½†æœªæŒ‡å®šï¼Œ
                                     é»˜è®¤ä¸º k*4 å’Œ 100 ä¸­çš„è¾ƒå¤§å€¼ï¼Œä»¥ç¡®ä¿æœ‰è¶³å¤Ÿçš„å€™é€‰æ–‡æ¡£
            enable_dedup (bool): æ˜¯å¦å¯ç”¨å»é‡è¿‡æ»¤ï¼Œé»˜è®¤ True

        Returns:
            List[tuple]: æ£€ç´¢ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º (Document, score) æ ¼å¼

        ä½¿ç”¨ç¤ºä¾‹:
        ```python
        results = client.search_with_metadata_filter(
            query="æŸ¥è¯¢å†…å®¹", k=5,
            field_name="type", field_value="title"
        )
        ```
        """
        if self.vector_db is None:
            raise ValueError("å‘é‡æ•°æ®åº“æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_vector_db() æˆ– build_vector_db()")

        try:
            # æ„å»ºè¿‡æ»¤å™¨åˆ—è¡¨
            filters = []

            # æ·»åŠ å…ƒæ•°æ®è¿‡æ»¤å™¨
            if field_name and field_value is not None:
                metadata_filter = self.make_metadata_filter(field_name, field_value)
                filters.append(metadata_filter)

            # æ·»åŠ å»é‡è¿‡æ»¤å™¨
            if enable_dedup:
                dedup_filter = self.make_dedup_filter()
                filters.append(dedup_filter)

            # æ ¹æ®æ˜¯å¦æœ‰è¿‡æ»¤å™¨å†³å®šæ£€ç´¢ç­–ç•¥
            if not filters:
                # æ²¡æœ‰è¿‡æ»¤æ¡ä»¶ï¼Œæ‰§è¡Œæ™®é€šæ£€ç´¢
                logger.debug("æ‰§è¡Œæ™®é€šå‘é‡æ£€ç´¢ï¼ˆæ— è¿‡æ»¤æ¡ä»¶ï¼‰")
                results = self.vector_db.similarity_search_with_score(query, k=k)
            else:
                # ç»„åˆæ‰€æœ‰è¿‡æ»¤å™¨
                combined_filter = self.combine_filters(*filters)

                # å¦‚æœä½¿ç”¨è¿‡æ»¤å™¨ä½†æœªæŒ‡å®š fetch_kï¼Œè®¾ç½®ä¸€ä¸ªè¶³å¤Ÿå¤§çš„é»˜è®¤å€¼
                # å› ä¸ºå¯ç”¨äº†å»é‡ï¼Œéœ€è¦æ›´å¤§çš„å€™é€‰æ± 
                if fetch_k is None:
                    # æ ¹æ®æ˜¯å¦å¯ç”¨å»é‡è°ƒæ•´ fetch_k
                    multiplier = 8 if enable_dedup else 4
                    fetch_k = max(k * multiplier, 100)
                    logger.debug(f"è‡ªåŠ¨è®¾ç½® fetch_k={fetch_k} (k={k}, å»é‡={'å¯ç”¨' if enable_dedup else 'ç¦ç”¨'})")

                filter_desc = []
                if field_name:
                    filter_desc.append(f"{field_name}={field_value}")
                if enable_dedup:
                    filter_desc.append("å»é‡")
                logger.info(f"æ‰§è¡Œè¿‡æ»¤æ£€ç´¢ - æ¡ä»¶: {', '.join(filter_desc)}, fetch_k={fetch_k}")

                # æ‰§è¡Œå¸¦è¿‡æ»¤çš„æ£€ç´¢
                results = self.vector_db.similarity_search_with_score(
                    query, k=k, filter=combined_filter, fetch_k=fetch_k
                )

            # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£æ ‡è®°ä¸ºå·²æ£€ç´¢
            if enable_dedup and results:
                for doc_item in results:
                    document = doc_item[0] if isinstance(doc_item, tuple) else doc_item
                    refactor_data = document.metadata.get("refactor", "")
                    if refactor_data:
                        self.mark_document_as_retrieved(refactor_data)

            logger.info(f"è¿‡æ»¤æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
            return results

        except AssertionError as e:
            # FAISS ç»´åº¦ä¸åŒ¹é…é”™è¯¯
            if hasattr(self.vector_db, 'index') and hasattr(self.vector_db.index, 'd'):
                stored_dim = self.vector_db.index.d
                test_embedding = self.embedding_model.embed_query("test")
                current_dim = len(test_embedding)

                error_msg = (
                    f"âŒ Embeddingç»´åº¦ä¸åŒ¹é…ï¼\n"
                    f"å‘é‡æ•°æ®åº“ç»´åº¦: {stored_dim}\n"
                    f"å½“å‰Embeddingæ¨¡å‹ç»´åº¦: {current_dim}\n"
                    f"æ•°æ®åº“è·¯å¾„: {self.db_path}\n\n"
                    f"è¯·åˆ é™¤æ—§çš„å‘é‡æ•°æ®åº“å¹¶é‡æ–°ç´¢å¼•æ–‡æ¡£ï¼Œæˆ–ä½¿ç”¨ä¸ç´¢å¼•æ—¶ç›¸åŒçš„embeddingæ¨¡å‹ã€‚"
                )
                logger.error(error_msg)
                raise ValueError(error_msg) from e
            else:
                raise

        except Exception as e:
            logger.error(f"å…ƒæ•°æ®è¿‡æ»¤æ£€ç´¢å¤±è´¥: {e}")
            # å¦‚æœæ˜¯ç»´åº¦ä¸åŒ¹é…ç­‰ä¸¥é‡é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
            if isinstance(e, (ValueError, AssertionError)):
                raise
            # å…¶ä»–é”™è¯¯æ—¶æ‰§è¡Œæ™®é€šæ£€ç´¢ä½œä¸ºå¤‡é€‰
            logger.info("æ‰§è¡Œå¤‡é€‰æ™®é€šæ£€ç´¢")
            try:
                return self.vector_db.similarity_search_with_score(query, k=k)
            except AssertionError as ae:
                # å¤‡é€‰æ£€ç´¢ä¹Ÿå¤±è´¥ï¼Œè¯´æ˜æ˜¯ç»´åº¦é—®é¢˜
                if hasattr(self.vector_db, 'index') and hasattr(self.vector_db.index, 'd'):
                    stored_dim = self.vector_db.index.d
                    test_embedding = self.embedding_model.embed_query("test")
                    current_dim = len(test_embedding)

                    error_msg = (
                        f"âŒ Embeddingç»´åº¦ä¸åŒ¹é…ï¼\n"
                        f"å‘é‡æ•°æ®åº“ç»´åº¦: {stored_dim}\n"
                        f"å½“å‰Embeddingæ¨¡å‹ç»´åº¦: {current_dim}\n"
                        f"æ•°æ®åº“è·¯å¾„: {self.db_path}\n\n"
                        f"è¯·åˆ é™¤æ—§çš„å‘é‡æ•°æ®åº“å¹¶é‡æ–°ç´¢å¼•æ–‡æ¡£ï¼Œæˆ–ä½¿ç”¨ä¸ç´¢å¼•æ—¶ç›¸åŒçš„embeddingæ¨¡å‹ã€‚"
                    )
                    logger.error(error_msg)
                    raise ValueError(error_msg) from ae
                else:
                    raise

    def search_by_pdf_name(
        self,
        query: str,
        pdf_name: str,
        k: int = 99,
        fetch_k: Optional[int] = None,
        enable_dedup: bool = True
    ) -> List[tuple]:
        """
        æŒ‰PDFåç§°è¿‡æ»¤è¿›è¡Œæ£€ç´¢çš„ä¾¿åˆ©æ–¹æ³•

        Args:
            query (str): æœç´¢æŸ¥è¯¢
            pdf_name (str): PDFæ–‡æ¡£åç§°
            k (int): è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤99
            fetch_k (int, optional): è¿‡æ»¤å‰è·å–çš„æ–‡æ¡£æ•°é‡
            enable_dedup (bool): æ˜¯å¦å¯ç”¨å»é‡è¿‡æ»¤ï¼Œé»˜è®¤ True

        Returns:
            List[tuple]: æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        return self.search_with_metadata_filter(
            query=query, k=k,
            field_name="pdf_name", field_value=pdf_name,
            fetch_k=fetch_k,
            enable_dedup=enable_dedup
        )

    def search_by_title(
        self,
        title: str,
        doc_type: str = "title",
        k: int = 1,
        fetch_k: Optional[int] = None,
        enable_dedup: bool = True
    ) -> List[tuple]:
        """
        æŒ‰æ ‡é¢˜æ£€ç´¢æ–‡æ¡£çš„ä¾¿åˆ©æ–¹æ³•

        Args:
            title (str): æ ‡é¢˜ï¼ˆç”¨ä½œæœç´¢æŸ¥è¯¢ï¼‰
            doc_type (str): æ–‡æ¡£ç±»å‹ï¼Œé»˜è®¤ä¸º "title"
            k (int): è¿”å›ç»“æœæ•°é‡ï¼Œé»˜è®¤99
            fetch_k (int, optional): è¿‡æ»¤å‰è·å–çš„æ–‡æ¡£æ•°é‡ã€‚å¦‚æœæœªæŒ‡å®šï¼Œ
                                     é»˜è®¤ä¸º k*4 å’Œ 100 ä¸­çš„è¾ƒå¤§å€¼
            enable_dedup (bool): æ˜¯å¦å¯ç”¨å»é‡è¿‡æ»¤ï¼Œé»˜è®¤ True

        Returns:
            List[tuple]: æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        return self.search_with_metadata_filter(
            query=title, k=k,
            field_name="type", field_value=doc_type,
            fetch_k=fetch_k,
            enable_dedup=enable_dedup
        )

    def _compute_document_hash(self, content: str) -> str:
        """
        è®¡ç®—æ–‡æ¡£å†…å®¹çš„å“ˆå¸Œå€¼

        ä½¿ç”¨ SHA256 å¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œå“ˆå¸Œï¼Œç”¨äºå»é‡æ£€æµ‹ã€‚

        Args:
            content (str): æ–‡æ¡£å†…å®¹

        Returns:
            str: åå…­è¿›åˆ¶æ ¼å¼çš„å“ˆå¸Œå€¼
        """
        if not content:
            return ""
        return hashlib.sha256(content.encode('utf-8')).hexdigest()

    def is_document_retrieved(self, content: str) -> bool:
        """
        æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å·²è¢«æ£€ç´¢è¿‡

        Args:
            content (str): æ–‡æ¡£å†…å®¹

        Returns:
            bool: True è¡¨ç¤ºå·²æ£€ç´¢è¿‡ï¼ŒFalse è¡¨ç¤ºæœªæ£€ç´¢è¿‡
        """
        if not content:
            return False
        doc_hash = self._compute_document_hash(content)
        return doc_hash in self._retrieved_doc_hashes

    def mark_document_as_retrieved(self, content: str) -> None:
        """
        å°†æ–‡æ¡£æ ‡è®°ä¸ºå·²æ£€ç´¢

        Args:
            content (str): æ–‡æ¡£å†…å®¹
        """
        if content:
            doc_hash = self._compute_document_hash(content)
            self._retrieved_doc_hashes.add(doc_hash)
            logger.debug(f"æ–‡æ¡£å·²æ ‡è®°ä¸ºå·²æ£€ç´¢ (hash: {doc_hash[:8]}...)")

    def reset_retrieval_history(self) -> None:
        """
        é‡ç½®æ£€ç´¢å†å²

        æ¸…é™¤æ‰€æœ‰å·²æ£€ç´¢æ–‡æ¡£çš„å“ˆå¸Œè®°å½•ï¼Œå…è®¸åœ¨æ–°çš„æŸ¥è¯¢ä¼šè¯ä¸­é‡æ–°æ£€ç´¢ç›¸åŒçš„æ–‡æ¡£ã€‚
        å»ºè®®åœ¨å¼€å§‹æ–°çš„ç”¨æˆ·æŸ¥è¯¢ä¼šè¯æ—¶è°ƒç”¨æ­¤æ–¹æ³•ã€‚
        """
        count = len(self._retrieved_doc_hashes)
        self._retrieved_doc_hashes.clear()
        logger.info(f"âœ… å·²é‡ç½®æ£€ç´¢å†å²ï¼Œæ¸…é™¤äº† {count} ä¸ªæ–‡æ¡£å“ˆå¸Œè®°å½•")

    def get_retrieval_stats(self) -> Dict[str, int]:
        """
        è·å–æ£€ç´¢ç»Ÿè®¡ä¿¡æ¯

        Returns:
            Dict[str, int]: åŒ…å«å·²æ£€ç´¢æ–‡æ¡£æ•°é‡çš„ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            "retrieved_documents_count": len(self._retrieved_doc_hashes)
        }

