"""
client.py - LLM provider and message history management for LLMReader

This module provides classes for managing chat message history with limits, and for abstracting over different LLM providers (Azure, OpenAI, Ollama).

Enhanced Features:
- Tool calling support for MCP integration
- Async operations support
- Enhanced error handling and logging
- Flexible configuration management
"""
import asyncio
import logging
from typing import Any, Optional, List, Dict, Union
from pydantic import Field
from langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings

from abc import ABC, abstractmethod

from src.config.settings import (
    LLM_CONFIG,
    LLM_EMBEDDING_CONFIG,
)
from src.config.prompts import SYSTEM_PROMPT_CONFIG
from src.config.constants import ProcessingLimits, LLMConstants
logging.basicConfig(
    level=logging.INFO,  # å¯æ ¹æ®éœ€è¦æ”¹ä¸º DEBUG
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)


class LimitedChatMessageHistory(InMemoryChatMessageHistory):
    """
    å¸¦æœ‰é™åˆ¶åŠŸèƒ½çš„èŠå¤©æ¶ˆæ¯å†å²è®°å½•ç®¡ç†ç±»

    æ‰©å±•InMemoryChatMessageHistoryï¼Œå¢åŠ ä»¥ä¸‹åŠŸèƒ½ï¼š
    - æ¶ˆæ¯æ•°é‡é™åˆ¶ï¼šé€šè¿‡max_messageså‚æ•°æ§åˆ¶æœ€å¤§æ¶ˆæ¯æ¡æ•°
    - Tokenæ•°é‡é™åˆ¶ï¼šé€šè¿‡max_tokenså‚æ•°æ§åˆ¶æ€»Tokenæ•°ä¸è¶…è¿‡æ¨¡å‹ä¸Šä¸‹æ–‡çª—å£
    - è‡ªåŠ¨æ¸…ç†ï¼šå½“æ¶ˆæ¯æ•°é‡æˆ–Tokenæ•°è¶…å‡ºé™åˆ¶æ—¶ï¼Œè‡ªåŠ¨ç§»é™¤æœ€æ—©çš„æ¶ˆæ¯

    Attributes:
        max_messages (int): æœ€å¤§æ¶ˆæ¯æ•°é‡é™åˆ¶ï¼Œé»˜è®¤ä»ProcessingLimits.DEFAULT_MAX_MESSAGESè·å–
        max_tokens (int): æœ€å¤§Tokenæ•°é‡é™åˆ¶ï¼Œé»˜è®¤ä»ProcessingLimits.DEFAULT_MAX_TOKENSè·å–
        encoding_name (str): Tokenç¼–ç åç§°ï¼Œé»˜è®¤ä»LLMConstants.DEFAULT_ENCODINGè·å–
    """

    # ä½¿ç”¨Pydanticå­—æ®µå®šä¹‰è‡ªå®šä¹‰å±æ€§
    max_messages: int = Field(default_factory=lambda: ProcessingLimits.DEFAULT_MAX_MESSAGES)
    max_tokens: int = Field(default_factory=lambda: ProcessingLimits.DEFAULT_MAX_TOKENS)
    encoding_name: str = Field(default_factory=lambda: LLMConstants.DEFAULT_ENCODING)

    def __init__(self, max_messages: int = None, max_tokens: int = None,
                 encoding_name: str = None, **kwargs):
        """
        åˆå§‹åŒ–é™åˆ¶å‹èŠå¤©æ¶ˆæ¯å†å²

        Args:
            max_messages (int): æœ€å¤§æ¶ˆæ¯æ•°é‡é™åˆ¶
            max_tokens (int): æœ€å¤§Tokenæ•°é‡é™åˆ¶
            encoding_name (str): Tokenç¼–ç åç§°
            **kwargs: ä¼ é€’ç»™çˆ¶ç±»çš„å…¶ä»–å‚æ•°
        """
        # è®¾ç½®è‡ªå®šä¹‰å­—æ®µçš„å€¼
        if max_messages is not None:
            kwargs['max_messages'] = max_messages
        if max_tokens is not None:
            kwargs['max_tokens'] = max_tokens
        if encoding_name is not None:
            kwargs['encoding_name'] = encoding_name
            
        super().__init__(**kwargs)

        logger.debug(f"LimitedChatMessageHistoryåˆå§‹åŒ–: max_messages={self.max_messages}, "
                    f"max_tokens={self.max_tokens}, encoding={self.encoding_name}")

    def _count_tokens(self, message):
        """
        è®¡ç®—å•æ¡æ¶ˆæ¯çš„Tokenæ•°é‡
        Args:
            message: èŠå¤©æ¶ˆæ¯å¯¹è±¡ï¼Œéœ€åŒ…å«contentå±æ€§
        Returns:
            int: æ¶ˆæ¯å†…å®¹çš„Tokenæ•°é‡
        Note:
            ä¼˜å…ˆä½¿ç”¨tiktokenè¿›è¡Œç²¾ç¡®è®¡ç®—ï¼Œå¦‚æœªå®‰è£…åˆ™ä½¿ç”¨å­—ç¬¦æ•°/4è¿›è¡Œä¼°ç®—
        """
        try:
            import tiktoken
            encoding = tiktoken.get_encoding(self.encoding_name)
            if hasattr(message, "content"):
                return len(encoding.encode(message.content))
            else:
                return 0
        except ImportError:
            logger.warning("tiktoken not installed, using rough token estimate.")
            if hasattr(message, "content"):
                return len(message.content) // 4
            else:
                return 0
        except Exception as e:
            logger.error(f"Error counting tokens: {e}")
            return 0

    def _total_tokens(self):
        """è®¡ç®—æ‰€æœ‰æ¶ˆæ¯çš„æ€»Tokenæ•°"""
        return sum(self._count_tokens(m) for m in self.messages)

    def add_message(self, message):
        """
        æ·»åŠ æ¶ˆæ¯åˆ°å†å²ï¼Œå¹¶è‡ªåŠ¨æ ¹æ® max_messages å’Œ max_tokens è¿›è¡Œè£å‰ªã€‚
        """
        super().add_message(message)
        # 1. é™åˆ¶æ¶ˆæ¯æ¡æ•° - ä¿ç•™æœ€æ–°çš„max_messagesæ¡æ¶ˆæ¯
        if len(self.messages) > self.max_messages:
            logger.info(f"[LimitedChatMessageHistory] æ¶ˆæ¯æ•°é‡è¶…å‡ºé™åˆ¶({self.max_messages})ï¼Œå·²æˆªæ–­ã€‚")
            self.messages = self.messages[-self.max_messages:]
        # 2. é™åˆ¶Tokenæ€»æ•° - å¾ªç¯ç§»é™¤æœ€æ—©æ¶ˆæ¯ç›´åˆ°Tokenæ•°è¾¾æ ‡
        while self._total_tokens() > self.max_tokens and len(self.messages) > 1:
            logger.info(f"[LimitedChatMessageHistory] Tokenæ€»æ•°è¶…å‡ºé™åˆ¶({self.max_tokens})ï¼Œç§»é™¤æœ€æ—©æ¶ˆæ¯ã€‚")
            self.messages.pop(0)
    
    def delete_last_message(self):
        """åˆ é™¤æœ€åä¸€æ¡æ¶ˆæ¯"""
        if self.messages:
            removed_message = self.messages.pop()
            logger.info(f"[LimitedChatMessageHistory] åˆ é™¤æœ€åä¸€æ¡æ¶ˆæ¯: {removed_message}")
        else:
            logger.warning("[LimitedChatMessageHistory] æ— æ¶ˆæ¯å¯åˆ é™¤ã€‚")

class LLMProviderBase(ABC):
    """
    LLM Provider æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰ç»Ÿä¸€æ¥å£ã€‚
    """
    @abstractmethod
    def get_chat_model(self, **kwargs):
        pass

    @abstractmethod
    def get_embedding_model(self, **kwargs):
        pass

class AzureLLMProvider(LLMProviderBase):

    def get_chat_model(self, **kwargs):
        return AzureChatOpenAI(
            openai_api_key=kwargs.get("openai_api_key", LLM_CONFIG.get("api_key")),
            openai_api_version=kwargs.get("openai_api_version", LLM_CONFIG.get("api_version")),
            azure_endpoint=kwargs.get("azure_endpoint", LLM_CONFIG.get("azure_endpoint")),
            deployment_name=kwargs.get("deployment_name", LLM_CONFIG.get("deployment_name")),
            model_name=kwargs.get("model_name", LLM_CONFIG.get("model_name")),
            temperature=kwargs.get("temperature", 0.7),
            max_retries=kwargs.get("max_retries", 5)
        )

    def get_embedding_model(self, **kwargs):
        return AzureOpenAIEmbeddings(
            openai_api_key=kwargs.get("openai_api_key", LLM_EMBEDDING_CONFIG.get("api_key")),
            openai_api_version=kwargs.get("openai_api_version", LLM_EMBEDDING_CONFIG.get("api_version")),
            azure_endpoint=kwargs.get("azure_endpoint", LLM_EMBEDDING_CONFIG.get("azure_endpoint")),
            deployment=kwargs.get("deployment", LLM_EMBEDDING_CONFIG.get("deployment")),
            model=kwargs.get("model", LLM_EMBEDDING_CONFIG.get("model")),
            max_retries=kwargs.get("max_retries", 5)
        )

class OpenAILLMProvider(LLMProviderBase):
    def get_chat_model(self, **kwargs):
        return ChatOpenAI(
            model=kwargs.get("model_name", LLM_CONFIG.get("openai_model_name")),
            openai_api_key=kwargs.get("openai_api_key", LLM_CONFIG.get("openai_api_key")),
            base_url=kwargs.get("openai_base_url", LLM_CONFIG.get("openai_base_url")),
            temperature=kwargs.get("temperature", 0.7),
            max_retries=kwargs.get("max_retries", 5)
        )

    def get_embedding_model(self, **kwargs):
        return OpenAIEmbeddings(
            openai_api_key=kwargs.get("openai_api_key", LLM_EMBEDDING_CONFIG.get("openai_api_key")),
            model=kwargs.get("model", LLM_EMBEDDING_CONFIG.get("openai_model", "text-embedding-ada-002")),
            max_retries=kwargs.get("max_retries", 5)
        )

class OllamaLLMProvider(LLMProviderBase):
    def get_chat_model(self, **kwargs):
        return ChatOllama(
            base_url=kwargs.get("base_url", LLM_CONFIG.get("ollama_base_url", "http://localhost:11434")),
            model=kwargs.get("model", LLM_CONFIG.get("ollama_model_name", "llama3")),
            temperature=kwargs.get("temperature", 0.7)
        )

    def get_embedding_model(self, **kwargs):
        return OllamaEmbeddings(
            base_url=kwargs.get("base_url", LLM_EMBEDDING_CONFIG.get("ollama_base_url", "http://localhost:11434")),
            model=kwargs.get("model", LLM_EMBEDDING_CONFIG.get("ollama_model", "llama3")),
        )

class LLMBase:
    """
    LLMBase ç»Ÿä¸€è°ƒåº¦å„ç±» LLMProviderã€‚
    ç®¡ç†å¤šä¼šè¯å†å²ï¼Œæ”¯æŒä¸åŒ LLM providerã€‚
    
    Enhanced Features:
    - Tool calling support for MCP integration
    - Async operations
    - Better error handling
    - Flexible configuration
    """
    def __init__(self, provider: str) -> None:
        """
        Args:
            provider (str): 'azure', 'openai', 'ollama'
        """
        self.message_histories = {}
        self.provider = provider.lower()
        self.providers = {
            "azure": AzureLLMProvider(),
            "openai": OpenAILLMProvider(),
            "ollama": OllamaLLMProvider(),
        }
        
        # Validate provider
        self._validate_provider()
            
        # Initialize models
        self.chat_model = self.get_chat_model()
        self.embedding_model = self.get_embedding_model()
        
        logger.info(f"LLMBase initialized with provider: {self.provider}")

    def _validate_provider(self):
        """éªŒè¯å½“å‰ provider æ˜¯å¦æœ‰æ•ˆã€‚"""
        if self.provider not in self.providers:
            logger.error(f"Unknown provider: {self.provider}")
            raise ValueError(f"Unknown provider: {self.provider}")

    def _format_system_prompt(self, role: str, system_format_dict: dict = None) -> str:
        """
        æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºè¯ã€‚
        
        Args:
            role: è§’è‰²æ ‡è¯†
            system_format_dict: æ ¼å¼åŒ–å‚æ•°å­—å…¸
            
        Returns:
            æ ¼å¼åŒ–åçš„ç³»ç»Ÿæç¤ºè¯
        """
        system_prompt = SYSTEM_PROMPT_CONFIG.get(role, "")
        
        if system_format_dict:
            try:
                system_prompt = system_prompt.format(**system_format_dict)
            except KeyError as e:
                logger.error(f"ç³»ç»Ÿæç¤ºè¯æ ¼å¼åŒ–å¤±è´¥ï¼Œç¼ºå°‘å‚æ•°: {e}")
        
        return system_prompt

    def get_chat_model_with_tools(self, tools: Optional[List[Dict]] = None, **kwargs):
        """
        Get chat model with optional tool binding support for MCP integration.
        
        Args:
            tools: List of tool definitions for binding
            **kwargs: Additional model parameters
            
        Returns:
            Chat model instance with tools bound if provided
        """
        model = self.get_chat_model(**kwargs)

        if tools and hasattr(model, 'bind_tools'):
            try:
                bound_model = model.bind_tools(tools)
                return bound_model
            except Exception as e:
                logger.warning(f"å·¥å…·ç»‘å®šå¤±è´¥: {e}")
                return model
        elif tools and not hasattr(model, 'bind_tools'):
            logger.warning(f"æ¨¡å‹ {type(model).__name__} ä¸æ”¯æŒå·¥å…·ç»‘å®š")

        return model

    async def async_call_llm_chain(
        self,
        role: str,
        input_prompt: str,
        session_id: str,
        output_parser=StrOutputParser(),
        system_format_dict: dict = None,
        tools: Optional[List[Dict]] = None
    ) -> Any:
        """
        ä¸»è¦çš„å¼‚æ­¥ LLM è°ƒç”¨æ–¹æ³•ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ã€‚
        
        Args:
            role (str): PDFReaderRole æšä¸¾å€¼
            input_prompt (str): è¾“å…¥æç¤º
            session_id (str): ä¼šè¯ ID
            output_parser: è¾“å‡ºè§£æå™¨
            system_format_dict: ç³»ç»Ÿæç¤ºè¯æ ¼å¼åŒ–å‚æ•°
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            
        Returns:
            Any: LLM å“åº”å¯¹è±¡
        """
        # Format system prompt
        system_prompt = self._format_system_prompt(role, system_format_dict)

        # Get model with tools if provided
        if tools:
            chat_model = self.get_chat_model_with_tools(tools)
        else:
            chat_model = self.chat_model

        chain = self.build_chain(
            client=chat_model,
            system_prompt=system_prompt,
            output_parser=output_parser,
            tools=tools
        )

        try:
            # Use async invoke if available
            if hasattr(chain, 'ainvoke'):
                response = await chain.ainvoke(
                    {"input_prompt": input_prompt},
                    config={"configurable": {"session_id": session_id}}
                )
            else:
                # Fallback to sync invoke in executor
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: chain.invoke(
                        {"input_prompt": input_prompt},
                        config={"configurable": {"session_id": session_id}}
                    )
                )

            return response

        except Exception as e:
            logger.error(f"{role} å¼‚æ­¥è°ƒç”¨LLMæŠ¥é”™: {e}")
            return ""

    def update_provider_config(self, provider: str = None, **config_updates):
        """
        åŠ¨æ€æ›´æ–°provideré…ç½®å¹¶é‡æ–°åˆå§‹åŒ–æ¨¡å‹ã€‚
        
        Args:
            provider: æ–°çš„providerç±»å‹ï¼ˆå¯é€‰ï¼‰
            **config_updates: é…ç½®æ›´æ–°å‚æ•°
        """
        if provider and provider.lower() != self.provider:
            self.provider = provider.lower()
            self._validate_provider()
            logger.info(f"Provider updated to: {self.provider}")
        
        # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
        try:
            self.chat_model = self.get_chat_model(**config_updates)
            self.embedding_model = self.get_embedding_model(**config_updates)
            logger.info("Models reinitialized with new configuration")
        except Exception as e:
            logger.error(f"Failed to reinitialize models: {e}")
            raise

    def get_provider_info(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰providerçš„è¯¦ç»†ä¿¡æ¯ã€‚
        
        Returns:
            Dict: Providerä¿¡æ¯å­—å…¸
        """
        return {
            "provider": self.provider,
            "chat_model_type": type(self.chat_model).__name__,
            "embedding_model_type": type(self.embedding_model).__name__,
            "available_providers": list(self.providers.keys()),
            "session_count": len(self.message_histories)
        }

    def clear_all_histories(self):
        """æ¸…ç©ºæ‰€æœ‰ä¼šè¯å†å²ã€‚"""
        self.message_histories.clear()
        logger.info("All message histories cleared")

    def get_session_info(self, session_id: str = None) -> Dict[str, Any]:
        """
        è·å–ä¼šè¯ä¿¡æ¯ã€‚
        
        Args:
            session_id: ä¼šè¯IDï¼ŒNoneåˆ™è¿”å›æ‰€æœ‰ä¼šè¯ä¿¡æ¯
            
        Returns:
            Dict: ä¼šè¯ä¿¡æ¯
        """
        if session_id:
            if session_id in self.message_histories:
                history = self.message_histories[session_id]
                return {
                    "session_id": session_id,
                    "message_count": len(history.messages),
                    "max_messages": getattr(history, 'max_messages', None),
                    "max_tokens": getattr(history, 'max_tokens', None)
                }
            else:
                return {"session_id": session_id, "exists": False}
        else:
            return {
                "total_sessions": len(self.message_histories),
                "sessions": list(self.message_histories.keys())
            }

    def get_message_history(self, session_id=None):
        """
        è·å–æŒ‡å®š session_id çš„æ¶ˆæ¯å†å²ï¼Œæ²¡æœ‰åˆ™è‡ªåŠ¨åˆ›å»ºã€‚
        """
        if session_id not in self.message_histories:
            if session_id in ["chat"]:
                self.message_histories[session_id] = LimitedChatMessageHistory()
            else:
                self.message_histories[session_id] = LimitedChatMessageHistory(max_messages=5)
        return self.message_histories[session_id]

    def add_message_to_history(self, session_id=None, message=None):
        """
        å‘æŒ‡å®š session_id çš„å†å²æ·»åŠ æ¶ˆæ¯ã€‚
        """
        if message is None:
            message = HumanMessage("")  # æˆ– SystemMessage("")ï¼Œæ ¹æ®ä½ çš„ä¸šåŠ¡åœºæ™¯
        if session_id not in self.message_histories:
            logger.warning(f"Can't find {session_id}, in current history. Create a new history.")
            if session_id in ["chat"]:
                self.message_histories[session_id] = LimitedChatMessageHistory()
            else:
                self.message_histories[session_id] = LimitedChatMessageHistory(max_messages=5)
        self.message_histories[session_id].add_message(message)

    def delete_last_message_in_history(self, session_id=None):
        """
        åˆ é™¤æŒ‡å®š session_id çš„å†å²ä¸­çš„æœ€åä¸€æ¡æ¶ˆæ¯ã€‚
        """
        if session_id in self.message_histories:
            self.message_histories[session_id].delete_last_message()
        else:
            logger.warning(f"Can't find {session_id}, in current history. No message deleted.")

    def is_content_in_history(self, content, session_id=None, exact_match=False):
        """
        åˆ¤æ–­ content æ˜¯å¦åœ¨ session_id çš„å†å²æ¶ˆæ¯ä¸­å‡ºç°è¿‡ã€‚
        Args:
            content (str): è¦æŸ¥æ‰¾çš„å†…å®¹ã€‚
            session_id (Any): ä¼šè¯IDã€‚
            exact_match (bool): æ˜¯å¦è¦æ±‚å®Œå…¨åŒ¹é…ï¼ˆé»˜è®¤Falseï¼Œè¡¨ç¤ºåªè¦åŒ…å«å³å¯ï¼‰ã€‚
        Returns:
            bool: True è¡¨ç¤ºæ‰¾åˆ°åŒ¹é…å†…å®¹ï¼ŒFalse è¡¨ç¤ºæœªæ‰¾åˆ°ã€‚
        """
        history = self.get_message_history(session_id)
        for idx, msg in enumerate(history.messages):
            if hasattr(msg, "content"):
                if exact_match:
                    if msg.content == content:
                        logger.info(f"[is_content_in_history] å®Œå…¨åŒ¹é…æˆåŠŸï¼Œç´¢å¼•: {idx}")
                        return True
                else:
                    if content in msg.content:
                        logger.info(f"[is_content_in_history] åŒ…å«å…³ç³»åŒ¹é…æˆåŠŸï¼Œç´¢å¼•: {idx}")
                        return True
        logger.info("[is_content_in_history] æœªæ‰¾åˆ°åŒ¹é…å†…å®¹ã€‚")
        return False

    def build_chain(
        self,
        client,
        system_prompt: str = "",
        output_parser=None,
        tools=None,
    ):
        """
        æ„å»ºå¸¦æœ‰ system_promptã€toolsã€session_id ä»¥åŠå¯é€‰ output_format çš„å¯¹è¯é“¾ã€‚
        output_format: å¯é€‰ï¼Œå­—ç¬¦ä¸²ï¼ŒæŒ‡å®šè¾“å‡ºæ ¼å¼è¯´æ˜ï¼Œä¼šæ‹¼æ¥åˆ° system_prompt åé¢ã€‚
        """
        # 1. å½“æœ‰å·¥å…·æ—¶ï¼Œä¸ä½¿ç”¨ StrOutputParser ä»¥ä¿ç•™å·¥å…·è°ƒç”¨ä¿¡æ¯
        if tools:
            # å·¥å…·è°ƒç”¨æ¨¡å¼ï¼šä¸ä½¿ç”¨è¾“å‡ºè§£æå™¨ï¼Œä¿æŒåŸå§‹å“åº”
            output_parser = None
            logger.debug("å·¥å…·è°ƒç”¨æ¨¡å¼ï¼šä¸ä½¿ç”¨è¾“å‡ºè§£æå™¨")
        elif not output_parser:
            output_parser = StrOutputParser()
            logger.debug("æ ‡å‡†æ¨¡å¼ï¼šä½¿ç”¨ StrOutputParser")
        
        # 2. æ„å»º promptï¼ŒåŒ…å« system promptã€å†å²æ¶ˆæ¯å’Œç”¨æˆ·è¾“å…¥
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content=system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            HumanMessagePromptTemplate.from_template("{input_prompt}"),
        ])
        
        # 3. æ ¹æ®æ˜¯å¦æœ‰è¾“å‡ºè§£æå™¨æ„å»ºä¸åŒçš„ runnable
        if output_parser:
            runnable = prompt | client | output_parser
        else:
            runnable = prompt | client
       
        return RunnableWithMessageHistory(
            runnable,
            self.get_message_history,
            input_messages_key="input_prompt",
            history_messages_key="chat_history"
        )

    def get_chat_model(self, **kwargs):
        """
        è·å–å½“å‰ provider çš„ chat modelã€‚
        """
        self._validate_provider()
        return self.providers[self.provider].get_chat_model(**kwargs)

    def get_embedding_model(self, **kwargs):
        """
        è·å–å½“å‰ provider çš„ embedding modelã€‚
        """
        self._validate_provider()
        return self.providers[self.provider].get_embedding_model(**kwargs)

    def call_llm_chain(
        self,
        role: str,
        input_prompt: str,
        session_id: str,
        output_parser=StrOutputParser(),
        system_format_dict: dict = None,
        tools: Optional[List[Dict]] = None
    ) -> Any:
        """
        åŒæ­¥ç‰ˆæœ¬çš„ LLM è°ƒç”¨æ–¹æ³•ï¼Œé€‚ç”¨äºéå¼‚æ­¥ç¯å¢ƒã€‚

        Args:
            role (str): PDFReaderRole æšä¸¾å€¼
            input_prompt (str): è¾“å…¥æç¤º
            session_id (str): ä¼šè¯ ID
            output_parser: è¾“å‡ºè§£æå™¨
            system_format_dict: ç³»ç»Ÿæç¤ºè¯æ ¼å¼åŒ–å‚æ•°
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨

        Returns:
            Any: LLM å“åº”å¯¹è±¡
        """
        # è°ƒè¯•ï¼šæ£€æŸ¥è°ƒç”¨å‰çš„æ¶ˆæ¯å†å²
        if session_id in self.message_histories:
            history = self.message_histories[session_id]
            logger.info(f"ğŸ“œ [HISTORY CHECK] ä¼šè¯ {session_id} å½“å‰æœ‰ {len(history.messages)} æ¡æ¶ˆæ¯")
            #for idx, msg in enumerate(history.messages):
            #    msg_type = type(msg).__name__
            #    has_tool_calls = hasattr(msg, 'tool_calls') and msg.tool_calls
            #    has_tool_call_id = hasattr(msg, 'tool_call_id')
            #    logger.info(f"  [{idx}] {msg_type} | tool_calls={has_tool_calls} | tool_call_id={has_tool_call_id}")
            #    if has_tool_calls:
            #        for tc in msg.tool_calls:
            #            tc_id = tc.get('id') if isinstance(tc, dict) else getattr(tc, 'id', 'unknown')
            #            tc_name = tc.get('name') if isinstance(tc, dict) else getattr(tc, 'name', 'unknown')
            #            logger.info(f"      â†’ tool_call: id={tc_id}, name={tc_name}")
            #    if has_tool_call_id:
            #        logger.info(f"      â†’ responding to: {msg.tool_call_id}")
        else:
            logger.info(f"ğŸ“œ [HISTORY CHECK] ä¼šè¯ {session_id} å°šæœªåˆ›å»º")

        # Format system prompt
        system_prompt = self._format_system_prompt(role, system_format_dict)

        # Get model with tools if provided
        if tools:
            chat_model = self.get_chat_model_with_tools(tools)
        else:
            chat_model = self.chat_model

        chain = self.build_chain(
            client=chat_model,
            system_prompt=system_prompt,
            output_parser=output_parser,
            tools=tools
        )

        try:
            # ç›´æ¥ä½¿ç”¨åŒæ­¥è°ƒç”¨
            response = chain.invoke(
                {"input_prompt": input_prompt},
                config={"configurable": {"session_id": session_id}}
            )

            return response

        except Exception as e:
            logger.error(f"{role} åŒæ­¥è°ƒç”¨LLMæŠ¥é”™: {e}")
            return ""

    def add_messages_to_history(self, session_id: str, messages: List) -> None:
        """
        å°†å¤šæ¡æ¶ˆæ¯æ·»åŠ åˆ°æŒ‡å®šä¼šè¯çš„å†å²è®°å½•ä¸­

        ç”¨äºå·¥å…·è°ƒç”¨åœºæ™¯ï¼šéœ€è¦å°† AIMessageï¼ˆå¸¦tool_callsï¼‰å’Œ ToolMessage éƒ½æ·»åŠ åˆ°å†å²

        Args:
            session_id (str): ä¼šè¯ ID
            messages (List): æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯ä»¥åŒ…å« AIMessage, ToolMessage ç­‰
        """
        logger.info(f"ğŸ“ [ADD MESSAGES] å‡†å¤‡å°† {len(messages)} æ¡æ¶ˆæ¯æ·»åŠ åˆ°ä¼šè¯ {session_id}")

        if session_id not in self.message_histories:
            logger.warning(f"ä¼šè¯ {session_id} ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ä¼šè¯")
            self.message_histories[session_id] = LimitedChatMessageHistory(
                max_messages=ProcessingLimits.MAX_MESSAGES,
                max_tokens=LLMConstants.MAX_CONTEXT_TOKENS
            )

        history = self.message_histories[session_id]
        logger.info(f"ğŸ“ [BEFORE ADD] ä¼šè¯å½“å‰æœ‰ {len(history.messages)} æ¡æ¶ˆæ¯")

        for idx, msg in enumerate(messages):
            msg_type = type(msg).__name__
            history.add_message(msg)

            # è¯¦ç»†è®°å½•æ¯æ¡æ¶ˆæ¯çš„ä¿¡æ¯
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                logger.info(f"ğŸ“ [{idx+1}/{len(messages)}] æ·»åŠ  {msg_type} (åŒ…å« {len(msg.tool_calls)} ä¸ª tool_calls)")
                for tc in msg.tool_calls:
                    tc_id = tc.get('id') if isinstance(tc, dict) else getattr(tc, 'id', 'unknown')
                    tc_name = tc.get('name') if isinstance(tc, dict) else getattr(tc, 'name', 'unknown')
                    logger.info(f"      â†’ tool_call: id={tc_id}, name={tc_name}")
            elif hasattr(msg, 'tool_call_id'):
                logger.info(f"ğŸ“ [{idx+1}/{len(messages)}] æ·»åŠ  {msg_type} (å“åº” tool_call_id={msg.tool_call_id})")
            else:
                logger.info(f"ğŸ“ [{idx+1}/{len(messages)}] æ·»åŠ  {msg_type}")

        logger.info(f"ğŸ“ [AFTER ADD] ä¼šè¯ç°åœ¨æœ‰ {len(history.messages)} æ¡æ¶ˆæ¯")


def get_embeddings(**kwargs):
    """
    è·å–å…¨å±€åµŒå…¥æ¨¡å‹å®ä¾‹
    
    æ ¹æ®é…ç½®è¿”å›å¯¹åº”çš„åµŒå…¥æ¨¡å‹ï¼ˆAzure OpenAI æˆ– OpenAIï¼‰
    
    Args:
        **kwargs: ä¼ é€’ç»™åµŒå…¥æ¨¡å‹çš„é¢å¤–å‚æ•°
        
    Returns:
        åµŒå…¥æ¨¡å‹å®ä¾‹ï¼ˆAzureOpenAIEmbeddings æˆ– OpenAIEmbeddingsï¼‰
    """
    provider = LLM_EMBEDDING_CONFIG.get("provider", "openai").lower()
    
    if provider == "azure":
        return AzureOpenAIEmbeddings(
            openai_api_key=kwargs.get("openai_api_key", LLM_EMBEDDING_CONFIG.get("api_key")),
            openai_api_version=kwargs.get("openai_api_version", LLM_EMBEDDING_CONFIG.get("api_version")),
            azure_endpoint=kwargs.get("azure_endpoint", LLM_EMBEDDING_CONFIG.get("azure_endpoint")),
            deployment=kwargs.get("deployment", LLM_EMBEDDING_CONFIG.get("deployment")),
            model=kwargs.get("model", LLM_EMBEDDING_CONFIG.get("model")),
            max_retries=kwargs.get("max_retries", 5)
        )
    elif provider == "openai":
        return OpenAIEmbeddings(
            openai_api_key=kwargs.get("openai_api_key", LLM_EMBEDDING_CONFIG.get("openai_api_key")),
            model=kwargs.get("model", LLM_EMBEDDING_CONFIG.get("openai_model", "text-embedding-ada-002")),
            max_retries=kwargs.get("max_retries", 5)
        )
    else:
        # é»˜è®¤ä½¿ç”¨ OpenAI
        logger.warning(f"æœªçŸ¥çš„åµŒå…¥æ¨¡å‹ provider: {provider}ï¼Œé»˜è®¤ä½¿ç”¨ OpenAI")
        return OpenAIEmbeddings(
            openai_api_key=kwargs.get("openai_api_key", LLM_EMBEDDING_CONFIG.get("openai_api_key")),
            model=kwargs.get("model", LLM_EMBEDDING_CONFIG.get("openai_model", "text-embedding-ada-002")),
            max_retries=kwargs.get("max_retries", 5)
        )