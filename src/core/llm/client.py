"""
client.py - Main LLM client for managing conversations and providers

This module provides the main LLMBase class for managing chat conversations,
integrating with different LLM providers and handling message history.

Enhanced Features:
- Multi-provider support (Azure, OpenAI, Ollama)
- Tool calling support for MCP integration
- Async operations support
- Session-based message history management
- Smart history management with LLM summarization
- Enhanced error handling and logging
- Flexible configuration management

Note:
    - Message history management moved to history.py
    - Provider implementations moved to providers.py
"""
import asyncio
import logging
from typing import Any, Optional, List, Dict
from langchain_openai import AzureOpenAIEmbeddings, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, MessagesPlaceholder
from langchain_core.messages import SystemMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.history import RunnableWithMessageHistory

from src.config.settings import LLM_EMBEDDING_CONFIG
from src.config.prompts import SYSTEM_PROMPT_CONFIG
from src.config.constants import SessionHistoryConfig

# Import from refactored modules
from src.core.llm.history import LimitedChatMessageHistory
from src.core.llm.providers import (
    AzureLLMProvider,
    OpenAILLMProvider,
    OllamaLLMProvider
)

logging.basicConfig(
    level=logging.INFO,  # å¯æ ¹æ®éœ€è¦æ”¹ä¸º DEBUG
    format="%(asctime)s [%(levelname)s] %(message)s"
)
logger = logging.getLogger(__name__)


class LLMBase:
    """
    LLMBase ç»Ÿä¸€è°ƒåº¦å„ç±» LLMProviderã€‚
    ç®¡ç†å¤šä¼šè¯å†å²ï¼Œæ”¯æŒä¸åŒ LLM providerã€‚
    
    Enhanced Features:
    - Tool calling support for MCP integration
    - Async operations
    - Better error handling
    - Flexible configuration
    """
    def __init__(self, provider: str) -> None:
        """
        Args:
            provider (str): 'azure', 'openai', 'ollama'
        """
        self.message_histories = {}
        self.provider = provider.lower()
        self.providers = {
            "azure": AzureLLMProvider(),
            "openai": OpenAILLMProvider(),
            "ollama": OllamaLLMProvider(),
        }
        
        # Validate provider
        self._validate_provider()
            
        # Initialize models
        self.chat_model = self.get_chat_model()
        self.embedding_model = self.get_embedding_model()
        
        logger.info(f"LLMBase initialized with provider: {self.provider}")

    def _validate_provider(self):
        """éªŒè¯å½“å‰ provider æ˜¯å¦æœ‰æ•ˆã€‚"""
        if self.provider not in self.providers:
            logger.error(f"Unknown provider: {self.provider}")
            raise ValueError(f"Unknown provider: {self.provider}")

    def _format_system_prompt(self, role: str, system_format_dict: dict = None) -> str:
        """
        æ ¼å¼åŒ–ç³»ç»Ÿæç¤ºè¯ã€‚
        
        Args:
            role: è§’è‰²æ ‡è¯†
            system_format_dict: æ ¼å¼åŒ–å‚æ•°å­—å…¸
            
        Returns:
            æ ¼å¼åŒ–åçš„ç³»ç»Ÿæç¤ºè¯
        """
        system_prompt = SYSTEM_PROMPT_CONFIG.get(role, "")
        
        if system_format_dict:
            try:
                system_prompt = system_prompt.format(**system_format_dict)
            except KeyError as e:
                logger.error(f"ç³»ç»Ÿæç¤ºè¯æ ¼å¼åŒ–å¤±è´¥ï¼Œç¼ºå°‘å‚æ•°: {e}")
        
        return system_prompt

    def get_chat_model_with_tools(self, tools: Optional[List[Dict]] = None, **kwargs):
        """
        Get chat model with optional tool binding support for MCP integration.
        
        Args:
            tools: List of tool definitions for binding
            **kwargs: Additional model parameters
            
        Returns:
            Chat model instance with tools bound if provided
        """
        model = self.get_chat_model(**kwargs)

        if tools and hasattr(model, 'bind_tools'):
            try:
                bound_model = model.bind_tools(tools)
                return bound_model
            except Exception as e:
                logger.warning(f"å·¥å…·ç»‘å®šå¤±è´¥: {e}")
                return model
        elif tools and not hasattr(model, 'bind_tools'):
            logger.warning(f"æ¨¡å‹ {type(model).__name__} ä¸æ”¯æŒå·¥å…·ç»‘å®š")

        return model

    async def async_call_llm_chain(
        self,
        role: str,
        input_prompt: str,
        session_id: str,
        output_parser=StrOutputParser(),
        system_format_dict: dict = None,
        tools: Optional[List[Dict]] = None,
        enable_llm_summary: bool = True
    ) -> Any:
        """
        ä¸»è¦çš„å¼‚æ­¥ LLM è°ƒç”¨æ–¹æ³•ï¼Œæ”¯æŒå·¥å…·è°ƒç”¨ã€‚

        Args:
            role (str): PDFReaderRole æšä¸¾å€¼
            input_prompt (str): è¾“å…¥æç¤º
            session_id (str): ä¼šè¯ ID
            output_parser: è¾“å‡ºè§£æå™¨
            system_format_dict: ç³»ç»Ÿæç¤ºè¯æ ¼å¼åŒ–å‚æ•°
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            enable_llm_summary: æ˜¯å¦å¯ç”¨LLMå†å²æ€»ç»“ï¼ˆé»˜è®¤Trueï¼ŒFalseåˆ™ä½¿ç”¨é•¿åº¦æˆªæ–­ï¼‰

        Returns:
            Any: LLM å“åº”å¯¹è±¡
        """
        # é¢„å…ˆåˆ›å»ºæ¶ˆæ¯å†å²ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œä»¥ä¾¿æ§åˆ¶ LLM æ€»ç»“åŠŸèƒ½
        if session_id not in self.message_histories:
            self.get_message_history(session_id, enable_llm_summary=enable_llm_summary)

        # Format system prompt
        system_prompt = self._format_system_prompt(role, system_format_dict)

        # Get model with tools if provided
        if tools:
            chat_model = self.get_chat_model_with_tools(tools)
        else:
            chat_model = self.chat_model

        chain = self.build_chain(
            client=chat_model,
            system_prompt=system_prompt,
            output_parser=output_parser,
            tools=tools
        )

        try:
            # Use async invoke if available
            if hasattr(chain, 'ainvoke'):
                response = await chain.ainvoke(
                    {"input_prompt": input_prompt},
                    config={"configurable": {"session_id": session_id}}
                )
            else:
                # Fallback to sync invoke in executor
                loop = asyncio.get_event_loop()
                response = await loop.run_in_executor(
                    None,
                    lambda: chain.invoke(
                        {"input_prompt": input_prompt},
                        config={"configurable": {"session_id": session_id}}
                    )
                )

            return response

        except Exception as e:
            logger.error(f"{role} å¼‚æ­¥è°ƒç”¨LLMæŠ¥é”™: {e}")
            return ""

    def update_provider_config(self, provider: str = None, **config_updates):
        """
        åŠ¨æ€æ›´æ–°provideré…ç½®å¹¶é‡æ–°åˆå§‹åŒ–æ¨¡å‹ã€‚
        
        Args:
            provider: æ–°çš„providerç±»å‹ï¼ˆå¯é€‰ï¼‰
            **config_updates: é…ç½®æ›´æ–°å‚æ•°
        """
        if provider and provider.lower() != self.provider:
            self.provider = provider.lower()
            self._validate_provider()
            logger.info(f"Provider updated to: {self.provider}")
        
        # é‡æ–°åˆå§‹åŒ–æ¨¡å‹
        try:
            self.chat_model = self.get_chat_model(**config_updates)
            self.embedding_model = self.get_embedding_model(**config_updates)
            logger.info("Models reinitialized with new configuration")
        except Exception as e:
            logger.error(f"Failed to reinitialize models: {e}")
            raise

    def get_provider_info(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰providerçš„è¯¦ç»†ä¿¡æ¯ã€‚
        
        Returns:
            Dict: Providerä¿¡æ¯å­—å…¸
        """
        return {
            "provider": self.provider,
            "chat_model_type": type(self.chat_model).__name__,
            "embedding_model_type": type(self.embedding_model).__name__,
            "available_providers": list(self.providers.keys()),
            "session_count": len(self.message_histories)
        }

    def clear_all_histories(self):
        """æ¸…ç©ºæ‰€æœ‰ä¼šè¯å†å²ã€‚"""
        self.message_histories.clear()
        logger.info("All message histories cleared")

    def clear_session_history(self, session_id: str) -> bool:
        """
        æ¸…ç©ºæŒ‡å®š session_id çš„æ‰€æœ‰å†å²æ¶ˆæ¯

        Args:
            session_id (str): ä¼šè¯ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸæ¸…ç©ºï¼ˆå¦‚æœä¼šè¯ä¸å­˜åœ¨åˆ™è¿”å›Falseï¼‰
        """
        if session_id in self.message_histories:
            message_count = self.message_histories[session_id].clear_all_messages()
            logger.info(f"âœ… ä¼šè¯ {session_id} çš„å†å²å·²æ¸…ç©ºï¼Œå…±åˆ é™¤ {message_count} æ¡æ¶ˆæ¯")
            return True
        else:
            logger.warning(f"âŒ ä¼šè¯ {session_id} ä¸å­˜åœ¨ï¼Œæ— æ³•æ¸…ç©º")
            return False

    def print_session_history(self, session_id: str, detailed: bool = False) -> str:
        """
        æ‰“å°æŒ‡å®š session_id çš„æ‰€æœ‰å†å²æ¶ˆæ¯

        Args:
            session_id (str): ä¼šè¯ID
            detailed (bool): æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼ˆæ¶ˆæ¯ç±»å‹ã€tokenæ•°ç­‰ï¼‰ï¼Œé»˜è®¤False

        Returns:
            str: æ ¼å¼åŒ–çš„æ¶ˆæ¯å†å²å­—ç¬¦ä¸²ï¼Œå¦‚æœä¼šè¯ä¸å­˜åœ¨åˆ™è¿”å›é”™è¯¯ä¿¡æ¯
        """
        if session_id in self.message_histories:
            logger.info(f"ğŸ“œ æ‰“å°ä¼šè¯ {session_id} çš„å†å²æ¶ˆæ¯")
            return self.message_histories[session_id].print_all_messages(detailed=detailed)
        else:
            error_msg = f"âŒ ä¼šè¯ {session_id} ä¸å­˜åœ¨ï¼Œæ— æ³•æ‰“å°å†å²"
            logger.warning(error_msg)
            print(error_msg)
            return error_msg

    def copy_session_history(self, source_session_id: str, target_session_id: str,
                            replace: bool = False) -> bool:
        """
        å°†æº session_id çš„æ‰€æœ‰æ¶ˆæ¯å¤åˆ¶åˆ°ç›®æ ‡ session_id

        Args:
            source_session_id (str): æºä¼šè¯ID
            target_session_id (str): ç›®æ ‡ä¼šè¯ID
            replace (bool): æ˜¯å¦æ›¿æ¢ç›®æ ‡ä¼šè¯çš„ç°æœ‰æ¶ˆæ¯ï¼ˆé»˜è®¤Falseï¼Œè¿½åŠ æ¨¡å¼ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸå¤åˆ¶
        """
        # æ£€æŸ¥æºä¼šè¯æ˜¯å¦å­˜åœ¨
        if source_session_id not in self.message_histories:
            logger.warning(f"âŒ æºä¼šè¯ {source_session_id} ä¸å­˜åœ¨ï¼Œæ— æ³•å¤åˆ¶")
            return False

        # è·å–æˆ–åˆ›å»ºç›®æ ‡ä¼šè¯
        target_history = self.get_message_history(target_session_id)

        # å¦‚æœæ˜¯æ›¿æ¢æ¨¡å¼ï¼Œå…ˆæ¸…ç©ºç›®æ ‡ä¼šè¯
        if replace:
            target_history.clear_all_messages()
            logger.info(f"ğŸ”„ æ›¿æ¢æ¨¡å¼ï¼šå·²æ¸…ç©ºç›®æ ‡ä¼šè¯ {target_session_id} çš„åŸæœ‰æ¶ˆæ¯")

        # æ‰§è¡Œå¤åˆ¶
        source_history = self.message_histories[source_session_id]
        copied_count = source_history.copy_messages_to(target_history)

        logger.info(f"âœ… æˆåŠŸå°† {copied_count} æ¡æ¶ˆæ¯ä»ä¼šè¯ {source_session_id} "
                   f"å¤åˆ¶åˆ°ä¼šè¯ {target_session_id} (replace={replace})")
        return True

    def export_session_history(self, session_id: str, include_metadata: bool = False) -> List[Dict[str, Any]]:
        """
        å¯¼å‡ºæŒ‡å®š session_id çš„æ‰€æœ‰å†å²æ¶ˆæ¯ä¸ºç»“æ„åŒ–æ•°æ®

        Args:
            session_id (str): ä¼šè¯ID
            include_metadata (bool): æ˜¯å¦åŒ…å«å…ƒæ•°æ®ï¼ˆtokenæ•°ã€ç±»å‹ç­‰ï¼‰ï¼Œé»˜è®¤False

        Returns:
            List[Dict[str, Any]]: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯æ¡æ¶ˆæ¯ä¸ºä¸€ä¸ªå­—å…¸
                åŸºç¡€å­—æ®µï¼š
                    - index (int): æ¶ˆæ¯ç´¢å¼•ï¼ˆä»1å¼€å§‹ï¼‰
                    - role (str): è§’è‰²åç§° ("user", "assistant", "system", "unknown")
                    - content (str): æ¶ˆæ¯å†…å®¹
                å¦‚æœ include_metadata=Trueï¼Œè¿˜åŒ…æ‹¬ï¼š
                    - type (str): æ¶ˆæ¯ç±»å‹
                    - token_count (int): Tokenæ•°é‡
                    - tool_calls (list): å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    - tool_call_id (str): å“åº”çš„å·¥å…·è°ƒç”¨IDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    - additional_kwargs (dict): é¢å¤–å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰

        Example:
            >>> llm_client.export_session_history("session_1")
            [
                {"index": 1, "role": "user", "content": "ä½ å¥½"},
                {"index": 2, "role": "assistant", "content": "ä½ å¥½ï¼æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©ä½ çš„ï¼Ÿ"}
            ]

            >>> llm_client.export_session_history("session_1", include_metadata=True)
            [
                {
                    "index": 1,
                    "role": "user",
                    "content": "ä½ å¥½",
                    "type": "HumanMessage",
                    "token_count": 2
                },
                ...
            ]

        Note:
            - å¦‚æœä¼šè¯ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨
            - è¿”å›çš„æ•°æ®å¯ä»¥ç›´æ¥åºåˆ—åŒ–ä¸ºJSON
            - ä¿ç•™äº†æ‰€æœ‰è§’è‰²ä¿¡æ¯å’Œå¯¹è¯é¡ºåº
        """
        if session_id not in self.message_histories:
            logger.warning(f"âŒ ä¼šè¯ {session_id} ä¸å­˜åœ¨ï¼Œæ— æ³•å¯¼å‡ºå†å²")
            return []

        logger.info(f"ğŸ“¤ å¯¼å‡ºä¼šè¯ {session_id} çš„å†å²æ¶ˆæ¯ (include_metadata={include_metadata})")
        exported_data = self.message_histories[session_id].export_messages(include_metadata=include_metadata)

        logger.info(f"âœ… æˆåŠŸå¯¼å‡º {len(exported_data)} æ¡æ¶ˆæ¯")
        return exported_data

    def get_session_info(self, session_id: str = None) -> Dict[str, Any]:
        """
        è·å–ä¼šè¯ä¿¡æ¯ã€‚
        
        Args:
            session_id: ä¼šè¯IDï¼ŒNoneåˆ™è¿”å›æ‰€æœ‰ä¼šè¯ä¿¡æ¯
            
        Returns:
            Dict: ä¼šè¯ä¿¡æ¯
        """
        if session_id:
            if session_id in self.message_histories:
                history = self.message_histories[session_id]
                return {
                    "session_id": session_id,
                    "message_count": len(history.messages),
                    "max_messages": getattr(history, 'max_messages', None),
                    "max_tokens": getattr(history, 'max_tokens', None)
                }
            else:
                return {"session_id": session_id, "exists": False}
        else:
            return {
                "total_sessions": len(self.message_histories),
                "sessions": list(self.message_histories.keys())
            }

    def get_message_history(self, session_id=None, enable_llm_summary=True):
        """
        è·å–æŒ‡å®š session_id çš„æ¶ˆæ¯å†å²ï¼Œæ²¡æœ‰åˆ™è‡ªåŠ¨åˆ›å»ºã€‚

        Args:
            session_id: ä¼šè¯ID
            enable_llm_summary: æ˜¯å¦ä¸ºæ–°åˆ›å»ºçš„å†å²å¯ç”¨LLMæ€»ç»“åŠŸèƒ½ï¼ˆé»˜è®¤Trueï¼‰

        Returns:
            LimitedChatMessageHistory å®ä¾‹
        """
        if session_id not in self.message_histories:
            # ä»ç»Ÿä¸€é…ç½®ä¸­è·å–å‚æ•°
            config = SessionHistoryConfig.get_config(session_id)

            self.message_histories[session_id] = LimitedChatMessageHistory(
                max_messages=config["max_messages"],
                max_tokens=config["max_tokens"],
                use_llm_summary=enable_llm_summary and config["use_llm_summary"],
                llm_client=self if enable_llm_summary and config["use_llm_summary"] else None,
                summary_threshold=config["summary_threshold"]
            )

            logger.debug(f"åˆ›å»ºæ–°çš„æ¶ˆæ¯å†å² - session_id: {session_id}, "
                        f"max_messages: {config['max_messages']}, "
                        f"max_tokens: {config['max_tokens']}, "
                        f"summary_threshold: {config['summary_threshold']}")

        return self.message_histories[session_id]

    def add_message_to_history(self, session_id=None, message=None, enable_llm_summary=True):
        """
        å‘æŒ‡å®š session_id çš„å†å²æ·»åŠ æ¶ˆæ¯ã€‚

        Args:
            session_id: ä¼šè¯ID
            message: è¦æ·»åŠ çš„æ¶ˆæ¯
            enable_llm_summary: å¦‚æœéœ€è¦åˆ›å»ºæ–°å†å²ï¼Œæ˜¯å¦å¯ç”¨LLMæ€»ç»“åŠŸèƒ½ï¼ˆé»˜è®¤Trueï¼‰
        """
        if message is None:
            message = HumanMessage("")  # æˆ– SystemMessage("")ï¼Œæ ¹æ®ä½ çš„ä¸šåŠ¡åœºæ™¯

        if session_id not in self.message_histories:
            logger.warning(f"Can't find {session_id}, in current history. Create a new history.")

            # ä»ç»Ÿä¸€é…ç½®ä¸­è·å–å‚æ•°
            config = SessionHistoryConfig.get_config(session_id)

            self.message_histories[session_id] = LimitedChatMessageHistory(
                max_messages=config["max_messages"],
                max_tokens=config["max_tokens"],
                use_llm_summary=enable_llm_summary and config["use_llm_summary"],
                llm_client=self if enable_llm_summary and config["use_llm_summary"] else None,
                summary_threshold=config["summary_threshold"]
            )

            logger.debug(f"åˆ›å»ºæ–°çš„æ¶ˆæ¯å†å² - session_id: {session_id}, "
                        f"max_messages: {config['max_messages']}, "
                        f"max_tokens: {config['max_tokens']}, "
                        f"summary_threshold: {config['summary_threshold']}")

        self.message_histories[session_id].add_message(message)

    def enable_llm_summary_for_session(self, session_id: str, summary_threshold: int = None):
        """
        ä¸ºæŒ‡å®šä¼šè¯å¯ç”¨LLMæ™ºèƒ½æ€»ç»“åŠŸèƒ½

        Args:
            session_id: ä¼šè¯ID
            summary_threshold: è§¦å‘æ€»ç»“çš„æ¶ˆæ¯æ•°é‡é˜ˆå€¼ï¼ˆé»˜è®¤Noneï¼Œä½¿ç”¨é…ç½®ä¸­çš„å€¼ï¼‰

        Returns:
            bool: æ˜¯å¦æˆåŠŸå¯ç”¨
        """
        # å¦‚æœæœªæŒ‡å®šé˜ˆå€¼ï¼Œä»é…ç½®ä¸­è·å–
        if summary_threshold is None:
            config = SessionHistoryConfig.get_config(session_id)
            summary_threshold = config["summary_threshold"]
        if session_id in self.message_histories:
            history = self.message_histories[session_id]
            history.use_llm_summary = True
            history.llm_client = self
            history.summary_threshold = summary_threshold
            logger.info(f"âœ… ä¼šè¯ {session_id} å·²å¯ç”¨ LLM æ€»ç»“åŠŸèƒ½ (é˜ˆå€¼={summary_threshold})")
            return True
        else:
            logger.warning(f"âŒ ä¼šè¯ {session_id} ä¸å­˜åœ¨ï¼Œæ— æ³•å¯ç”¨ LLM æ€»ç»“")
            return False

    def disable_llm_summary_for_session(self, session_id: str):
        """
        ä¸ºæŒ‡å®šä¼šè¯ç¦ç”¨LLMæ™ºèƒ½æ€»ç»“åŠŸèƒ½

        Args:
            session_id: ä¼šè¯ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸç¦ç”¨
        """
        if session_id in self.message_histories:
            history = self.message_histories[session_id]
            history.use_llm_summary = False
            history.llm_client = None
            logger.info(f"âœ… ä¼šè¯ {session_id} å·²ç¦ç”¨ LLM æ€»ç»“åŠŸèƒ½")
            return True
        else:
            logger.warning(f"âŒ ä¼šè¯ {session_id} ä¸å­˜åœ¨ï¼Œæ— æ³•ç¦ç”¨ LLM æ€»ç»“")
            return False

    def delete_last_message_in_history(self, session_id=None):
        """
        åˆ é™¤æŒ‡å®š session_id çš„å†å²ä¸­çš„æœ€åä¸€æ¡æ¶ˆæ¯ã€‚
        """
        if session_id in self.message_histories:
            self.message_histories[session_id].delete_last_message()
        else:
            logger.warning(f"Can't find {session_id}, in current history. No message deleted.")

    def is_content_in_history(self, content, session_id=None, exact_match=False):
        """
        åˆ¤æ–­ content æ˜¯å¦åœ¨ session_id çš„å†å²æ¶ˆæ¯ä¸­å‡ºç°è¿‡ã€‚
        Args:
            content (str): è¦æŸ¥æ‰¾çš„å†…å®¹ã€‚
            session_id (Any): ä¼šè¯IDã€‚
            exact_match (bool): æ˜¯å¦è¦æ±‚å®Œå…¨åŒ¹é…ï¼ˆé»˜è®¤Falseï¼Œè¡¨ç¤ºåªè¦åŒ…å«å³å¯ï¼‰ã€‚
        Returns:
            bool: True è¡¨ç¤ºæ‰¾åˆ°åŒ¹é…å†…å®¹ï¼ŒFalse è¡¨ç¤ºæœªæ‰¾åˆ°ã€‚
        """
        history = self.get_message_history(session_id)
        for idx, msg in enumerate(history.messages):
            if hasattr(msg, "content"):
                if exact_match:
                    if msg.content == content:
                        logger.info(f"[is_content_in_history] å®Œå…¨åŒ¹é…æˆåŠŸï¼Œç´¢å¼•: {idx}")
                        return True
                else:
                    if content in msg.content:
                        logger.info(f"[is_content_in_history] åŒ…å«å…³ç³»åŒ¹é…æˆåŠŸï¼Œç´¢å¼•: {idx}")
                        return True
        logger.info("[is_content_in_history] æœªæ‰¾åˆ°åŒ¹é…å†…å®¹ã€‚")
        return False

    def build_chain(
        self,
        client,
        system_prompt: str = "",
        output_parser=None,
        tools=None,
    ):
        """
        æ„å»ºå¸¦æœ‰ system_promptã€toolsã€session_id ä»¥åŠå¯é€‰ output_format çš„å¯¹è¯é“¾ã€‚
        output_format: å¯é€‰ï¼Œå­—ç¬¦ä¸²ï¼ŒæŒ‡å®šè¾“å‡ºæ ¼å¼è¯´æ˜ï¼Œä¼šæ‹¼æ¥åˆ° system_prompt åé¢ã€‚
        """
        # 1. å½“æœ‰å·¥å…·æ—¶ï¼Œä¸ä½¿ç”¨ StrOutputParser ä»¥ä¿ç•™å·¥å…·è°ƒç”¨ä¿¡æ¯
        if tools:
            # å·¥å…·è°ƒç”¨æ¨¡å¼ï¼šä¸ä½¿ç”¨è¾“å‡ºè§£æå™¨ï¼Œä¿æŒåŸå§‹å“åº”
            output_parser = None
            logger.debug("å·¥å…·è°ƒç”¨æ¨¡å¼ï¼šä¸ä½¿ç”¨è¾“å‡ºè§£æå™¨")
        elif not output_parser:
            output_parser = StrOutputParser()
            logger.debug("æ ‡å‡†æ¨¡å¼ï¼šä½¿ç”¨ StrOutputParser")
        
        # 2. æ„å»º promptï¼ŒåŒ…å« system promptã€å†å²æ¶ˆæ¯å’Œç”¨æˆ·è¾“å…¥
        prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content=system_prompt),
            MessagesPlaceholder(variable_name="chat_history"),
            HumanMessagePromptTemplate.from_template("{input_prompt}"),
        ])
        
        # 3. æ ¹æ®æ˜¯å¦æœ‰è¾“å‡ºè§£æå™¨æ„å»ºä¸åŒçš„ runnable
        if output_parser:
            runnable = prompt | client | output_parser
        else:
            runnable = prompt | client
       
        return RunnableWithMessageHistory(
            runnable,
            self.get_message_history,
            input_messages_key="input_prompt",
            history_messages_key="chat_history"
        )

    def get_chat_model(self, **kwargs):
        """
        è·å–å½“å‰ provider çš„ chat modelã€‚
        """
        self._validate_provider()
        return self.providers[self.provider].get_chat_model(**kwargs)

    def get_embedding_model(self, **kwargs):
        """
        è·å–å½“å‰ provider çš„ embedding modelã€‚
        """
        self._validate_provider()
        return self.providers[self.provider].get_embedding_model(**kwargs)

    def call_llm_chain(
        self,
        role: str,
        input_prompt: str,
        session_id: str,
        output_parser=StrOutputParser(),
        system_format_dict: dict = None,
        tools: Optional[List[Dict]] = None,
        enable_llm_summary: bool = True
    ) -> Any:
        """
        åŒæ­¥ç‰ˆæœ¬çš„ LLM è°ƒç”¨æ–¹æ³•ï¼Œé€‚ç”¨äºéå¼‚æ­¥ç¯å¢ƒã€‚

        Args:
            role (str): PDFReaderRole æšä¸¾å€¼
            input_prompt (str): è¾“å…¥æç¤º
            session_id (str): ä¼šè¯ ID
            output_parser: è¾“å‡ºè§£æå™¨
            system_format_dict: ç³»ç»Ÿæç¤ºè¯æ ¼å¼åŒ–å‚æ•°
            tools: å·¥å…·å®šä¹‰åˆ—è¡¨
            enable_llm_summary: æ˜¯å¦å¯ç”¨LLMå†å²æ€»ç»“ï¼ˆé»˜è®¤Trueï¼ŒFalseåˆ™ä½¿ç”¨é•¿åº¦æˆªæ–­ï¼‰

        Returns:
            Any: LLM å“åº”å¯¹è±¡
        """
        # é¢„å…ˆåˆ›å»ºæ¶ˆæ¯å†å²ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰ï¼Œä»¥ä¾¿æ§åˆ¶ LLM æ€»ç»“åŠŸèƒ½
        if session_id not in self.message_histories:
            self.get_message_history(session_id, enable_llm_summary=enable_llm_summary)

        # è°ƒè¯•ï¼šæ£€æŸ¥è°ƒç”¨å‰çš„æ¶ˆæ¯å†å²
        if session_id in self.message_histories:
            history = self.message_histories[session_id]
            logger.info(f"ğŸ“œ [HISTORY CHECK] ä¼šè¯ {session_id} å½“å‰æœ‰ {len(history.messages)} æ¡æ¶ˆæ¯")
            #for idx, msg in enumerate(history.messages):
            #    msg_type = type(msg).__name__
            #    has_tool_calls = hasattr(msg, 'tool_calls') and msg.tool_calls
            #    has_tool_call_id = hasattr(msg, 'tool_call_id')
            #    logger.info(f"  [{idx}] {msg_type} | tool_calls={has_tool_calls} | tool_call_id={has_tool_call_id}")
            #    if has_tool_calls:
            #        for tc in msg.tool_calls:
            #            tc_id = tc.get('id') if isinstance(tc, dict) else getattr(tc, 'id', 'unknown')
            #            tc_name = tc.get('name') if isinstance(tc, dict) else getattr(tc, 'name', 'unknown')
            #            logger.info(f"      â†’ tool_call: id={tc_id}, name={tc_name}")
            #    if has_tool_call_id:
            #        logger.info(f"      â†’ responding to: {msg.tool_call_id}")
        else:
            logger.info(f"ğŸ“œ [HISTORY CHECK] ä¼šè¯ {session_id} å°šæœªåˆ›å»º")

        # Format system prompt
        system_prompt = self._format_system_prompt(role, system_format_dict)

        # Get model with tools if provided
        if tools:
            chat_model = self.get_chat_model_with_tools(tools)
        else:
            chat_model = self.chat_model

        chain = self.build_chain(
            client=chat_model,
            system_prompt=system_prompt,
            output_parser=output_parser,
            tools=tools
        )

        try:
            # ç›´æ¥ä½¿ç”¨åŒæ­¥è°ƒç”¨
            response = chain.invoke(
                {"input_prompt": input_prompt},
                config={"configurable": {"session_id": session_id}}
            )

            return response

        except Exception as e:
            logger.error(f"{role} åŒæ­¥è°ƒç”¨LLMæŠ¥é”™: {e}")
            return ""

    def add_messages_to_history(self, session_id: str, messages: List) -> None:
        """
        å°†å¤šæ¡æ¶ˆæ¯æ·»åŠ åˆ°æŒ‡å®šä¼šè¯çš„å†å²è®°å½•ä¸­

        ç”¨äºå·¥å…·è°ƒç”¨åœºæ™¯ï¼šéœ€è¦å°† AIMessageï¼ˆå¸¦tool_callsï¼‰å’Œ ToolMessage éƒ½æ·»åŠ åˆ°å†å²

        Args:
            session_id (str): ä¼šè¯ ID
            messages (List): æ¶ˆæ¯åˆ—è¡¨ï¼Œå¯ä»¥åŒ…å« AIMessage, ToolMessage ç­‰
        """
        logger.info(f"ğŸ“ [ADD MESSAGES] å‡†å¤‡å°† {len(messages)} æ¡æ¶ˆæ¯æ·»åŠ åˆ°ä¼šè¯ {session_id}")

        if session_id not in self.message_histories:
            logger.warning(f"ä¼šè¯ {session_id} ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°ä¼šè¯")

            # ä»ç»Ÿä¸€é…ç½®ä¸­è·å–å‚æ•°
            config = SessionHistoryConfig.get_config(session_id)

            self.message_histories[session_id] = LimitedChatMessageHistory(
                max_messages=config["max_messages"],
                max_tokens=config["max_tokens"],
                use_llm_summary=config["use_llm_summary"],
                llm_client=self if config["use_llm_summary"] else None,
                summary_threshold=config["summary_threshold"]
            )

            logger.debug(f"åˆ›å»ºæ–°çš„æ¶ˆæ¯å†å² - session_id: {session_id}, "
                        f"max_messages: {config['max_messages']}, "
                        f"max_tokens: {config['max_tokens']}, "
                        f"summary_threshold: {config['summary_threshold']}")

        history = self.message_histories[session_id]
        logger.info(f"ğŸ“ [BEFORE ADD] ä¼šè¯å½“å‰æœ‰ {len(history.messages)} æ¡æ¶ˆæ¯")

        for idx, msg in enumerate(messages):
            msg_type = type(msg).__name__
            history.add_message(msg)

            # è¯¦ç»†è®°å½•æ¯æ¡æ¶ˆæ¯çš„ä¿¡æ¯
            if hasattr(msg, 'tool_calls') and msg.tool_calls:
                logger.info(f"ğŸ“ [{idx+1}/{len(messages)}] æ·»åŠ  {msg_type} (åŒ…å« {len(msg.tool_calls)} ä¸ª tool_calls)")
                for tc in msg.tool_calls:
                    tc_id = tc.get('id') if isinstance(tc, dict) else getattr(tc, 'id', 'unknown')
                    tc_name = tc.get('name') if isinstance(tc, dict) else getattr(tc, 'name', 'unknown')
                    logger.info(f"      â†’ tool_call: id={tc_id}, name={tc_name}")
            elif hasattr(msg, 'tool_call_id'):
                logger.info(f"ğŸ“ [{idx+1}/{len(messages)}] æ·»åŠ  {msg_type} (å“åº” tool_call_id={msg.tool_call_id})")
            else:
                logger.info(f"ğŸ“ [{idx+1}/{len(messages)}] æ·»åŠ  {msg_type}")

        logger.info(f"ğŸ“ [AFTER ADD] ä¼šè¯ç°åœ¨æœ‰ {len(history.messages)} æ¡æ¶ˆæ¯")


def get_embeddings(**kwargs):
    """
    è·å–å…¨å±€åµŒå…¥æ¨¡å‹å®ä¾‹
    
    æ ¹æ®é…ç½®è¿”å›å¯¹åº”çš„åµŒå…¥æ¨¡å‹ï¼ˆAzure OpenAI æˆ– OpenAIï¼‰
    
    Args:
        **kwargs: ä¼ é€’ç»™åµŒå…¥æ¨¡å‹çš„é¢å¤–å‚æ•°
        
    Returns:
        åµŒå…¥æ¨¡å‹å®ä¾‹ï¼ˆAzureOpenAIEmbeddings æˆ– OpenAIEmbeddingsï¼‰
    """
    provider = LLM_EMBEDDING_CONFIG.get("provider", "openai").lower()
    
    if provider == "azure":
        return AzureOpenAIEmbeddings(
            openai_api_key=kwargs.get("openai_api_key", LLM_EMBEDDING_CONFIG.get("api_key")),
            openai_api_version=kwargs.get("openai_api_version", LLM_EMBEDDING_CONFIG.get("api_version")),
            azure_endpoint=kwargs.get("azure_endpoint", LLM_EMBEDDING_CONFIG.get("azure_endpoint")),
            deployment=kwargs.get("deployment", LLM_EMBEDDING_CONFIG.get("deployment")),
            model=kwargs.get("model", LLM_EMBEDDING_CONFIG.get("model")),
            max_retries=kwargs.get("max_retries", 5)
        )
    elif provider == "openai":
        return OpenAIEmbeddings(
            openai_api_key=kwargs.get("openai_api_key", LLM_EMBEDDING_CONFIG.get("openai_api_key")),
            model=kwargs.get("model", LLM_EMBEDDING_CONFIG.get("openai_model", "text-embedding-ada-002")),
            max_retries=kwargs.get("max_retries", 5)
        )
    else:
        # é»˜è®¤ä½¿ç”¨ OpenAI
        logger.warning(f"æœªçŸ¥çš„åµŒå…¥æ¨¡å‹ provider: {provider}ï¼Œé»˜è®¤ä½¿ç”¨ OpenAI")
        return OpenAIEmbeddings(
            openai_api_key=kwargs.get("openai_api_key", LLM_EMBEDDING_CONFIG.get("openai_api_key")),
            model=kwargs.get("model", LLM_EMBEDDING_CONFIG.get("openai_model", "text-embedding-ada-002")),
            max_retries=kwargs.get("max_retries", 5)
        )